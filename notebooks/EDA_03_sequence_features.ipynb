{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb0c159",
   "metadata": {},
   "source": [
    "# EDA 03: Sequence Features & Baseline Model\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Analyze the protein sequences themselves (Amino Acid Composition).\n",
    "2. Explore K-mer frequencies (short subsequences).\n",
    "3. Build a **Frequency Baseline Model** to establish a minimum performance benchmark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e07d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Setup plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Paths\n",
    "TRAIN_SEQ_PATH = '../Train/train_sequences.fasta'\n",
    "TRAIN_TERMS_PATH = '../Train/train_terms.tsv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad8f99",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "We'll load the sequences and the terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sequences\n",
    "sequences = []\n",
    "for record in SeqIO.parse(TRAIN_SEQ_PATH, \"fasta\"):\n",
    "    sequences.append({\n",
    "        'EntryID': record.id,\n",
    "        'sequence': str(record.seq),\n",
    "        'length': len(record.seq)\n",
    "    })\n",
    "\n",
    "df_seq = pd.DataFrame(sequences)\n",
    "print(f\"Loaded {len(df_seq)} sequences.\")\n",
    "df_seq.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3567ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Terms\n",
    "df_terms = pd.read_csv(TRAIN_TERMS_PATH, sep='\\t')\n",
    "print(f\"Loaded {len(df_terms)} annotations.\")\n",
    "df_terms.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a04d25",
   "metadata": {},
   "source": [
    "## 2. Amino Acid Composition\n",
    "Proteins are made of 20 standard amino acids. Let's see the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdccb9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count all amino acids in the dataset\n",
    "# We'll use a sample if the dataset is huge, but for ~140k proteins it should be fast enough\n",
    "all_residues = Counter()\n",
    "\n",
    "# Using a subset for speed in visualization if needed, but let's try full pass\n",
    "for seq in df_seq['sequence']:\n",
    "    all_residues.update(seq)\n",
    "\n",
    "# Convert to dataframe\n",
    "aa_df = pd.DataFrame.from_dict(all_residues, orient='index', columns=['count']).reset_index()\n",
    "aa_df.columns = ['Amino Acid', 'Count']\n",
    "aa_df['Frequency'] = aa_df['Count'] / aa_df['Count'].sum()\n",
    "aa_df = aa_df.sort_values('Frequency', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=aa_df, x='Amino Acid', y='Frequency', palette='viridis')\n",
    "plt.title('Global Amino Acid Composition')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Amino Acids:\")\n",
    "print(aa_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d6e785",
   "metadata": {},
   "source": [
    "## 3. K-mer Analysis (3-mers)\n",
    "K-mers are subsequences of length k. They can capture local motifs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0f65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmers(sequence, k=3):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "# Analyze a sample of sequences to save time\n",
    "sample_seqs = df_seq['sequence'].sample(n=1000, random_state=42)\n",
    "kmer_counts = Counter()\n",
    "\n",
    "for seq in sample_seqs:\n",
    "    kmer_counts.update(get_kmers(seq, k=3))\n",
    "\n",
    "# Top 20 3-mers\n",
    "top_kmers = pd.DataFrame(kmer_counts.most_common(20), columns=['K-mer', 'Count'])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(data=top_kmers, x='K-mer', y='Count', palette='magma')\n",
    "plt.title('Top 20 Most Common 3-mers (Sample n=1000)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6acf5f",
   "metadata": {},
   "source": [
    "## 4. Frequency Baseline Model\n",
    "This is the simplest possible model. We will:\n",
    "1. Split proteins into Train (80%) and Validation (20%).\n",
    "2. Calculate the frequency of each GO term in the Train set.\n",
    "3. For every protein in Validation, predict the top N most frequent terms.\n",
    "4. Calculate the F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split Data\n",
    "# We need to split by Protein ID, not by row\n",
    "unique_proteins = df_seq['EntryID'].unique()\n",
    "train_ids, val_ids = train_test_split(unique_proteins, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train proteins: {len(train_ids)}\")\n",
    "print(f\"Val proteins: {len(val_ids)}\")\n",
    "\n",
    "# Filter terms dataframe\n",
    "train_terms = df_terms[df_terms['EntryID'].isin(train_ids)]\n",
    "val_terms = df_terms[df_terms['EntryID'].isin(val_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train: Calculate Term Frequencies\n",
    "# We'll calculate separate frequencies for each ontology (BPO, CCO, MFO) if possible, \n",
    "# but for a naive baseline, we can just take the global top terms.\n",
    "# However, CAFA evaluates on specific ontologies. Let's look at the 'aspect' column if it exists.\n",
    "# If not, we'll just do global.\n",
    "\n",
    "# Check columns\n",
    "print(train_terms.columns)\n",
    "\n",
    "# Calculate frequency\n",
    "term_counts = train_terms['term'].value_counts()\n",
    "term_probs = term_counts / len(train_ids) # Probability = Count / Total Training Proteins\n",
    "\n",
    "print(\"Top 10 Most Frequent Terms:\")\n",
    "print(term_probs.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predict & Evaluate\n",
    "# For the baseline, we predict the same set of terms for EVERY protein.\n",
    "# Let's pick the top 50 terms.\n",
    "\n",
    "top_n = 50\n",
    "top_terms = term_probs.head(top_n).index.tolist()\n",
    "\n",
    "# Prepare Ground Truth for Validation\n",
    "# We need a format suitable for scikit-learn or manual F1 calculation.\n",
    "# Since this is multi-label, let's do a simplified evaluation:\n",
    "# Average Intersection over Union (Jaccard) or Precision/Recall per protein.\n",
    "\n",
    "def evaluate_baseline(val_ids, val_terms_df, predicted_terms):\n",
    "    # Create a dictionary of true terms for fast lookup\n",
    "    true_terms_dict = val_terms_df.groupby('EntryID')['term'].apply(set).to_dict()\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    pred_set = set(predicted_terms)\n",
    "    \n",
    "    for pid in val_ids:\n",
    "        true_set = true_terms_dict.get(pid, set())\n",
    "        \n",
    "        if len(true_set) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Intersection\n",
    "        tp = len(pred_set.intersection(true_set))\n",
    "        fp = len(pred_set) - tp\n",
    "        fn = len(true_set) - tp\n",
    "        \n",
    "        p = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        r = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n",
    "        \n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        f1s.append(f1)\n",
    "        \n",
    "    return np.mean(precisions), np.mean(recalls), np.mean(f1s)\n",
    "\n",
    "precision, recall, f1 = evaluate_baseline(val_ids, val_terms, top_terms)\n",
    "\n",
    "print(f\"Baseline Results (Top {top_n} terms):\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022c6e3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This F1 score represents the \"floor\" performance. Any machine learning model we build (BLAST, CNN, ProtBERT) must beat this score to be considered useful.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Implement a BLAST-based baseline (usually much stronger).\n",
    "2. Start building the actual Data Loaders for the deep learning pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
