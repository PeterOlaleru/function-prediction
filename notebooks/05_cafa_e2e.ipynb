{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6141555",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6141555",
    "language": "python",
    "outputId": "a0f643f2-8efe-4199-bf00-8438fe23dd5a"
   },
   "outputs": [],
   "source": [
    "# CELL 01 - Setup (NO REPO)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Always run from a simple writable location; never cd into a repo.\n",
    "if os.path.exists('/content'):\n",
    "    os.chdir('/content')\n",
    "RUNTIME_ROOT = Path.cwd()\n",
    "DATA_ROOT = (RUNTIME_ROOT / 'cafa6_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print(f'CWD: {Path.cwd()}')\n",
    "print(f'DATA_ROOT: {DATA_ROOT.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f4886d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8f4886d",
    "language": "python",
    "outputId": "0ff3d51a-c4f1-4dc3-f9a4-12ec55fa121e"
   },
   "outputs": [],
   "source": [
    "# CELL 02 - Install dependencies (mandatory, early)\n",
    "# Local (Windows): install ONLY missing packages to avoid long-path issues.\n",
    "import importlib.util\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def _detect_kaggle() -> bool:\n",
    "    return bool(\n",
    "        os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\")\n",
    "        or os.environ.get(\"KAGGLE_URL_BASE\")\n",
    "        or os.environ.get(\"KAGGLE_DATA_PROXY_URL\")\n",
    "    )\n",
    "\n",
    "\n",
    "def _detect_colab() -> bool:\n",
    "    return bool(\n",
    "        os.environ.get(\"COLAB_RELEASE_TAG\")\n",
    "        or os.environ.get(\"COLAB_GPU\")\n",
    "        or os.environ.get(\"COLAB_TPU_ADDR\")\n",
    "    )\n",
    "\n",
    "\n",
    "IS_KAGGLE = _detect_kaggle()\n",
    "IS_COLAB = (not IS_KAGGLE) and _detect_colab()\n",
    "if IS_KAGGLE:\n",
    "    print(\"Environment: Kaggle Detected\")\n",
    "elif IS_COLAB:\n",
    "    print(\"Environment: Colab Detected\")\n",
    "else:\n",
    "    print(\"Environment: Local Detected\")\n",
    "\n",
    "\n",
    "def _pip_install(pkgs: list[str]) -> None:\n",
    "    if not pkgs:\n",
    "        return\n",
    "    print(\"+\", sys.executable, \"-m\", \"pip\", \"install\", *pkgs)\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "\n",
    "\n",
    "# We still guarantee requirements are present by installing missing ones.\n",
    "# IMPORTANT (Windows): avoid forcing heavy Jupyter installs; the notebook already runs inside a kernel.\n",
    "REQUIRED = {\n",
    "    # Core\n",
    "    \"pandas\": \"pandas\",\n",
    "    \"numpy\": \"numpy\",\n",
    "    \"scipy\": \"scipy\",\n",
    "    \"pyarrow\": \"pyarrow\",\n",
    "    # ML\n",
    "    \"scikit-learn\": \"sklearn\",\n",
    "    \"torch\": \"torch\",\n",
    "    \"transformers\": \"transformers\",\n",
    "    \"py-boost\": \"py_boost\",\n",
    "    # Bio / graph\n",
    "    \"biopython\": \"Bio\",\n",
    "    \"obonet\": \"obonet\",\n",
    "    \"networkx\": \"networkx\",\n",
    "    # Visualisation\n",
    "    \"matplotlib\": \"matplotlib\",\n",
    "    \"seaborn\": \"seaborn\",\n",
    "    # Utils\n",
    "    \"tqdm\": \"tqdm\",\n",
    "    \"requests\": \"requests\",\n",
    "    \"urllib3\": \"urllib3\",\n",
    "    \"joblib\": \"joblib\",\n",
    "    \"psutil\": \"psutil\",\n",
    "    \"fastparquet\": \"fastparquet\",\n",
    "    \"pyyaml\": \"yaml\",\n",
    "    # Checkpointing\n",
    "    \"huggingface_hub\": \"huggingface_hub\",\n",
    "}\n",
    "\n",
    "missing = [pkg for pkg, mod in REQUIRED.items() if importlib.util.find_spec(mod) is None]\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    if missing:\n",
    "        _pip_install(missing)\n",
    "    else:\n",
    "        print(\"Kaggle: skipping pip install (already satisfied).\")\n",
    "else:\n",
    "    if missing:\n",
    "        try:\n",
    "            _pip_install(missing)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to install required packages: {missing}. Error: {e}\") from e\n",
    "    else:\n",
    "        print(\"Local/Colab: skipping pip install (already satisfied).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c51e8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a54c51e8",
    "language": "python",
    "outputId": "c6abb439-75c4-48b4-b577-10b567adc154"
   },
   "outputs": [],
   "source": [
    "# CELL 02b - Hugging Face auth bootstrap (token only)\n",
    "# Reads from env (optionally loaded from .env). Never prints the token.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _detect_kaggle_env() -> bool:\n",
    "    return bool(\n",
    "        os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\")\n",
    "        or os.environ.get(\"KAGGLE_URL_BASE\")\n",
    "        or os.environ.get(\"KAGGLE_DATA_PROXY_URL\")\n",
    "    )\n",
    "\n",
    "\n",
    "def _detect_colab_env() -> bool:\n",
    "    return bool(\n",
    "        os.environ.get(\"COLAB_RELEASE_TAG\")\n",
    "        or os.environ.get(\"COLAB_GPU\")\n",
    "        or os.environ.get(\"COLAB_TPU_ADDR\")\n",
    "    )\n",
    "\n",
    "\n",
    "IS_KAGGLE_ENV = _detect_kaggle_env()\n",
    "IS_COLAB_ENV = (not IS_KAGGLE_ENV) and _detect_colab_env()\n",
    "\n",
    "\n",
    "def _load_dotenv_if_present(dotenv_path: Path) -> None:\n",
    "    try:\n",
    "        dotenv_path = Path(dotenv_path)\n",
    "        if not dotenv_path.exists():\n",
    "            return\n",
    "        for raw in dotenv_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "                continue\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            k = k.strip()\n",
    "            v = v.strip().strip('\"').strip(\"'\")\n",
    "            if k and k not in os.environ:\n",
    "                os.environ[k] = v\n",
    "    except Exception:\n",
    "        return\n",
    "\n",
    "\n",
    "# Local convenience: allow a repo-level .env (gitignored) to populate env vars.\n",
    "_cwd = Path.cwd()\n",
    "_load_dotenv_if_present(_cwd / \".env\")\n",
    "_load_dotenv_if_present(_cwd.parent / \".env\")\n",
    "\n",
    "\n",
    "def _get_secret_env_first(name: str) -> str:\n",
    "    # Policy:\n",
    "    # - Colab: ONLY use userdata.get(name)\n",
    "    # - Elsewhere: env vars (possibly loaded from .env). We do not rely on Kaggle uploads.\n",
    "    v = (os.environ.get(name, \"\") or \"\").strip()\n",
    "    if v:\n",
    "        return v\n",
    "    if IS_COLAB_ENV:\n",
    "        try:\n",
    "            from google.colab import userdata  # type: ignore\n",
    "\n",
    "            return (userdata.get(name) or \"\").strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    if IS_KAGGLE_ENV:\n",
    "        # Kaggle notebooks can still use Secrets to inject tokens.\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient  # type: ignore\n",
    "\n",
    "            return (UserSecretsClient().get_secret(name) or \"\").strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "_tok = _get_secret_env_first(\"HUGGINGFACE_TOKEN\") or _get_secret_env_first(\"HF_TOKEN\")\n",
    "if _tok:\n",
    "    # huggingface_hub uses HF_TOKEN by default\n",
    "    os.environ.setdefault(\"HF_TOKEN\", _tok)\n",
    "    os.environ.setdefault(\"HUGGINGFACE_TOKEN\", _tok)\n",
    "    print(\"Hugging Face token: present\")\n",
    "else:\n",
    "    print(\"Hugging Face token: MISSING (uploads will fail)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2661d8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2661d8c",
    "language": "python",
    "outputId": "0086e706-411f-48d8-ae67-ebf367d8edc2"
   },
   "outputs": [],
   "source": [
    "# CELL 02c - Hugging Face Hub sanity check (debug)\n",
    "# Verifies the token can authenticate and (optionally) access the target repo.\n",
    "\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"huggingface_hub not installed; run the setup cell(s) above\") from e\n",
    "\n",
    "def _get_conf(name: str) -> str:\n",
    "    # Check env var first\n",
    "    v = (os.environ.get(name) or \"\").strip()\n",
    "    if v:\n",
    "        return v\n",
    "    # Check Colab secrets\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        return (userdata.get(name) or \"\").strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "repo_id = _get_conf(\"CAFA_HF_REPO_ID\")\n",
    "repo_type = _get_conf(\"CAFA_HF_REPO_TYPE\") or \"dataset\"\n",
    "\n",
    "token = _get_conf(\"HF_TOKEN\") or _get_conf(\"HUGGINGFACE_TOKEN\")\n",
    "api = HfApi(token=token or None)\n",
    "\n",
    "if not token:\n",
    "    print(\"HF token: MISSING (run Cell 3 / set HF_TOKEN or HUGGINGFACE_TOKEN)\")\n",
    "else:\n",
    "    print(\"HF token: present\")\n",
    "\n",
    "try:\n",
    "    me = api.whoami() if token else {}\n",
    "    print(\"HF whoami:\", (me.get(\"name\") or me.get(\"fullname\") or \"<unknown>\") if me else \"<skipped>\")\n",
    "except Exception as e:\n",
    "    print(\"HF whoami failed:\", repr(e))\n",
    "\n",
    "if repo_id:\n",
    "    try:\n",
    "        info = api.repo_info(repo_id=repo_id, repo_type=repo_type)\n",
    "        # huggingface_hub uses different info types per repo_type; dataset returns DatasetInfo with .id\n",
    "        resolved = getattr(info, \"id\", None) or getattr(info, \"repo_id\", None) or repo_id\n",
    "        print(\"HF repo accessible:\", resolved)\n",
    "    except Exception as e:\n",
    "        print(\"HF repo not accessible yet (or missing):\", repr(e))\n",
    "else:\n",
    "    print(\"CAFA_HF_REPO_ID not set (skip repo check)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8f969d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "db8f969d",
    "language": "python",
    "outputId": "f012b534-2aed-478f-c409-012e98a7a55a"
   },
   "outputs": [],
   "source": [
    "# CELL 03 - Solution: 1. SETUP, CONFIG & DIAGNOSTICS\n",
    "# PRODUCES: manifest.json (checkpoint artefact manifest)\n",
    "# 1. SETUP, CONFIG & DIAGNOSTICS\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------\n",
    "# Environment Detection & Paths\n",
    "# ------------------------------------------\n",
    "# Kaggle images can have `google-colab` installed; never use `import google.colab` as a signal.\n",
    "\n",
    "def _detect_kaggle() -> bool:\n",
    "    # Kaggle kernels reliably set at least one of these env vars.\n",
    "    return bool(\n",
    "        os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\n",
    "        or os.environ.get('KAGGLE_URL_BASE')\n",
    "        or os.environ.get('KAGGLE_DATA_PROXY_URL')\n",
    "    )\n",
    "\n",
    "def _detect_colab() -> bool:\n",
    "    # Colab sets these env vars; this avoids false positives on Kaggle.\n",
    "    return bool(os.environ.get('COLAB_RELEASE_TAG') or os.environ.get('COLAB_GPU') or os.environ.get('COLAB_TPU_ADDR'))\n",
    "\n",
    "IS_KAGGLE = _detect_kaggle()\n",
    "IS_COLAB = (not IS_KAGGLE) and _detect_colab()\n",
    "\n",
    "KAGGLE_INPUT_ROOT = Path('/kaggle/input')\n",
    "if IS_KAGGLE:\n",
    "    print('Environment: Kaggle Detected')\n",
    "    WORKING_ROOT = Path('/kaggle/working')\n",
    "    if KAGGLE_INPUT_ROOT.exists():\n",
    "        for dirname, _, filenames in os.walk(str(KAGGLE_INPUT_ROOT)):\n",
    "            for filename in filenames:\n",
    "                print(os.path.join(dirname, filename))\n",
    "elif IS_COLAB:\n",
    "    print('Environment: Colab Detected')\n",
    "    WORKING_ROOT = Path(os.environ.get('CAFA_WORKING_ROOT', str(Path('/content'))))\n",
    "else:\n",
    "    print('Environment: Local Detected')\n",
    "    # No repo assumptions: treat current working dir as runtime root.\n",
    "    WORKING_ROOT = Path.cwd()\n",
    "# ------------------------------------------\n",
    "# Local cache roots (ephemeral) + artefacts root\n",
    "# ------------------------------------------\n",
    "# Single source of truth for this notebook: everything lives under cafa6_data/.\n",
    "# If Cell 1 ran, reuse its DATA_ROOT so we don't fork paths.\n",
    "if 'DATA_ROOT' in globals():\n",
    "    WORK_ROOT = Path(DATA_ROOT)\n",
    "    WORKING_ROOT = WORK_ROOT.parent\n",
    "else:\n",
    "    WORK_ROOT = WORKING_ROOT / 'cafa6_data'\n",
    "WORK_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "for _d in ['parsed', 'features', 'external', 'Train', 'Test']:\n",
    "    (WORK_ROOT / _d).mkdir(parents=True, exist_ok=True)\n",
    "# Keep caches OUT of WORK_ROOT so we never accidentally publish them.\n",
    "CACHE_ROOT = WORKING_ROOT / 'cache'\n",
    "CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "os.environ.setdefault('HF_HOME', str(CACHE_ROOT / 'hf_home'))\n",
    "os.environ.setdefault('TRANSFORMERS_CACHE', str(CACHE_ROOT / 'hf_home'))\n",
    "os.environ.setdefault('HF_HUB_CACHE', str(CACHE_ROOT / 'hf_hub'))\n",
    "os.environ.setdefault('TORCH_HOME', str(CACHE_ROOT / 'torch_home'))\n",
    "# Runtime provenance guards: never publish downloaded artefacts.\n",
    "RUN_START_TS = time.time()\n",
    "DOWNLOADED_PATHS: set[Path] = set()\n",
    "\n",
    "def _mark_downloaded(p: Path) -> None:\n",
    "    DOWNLOADED_PATHS.add(Path(p).resolve())\n",
    "# ------------------------------------------\n",
    "# Dataset Discovery (competition data)\n",
    "DATASET_SLUG = 'cafa-6-protein-function-prediction'\n",
    "# Required competition files (MANDATORY):\n",
    "REQUIRED_COMP_FILES = [\n",
    "    'IA.tsv',\n",
    "    'sample_submission.tsv',\n",
    "    'Train/go-basic.obo',\n",
    "    'Train/train_sequences.fasta',\n",
    "    'Train/train_terms.tsv',\n",
    "    'Train/train_taxonomy.tsv',\n",
    "    'Test/testsuperset.fasta',\n",
    "    'Test/testsuperset-taxon-list.tsv',\n",
    "]\n",
    "\n",
    "def _ensure_kaggle_cli() -> None:\n",
    "    try:\n",
    "        subprocess.run(['kaggle', '--version'], check=True, capture_output=True, text=True, encoding='utf-8', errors='replace')\n",
    "        return\n",
    "    except (FileNotFoundError, subprocess.CalledProcessError):\n",
    "        pass\n",
    "\n",
    "    print(\"Installing kaggle...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install', 'kaggle'])\n",
    "    \n",
    "    # Check if it is now in PATH\n",
    "    if shutil.which('kaggle'):\n",
    "        return\n",
    "        \n",
    "    # If not, look for it in the python bin directory\n",
    "    p = Path(sys.executable).parent\n",
    "    candidates = [p / 'kaggle', p / 'kaggle.exe']\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            print(f\"Adding {p} to PATH for kaggle\")\n",
    "            os.environ['PATH'] = str(p) + os.pathsep + os.environ['PATH']\n",
    "            return\n",
    "            \n",
    "    # One more try with updated PATH\n",
    "    if shutil.which('kaggle'):\n",
    "        return\n",
    "        \n",
    "    raise RuntimeError(\"Kaggle CLI installed but not found in PATH. Please ensure 'kaggle' is in your PATH.\")\n",
    "\n",
    "def _get_secret(name: str) -> str:\n",
    "    # Helper to get secret from env or Colab userdata\n",
    "    v = os.environ.get(name)\n",
    "    if v:\n",
    "        return v\n",
    "    if IS_COLAB:\n",
    "        try:\n",
    "            from google.colab import userdata\n",
    "            return userdata.get(name)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return \"\"\n",
    "\n",
    "def _kaggle_env(require: bool = True) -> dict[str, str]:\n",
    "    env = os.environ.copy()\n",
    "    # Inject secrets if missing in env\n",
    "    if not env.get('KAGGLE_USERNAME'):\n",
    "        k = _get_secret('KAGGLE_USERNAME')\n",
    "        if k: env['KAGGLE_USERNAME'] = k\n",
    "    if not env.get('KAGGLE_KEY'):\n",
    "        k = _get_secret('KAGGLE_KEY')\n",
    "        if k: env['KAGGLE_KEY'] = k\n",
    "\n",
    "    if require and (not env.get('KAGGLE_USERNAME') or not env.get('KAGGLE_KEY')):\n",
    "        raise RuntimeError('Missing Kaggle API auth: set KAGGLE_USERNAME and KAGGLE_KEY as secrets/env vars.')\n",
    "    return env\n",
    "\n",
    "def _download_comp_file(rel_path: str, target_root: Path) -> None:\n",
    "    \"\"\"Download one competition file into target_root, preserving folders.\"\"\"\n",
    "    rel_path = rel_path.replace('\\\\', '/')\n",
    "    out_path = target_root / rel_path\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        return\n",
    "    _ensure_kaggle_cli()\n",
    "    env = _kaggle_env(require=True)\n",
    "    tmp = target_root / '_tmp_download'\n",
    "    tmp.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = ['kaggle', 'competitions', 'download', '-c', DATASET_SLUG, '-f', rel_path, '-p', str(tmp)]\n",
    "    print('+', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True, env=env)\n",
    "    name = Path(rel_path).name\n",
    "    zip_path = tmp / f'{name}.zip'\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(tmp)\n",
    "        zip_path.unlink()\n",
    "    cand1 = tmp / rel_path\n",
    "    cand2 = tmp / name\n",
    "    src = cand1 if cand1.exists() else (cand2 if cand2.exists() else None)\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(f'Downloaded file not found after unzip: {rel_path}')\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.move(str(src), str(out_path))\n",
    "    _mark_downloaded(out_path)\n",
    "\n",
    "def ensure_competition_data(data_root: Path) -> Path:\n",
    "    \"\"\"Ensures competition data exists under cafa6_data/. Returns dataset root.\"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    # Kaggle: copy from mounted input if available.\n",
    "    if IS_KAGGLE and Path('/kaggle/input').exists():\n",
    "        mounted = Path('/kaggle/input') / DATASET_SLUG\n",
    "        if mounted.exists():\n",
    "            for rel in REQUIRED_COMP_FILES:\n",
    "                dst = data_root / rel\n",
    "                if dst.exists():\n",
    "                    continue\n",
    "                src = mounted / rel\n",
    "                dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                if not src.exists():\n",
    "                    raise FileNotFoundError(f'Missing in mounted Kaggle input: {src}')\n",
    "                shutil.copy2(src, dst)\n",
    "                _mark_downloaded(dst)\n",
    "            return data_root\n",
    "    # Colab/Local: download file-by-file via Kaggle API.\n",
    "    for rel in REQUIRED_COMP_FILES:\n",
    "        _download_comp_file(rel, data_root)\n",
    "    return data_root\n",
    "# Canonical dataset root for the entire notebook\n",
    "DATASET_ROOT = ensure_competition_data(WORK_ROOT)\n",
    "# Canonical competition paths (always under WORK_ROOT/cafa6_data)\n",
    "PATH_IA = WORK_ROOT / 'IA.tsv'\n",
    "PATH_SAMPLE_SUB = WORK_ROOT / 'sample_submission.tsv'\n",
    "PATH_GO_OBO = WORK_ROOT / 'Train' / 'go-basic.obo'\n",
    "PATH_TRAIN_FASTA = WORK_ROOT / 'Train' / 'train_sequences.fasta'\n",
    "PATH_TRAIN_TERMS = WORK_ROOT / 'Train' / 'train_terms.tsv'\n",
    "PATH_TRAIN_TAXON = WORK_ROOT / 'Train' / 'train_taxonomy.tsv'\n",
    "PATH_TEST_FASTA = WORK_ROOT / 'Test' / 'testsuperset.fasta'\n",
    "PATH_TEST_TAXON = WORK_ROOT / 'Test' / 'testsuperset-taxon-list.tsv'\n",
    "print('DATASET_ROOT:', DATASET_ROOT.resolve())\n",
    "# Checkpoint store (Hugging Face Hub = single source of truth)\n",
    "# ------------------------------------------\n",
    "\n",
    "from huggingface_hub import CommitOperationAdd, HfApi, hf_hub_download\n",
    "\n",
    "# No Kaggle publishing. HF repo is the only remote artefact store.\n",
    "HF_REPO_ID = (_get_secret('CAFA_HF_REPO_ID') or '').strip()\n",
    "HF_REPO_TYPE = (_get_secret('CAFA_HF_REPO_TYPE') or 'dataset').strip() or 'dataset'\n",
    "\n",
    "# Upload-after-write default: ON if CAFA_HF_REPO_ID is set.\n",
    "HF_PUSH = str(os.environ.get('CAFA_HF_PUSH', '') or os.environ.get('CAFA_CHECKPOINT_PUSH', '') or ('1' if HF_REPO_ID else '0')).strip().lower() in {'1', 'true', 'yes'}\n",
    "HF_PULL = str(os.environ.get('CAFA_HF_PULL', '1')).strip().lower() in {'1', 'true', 'yes'}\n",
    "HF_CREATE_REPO = str(os.environ.get('CAFA_HF_CREATE_REPO', '0')).strip().lower() in {'1', 'true', 'yes'}\n",
    "\n",
    "# Keep behaviour for \"push existing artefacts\" knobs used later in the notebook.\n",
    "PUSH_EXISTING_CHECKPOINTS = str(os.environ.get('CAFA_CHECKPOINT_PUSH_EXISTING', '0')).strip().lower() in {'1', 'true', 'yes'}\n",
    "\n",
    "MANIFEST_PATH = WORK_ROOT / 'manifest.json'\n",
    "\n",
    "def _load_manifest() -> dict:\n",
    "    if MANIFEST_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(MANIFEST_PATH.read_text(encoding='utf-8'))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def _update_manifest(stage: str, required_paths: list[Path], note: str = '') -> None:\n",
    "    m = _load_manifest()\n",
    "    stages = m.get('stages', {})\n",
    "    files = []\n",
    "    for p in required_paths:\n",
    "        p = Path(p)\n",
    "        rel = str(p.relative_to(WORK_ROOT)) if str(p).startswith(str(WORK_ROOT)) else str(p)\n",
    "        files.append({'path': rel, 'bytes': int(p.stat().st_size) if p.exists() else None})\n",
    "    stages[stage] = {\n",
    "        'ts_utc': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "        'note': note,\n",
    "        'files': files,\n",
    "    }\n",
    "    m['stages'] = stages\n",
    "    MANIFEST_PATH.write_text(json.dumps(m, indent=2), encoding='utf-8')\n",
    "\n",
    "def _stage_files_signature(required_paths: list[Path]) -> list[dict]:\n",
    "    sig = []\n",
    "    for p in required_paths:\n",
    "        p = Path(p)\n",
    "        rel = str(p.relative_to(WORK_ROOT)) if str(p).startswith(str(WORK_ROOT)) else str(p)\n",
    "        sig.append({'path': rel, 'bytes': int(p.stat().st_size) if p.exists() else None})\n",
    "    return sorted(sig, key=lambda x: x['path'])\n",
    "\n",
    "def _hf_token_present() -> bool:\n",
    "    return bool((_get_secret('HF_TOKEN') or _get_secret('HUGGINGFACE_TOKEN') or '').strip())\n",
    "\n",
    "@dataclass\n",
    "class HfCheckpointStore:\n",
    "    work_root: Path\n",
    "    repo_id: str\n",
    "    repo_type: str\n",
    "    pull_enabled: bool\n",
    "    push_enabled: bool\n",
    "    create_repo: bool\n",
    "    manifest_name: str = 'manifest.json'\n",
    "\n",
    "    def _api(self) -> HfApi:\n",
    "        return HfApi(token=(_get_secret('HF_TOKEN') or None))\n",
    "\n",
    "    def _ensure_repo(self) -> None:\n",
    "        if not self.repo_id:\n",
    "            raise ValueError('Missing CAFA_HF_REPO_ID=<user>/<repo>; cannot use HF checkpointing.')\n",
    "        if not self.create_repo:\n",
    "            return\n",
    "        try:\n",
    "            self._api().create_repo(repo_id=self.repo_id, repo_type=self.repo_type, exist_ok=True)\n",
    "        except Exception:\n",
    "            # If repo already exists or token lacks create permission, we just proceed.\n",
    "            pass\n",
    "\n",
    "    def pull(self, required_files: list[str] | None = None) -> None:\n",
    "        if not self.pull_enabled:\n",
    "            return\n",
    "        required_files = list(required_files or [])\n",
    "        if not required_files:\n",
    "            return\n",
    "        if not self.repo_id:\n",
    "            print('HF pull skipped: CAFA_HF_REPO_ID not set.')\n",
    "            return\n",
    "\n",
    "        missing_locally = [f for f in required_files if not (self.work_root / f).exists()]\n",
    "        if not missing_locally:\n",
    "            print(f'All {len(required_files)} required artefacts present locally. Skipping pull.')\n",
    "            return\n",
    "\n",
    "        print(f'Pulling {len(missing_locally)} checkpoint artefacts from HF: {self.repo_id} ({self.repo_type})')\n",
    "        for rel in missing_locally:\n",
    "            rel = str(rel).replace('\\\\', '/').lstrip('/')\n",
    "            dst = self.work_root / rel\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                cached = hf_hub_download(\n",
    "                    repo_id=self.repo_id,\n",
    "                    repo_type=self.repo_type,\n",
    "                    filename=rel,\n",
    "                    token=(_get_secret('HF_TOKEN') or None),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f'WARNING: missing on HF (or not accessible): {rel} ({e})')\n",
    "                continue\n",
    "            shutil.copy2(cached, dst)\n",
    "            _mark_downloaded(dst)\n",
    "        print('HF checkpoint pull complete.')\n",
    "\n",
    "    def maybe_push(self, stage: str, required_paths: list[Path], note: str = '') -> bool:\n",
    "        if not self.push_enabled:\n",
    "            print(f'[CHECKPOINT] {stage}: push disabled (set CAFA_HF_PUSH=1 to enable).')\n",
    "            return False\n",
    "        if not self.repo_id:\n",
    "            print(f'[CHECKPOINT] {stage}: repo not set (CAFA_HF_REPO_ID missing).')\n",
    "            return False\n",
    "        if not _hf_token_present():\n",
    "            print(f'[CHECKPOINT] {stage}: token missing (set HF_TOKEN/HUGGINGFACE_TOKEN).')\n",
    "            return False\n",
    "        return self.push(stage, required_paths, note=note)\n",
    "\n",
    "    def push(self, stage: str, required_paths: list[Path], note: str = '') -> bool:\n",
    "        if not self.push_enabled:\n",
    "            return False\n",
    "        if not self.repo_id:\n",
    "            raise ValueError('Missing CAFA_HF_REPO_ID=<user>/<repo>; cannot checkpoint.')\n",
    "        if not _hf_token_present():\n",
    "            raise RuntimeError('HF token missing (set HF_TOKEN/HUGGINGFACE_TOKEN).')\n",
    "\n",
    "        missing = [Path(p) for p in required_paths if not Path(p).exists()]\n",
    "        if missing:\n",
    "            print(f'[CHECKPOINT] {stage}: missing artefacts; skipping push: {missing}')\n",
    "            return False\n",
    "        # Never publish pulled/downloaded artefacts (non-negotiable rule).\n",
    "        downloaded = []\n",
    "        try:\n",
    "            downloaded = [Path(p).resolve() for p in required_paths if Path(p).resolve() in DOWNLOADED_PATHS]\n",
    "        except Exception:\n",
    "            downloaded = []\n",
    "        if downloaded:\n",
    "            print(f'[CHECKPOINT] {stage}: refusing to publish pulled/downloaded artefacts ({len(downloaded)} files).')\n",
    "            return False\n",
    "        # Default: do not push existing artefacts on reruns unless explicitly enabled.\n",
    "        if not bool(PUSH_EXISTING_CHECKPOINTS):\n",
    "            try:\n",
    "                fresh = [Path(p) for p in required_paths if Path(p).stat().st_mtime >= (RUN_START_TS - 2.0)]\n",
    "            except Exception:\n",
    "                fresh = []\n",
    "            if not fresh:\n",
    "                print(f'[CHECKPOINT] {stage}: nothing freshly built in this runtime; skipping push (set CAFA_CHECKPOINT_PUSH_EXISTING=1 to override).')\n",
    "                return False\n",
    "\n",
    "        # Optional skip: identical stage signature\n",
    "        m = _load_manifest()\n",
    "        existing = (m.get('stages', {}) or {}).get(stage) if isinstance(m, dict) else None\n",
    "        if isinstance(existing, dict):\n",
    "            prev_files = existing.get('files', [])\n",
    "            if isinstance(prev_files, list):\n",
    "                prev_sig = sorted([{'path': f.get('path'), 'bytes': f.get('bytes')} for f in prev_files if isinstance(f, dict)], key=lambda x: str(x.get('path')))\n",
    "                cur_sig = _stage_files_signature(required_paths)\n",
    "                if prev_sig == cur_sig:\n",
    "                    print(f'[CHECKPOINT] {stage}: unchanged; skipping push')\n",
    "                    return False\n",
    "\n",
    "        _update_manifest(stage, required_paths, note=note)\n",
    "        print(f'[CHECKPOINT] {stage}: publishing {len(required_paths)} files to HF repo {self.repo_id} ({self.repo_type})')\n",
    "\n",
    "        work_root_resolved = self.work_root.resolve()\n",
    "        ops: list[CommitOperationAdd] = []\n",
    "        for p in required_paths:\n",
    "            p = Path(p).resolve()\n",
    "            try:\n",
    "                rel = p.relative_to(work_root_resolved).as_posix()\n",
    "            except Exception:\n",
    "                raise ValueError(f'All checkpoint artefacts must live under WORK_ROOT. Got: {p}')\n",
    "            ops.append(CommitOperationAdd(path_in_repo=rel, path_or_fileobj=str(p)))\n",
    "\n",
    "        if MANIFEST_PATH.exists():\n",
    "            ops.append(CommitOperationAdd(path_in_repo=self.manifest_name, path_or_fileobj=str(MANIFEST_PATH)))\n",
    "\n",
    "        msg = f'{stage}: {note}'.strip() if note else stage\n",
    "        self._ensure_repo()\n",
    "        self._api().create_commit(repo_id=self.repo_id, repo_type=self.repo_type, operations=ops, commit_message=msg)\n",
    "        print('[CHECKPOINT] published:', msg)\n",
    "        return True\n",
    "\n",
    "print('HF checkpoint config:')\n",
    "print('  CAFA_HF_REPO_ID:', HF_REPO_ID or 'MISSING')\n",
    "print('  HF_PUSH:', HF_PUSH, 'HF_PULL:', HF_PULL, 'PUSH_EXISTING_CHECKPOINTS:', PUSH_EXISTING_CHECKPOINTS)\n",
    "\n",
    "# Checkpoint artefacts required for this notebook (MANDATORY, pulled file-by-file)\n",
    "# We use stage-based required sets so you can resume from a specific milestone.\n",
    "# Select via CAFA_CHECKPOINT_PULL_STAGE.\n",
    "#\n",
    "# Stages are cumulative by design (i.e. later stages include their prerequisites).\n",
    "# This avoids half-hydrated states when you resume from later cells.\n",
    "has_test = bool(PATH_TEST_FASTA.exists())\n",
    "STAGE_REQUIRED_FILES: dict[str, list[str]] = {}\n",
    "# Stage 01: parsed core\n",
    "STAGE_REQUIRED_FILES['stage_01_parsed'] = [\n",
    "    'parsed/train_seq.feather',\n",
    "    'parsed/train_terms.parquet',\n",
    "    'parsed/train_taxa.feather',\n",
    "    'parsed/term_counts.parquet',\n",
    "    'parsed/term_priors.parquet',\n",
    "] + ([\n",
    "    'parsed/test_seq.feather',\n",
    "    'parsed/test_taxa.feather',\n",
    "] if has_test else [])\n",
    "# Stage 02: external text corpus\n",
    "STAGE_REQUIRED_FILES['stage_02_external_text'] = STAGE_REQUIRED_FILES['stage_01_parsed'] + [\n",
    "    'external/entryid_text.tsv',\n",
    "]\n",
    "# Stage 03: TF-IDF text embeddings\n",
    "STAGE_REQUIRED_FILES['stage_03_tfidf_text'] = STAGE_REQUIRED_FILES['stage_02_external_text'] + [\n",
    "    'features/text_vectorizer.joblib',\n",
    "    'features/train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy',\n",
    "]\n",
    "# Stage 04: external propagated GOA priors (IEA)\n",
    "STAGE_REQUIRED_FILES['stage_04_external_goa_priors'] = STAGE_REQUIRED_FILES['stage_03_tfidf_text'] + [\n",
    "    'external/goa_filtered_iea.tsv.gz',\n",
    "    'external/prop_train_no_kaggle.tsv.gz',\n",
    "    'external/prop_test_no_kaggle.tsv.gz',\n",
    "]\n",
    "# Stage 06: sequence embeddings (core modalities)\n",
    "STAGE_REQUIRED_FILES['stage_06_embeddings_core'] = STAGE_REQUIRED_FILES['stage_04_external_goa_priors'] + [\n",
    "    'features/train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy',\n",
    "    'features/train_embeds_ankh.npy',\n",
    "    'features/test_embeds_ankh.npy',\n",
    "]\n",
    "# Stage 07: Level-1 OOF + test predictions (enables going straight to stacker)\n",
    "STAGE_REQUIRED_FILES['stage_07_level1_preds'] = STAGE_REQUIRED_FILES['stage_06_embeddings_core'] + [\n",
    "    'features/top_terms_13500.json',\n",
    "    'features/level1_preds/oof_pred_logreg.npy',\n",
    "    'features/level1_preds/test_pred_logreg.npy',\n",
    "    'features/level1_preds/oof_pred_gbdt.npy',\n",
    "    'features/level1_preds/test_pred_gbdt.npy',\n",
    "    'features/level1_preds/oof_pred_dnn.npy',\n",
    "    'features/level1_preds/test_pred_dnn.npy',\n",
    "    'features/level1_preds/oof_pred_knn.npy',\n",
    "    'features/level1_preds/test_pred_knn.npy',\n",
    "]\n",
    "# Stage 08: hierarchy-aware stacker output (enables going straight to submission)\n",
    "STAGE_REQUIRED_FILES['stage_08_stacker_gcn'] = STAGE_REQUIRED_FILES['stage_07_level1_preds'] + [\n",
    "    'features/test_pred_gcn.npy',\n",
    "]\n",
    "# Stage 09: final submission (rarely needed for pull, but included for completeness)\n",
    "STAGE_REQUIRED_FILES['stage_09_submission'] = STAGE_REQUIRED_FILES['stage_08_stacker_gcn'] + [\n",
    "    'features/test_pred_gcn.npy',\n",
    "    'submission.tsv',\n",
    "]\n",
    "def _select_required_checkpoint_files() -> list[str]:\n",
    "    stage = os.getenv('CAFA_CHECKPOINT_PULL_STAGE', '').strip()\n",
    "    if not stage:\n",
    "        # Default to pulling everything (all stages) so we have a full set of artefacts.\n",
    "        stage = 'stage_09_submission'\n",
    "    if stage.lower() in {'off', 'none', 'no', '0'}:\n",
    "        return []\n",
    "    if stage not in STAGE_REQUIRED_FILES:\n",
    "        raise ValueError(f'Unknown CAFA_CHECKPOINT_PULL_STAGE={stage!r}. Known: {sorted(STAGE_REQUIRED_FILES)}')\n",
    "    return list(STAGE_REQUIRED_FILES[stage])\n",
    "REQUIRED_CHECKPOINT_FILES = _select_required_checkpoint_files()\n",
    "STORE = HfCheckpointStore(\n",
    "    work_root=WORK_ROOT,\n",
    "    repo_id=HF_REPO_ID,\n",
    "    repo_type=HF_REPO_TYPE,\n",
    "    pull_enabled=HF_PULL,\n",
    "    push_enabled=HF_PUSH,\n",
    "    create_repo=HF_CREATE_REPO,\n",
    ")\n",
    "# Pull once at startup (fresh runtimes resume here)\n",
    "STORE.pull(required_files=REQUIRED_CHECKPOINT_FILES)\n",
    "# Post-pull diagnostics: make it obvious whether artefacts are present.\n",
    "\n",
    "def _p(path: Path) -> str:\n",
    "    return str(path)\n",
    "\n",
    "def _exists_bytes(path: Path) -> str:\n",
    "    if not path.exists():\n",
    "        return 'MISSING'\n",
    "    try:\n",
    "        if path.is_dir():\n",
    "            # Fast recursive size\n",
    "            total = sum(f.stat().st_size for f in path.rglob('*') if f.is_file())\n",
    "            return f'OK (DIR, {total / (1024**2):.1f} MB)'\n",
    "        return f'OK ({path.stat().st_size / (1024**2):.1f} MB)'\n",
    "    except Exception:\n",
    "        return 'OK'\n",
    "print('Checkpoint status (after pull):')\n",
    "print('  WORK_ROOT:', _p(WORK_ROOT))\n",
    "print('  parsed/:', _exists_bytes(WORK_ROOT / 'parsed'))\n",
    "print('  external/:', _exists_bytes(WORK_ROOT / 'external'))\n",
    "print('  features/:', _exists_bytes(WORK_ROOT / 'features'))\n",
    "print('  external/entryid_text.tsv:', _exists_bytes(WORK_ROOT / 'external' / 'entryid_text.tsv'))\n",
    "print('  parsed/train_seq.feather:', _exists_bytes(WORK_ROOT / 'parsed' / 'train_seq.feather'))\n",
    "\n",
    "def stage_present(required_paths: list[Path]) -> bool:\n",
    "    return all(Path(p).exists() for p in required_paths)\n",
    "# ------------------------------------------\n",
    "# Initial Diagnostics (Sequence Lengths)\n",
    "# ------------------------------------------\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "def read_fasta_lengths(path: Path, max_records=20000):\n",
    "    lengths = []\n",
    "    current = 0\n",
    "    n = 0\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if n > 0:\n",
    "                    lengths.append(current)\n",
    "                n += 1\n",
    "                current = 0\n",
    "                if max_records and n > max_records:\n",
    "                    break\n",
    "            else:\n",
    "                current += len(line)\n",
    "        if n > 0:\n",
    "            lengths.append(current)\n",
    "    return np.array(lengths)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(read_fasta_lengths(PATH_TRAIN_FASTA), bins=50, alpha=0.5, label='Train')\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    plt.hist(read_fasta_lengths(PATH_TEST_FASTA), bins=50, alpha=0.5, label='Test')\n",
    "plt.title('Sequence Length Distribution (First 20k)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36d8ae2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e36d8ae2",
    "language": "python",
    "outputId": "1d84bb25-01aa-423e-8d7b-61c982d01991"
   },
   "outputs": [],
   "source": [
    "# CELL 04 - Solution: 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# PRODUCES: parsed/term_counts.parquet, parsed/term_priors.parquet\n",
    "# 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "# ------------------------------------------\n",
    "# B. Parse OBO & Terms (needed in-memory downstream)\n",
    "# ------------------------------------------\n",
    "\n",
    "def parse_obo(path: Path):\n",
    "    parents = {}\n",
    "    namespaces = {}\n",
    "    cur_id, cur_ns = None, None\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '[Term]':\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "                cur_id, cur_ns = None, None\n",
    "            elif line.startswith('id: GO:'):\n",
    "                cur_id = line.split('id: ', 1)[1]\n",
    "            elif line.startswith('namespace:'):\n",
    "                cur_ns = line.split('namespace: ', 1)[1]\n",
    "            elif line.startswith('is_a:') and cur_id:\n",
    "                parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                parents.setdefault(cur_id, set()).add(parent)\n",
    "        if cur_id and cur_ns:\n",
    "            namespaces[cur_id] = cur_ns\n",
    "    return parents, namespaces\n",
    "print(\"Parsing OBO...\")\n",
    "go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "print(f\"GO Graph: {len(go_parents)} nodes with parents, {len(go_namespaces)} terms with namespace.\")\n",
    "# ------------------------------------------\n",
    "# Milestone checkpoint: stage_01_parsed\n",
    "# ------------------------------------------\n",
    "parsed_dir = WORK_ROOT / 'parsed'\n",
    "parsed_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_train_seq = parsed_dir / 'train_seq.feather'\n",
    "out_test_seq = parsed_dir / 'test_seq.feather'\n",
    "out_train_terms = parsed_dir / 'train_terms.parquet'\n",
    "out_term_counts = parsed_dir / 'term_counts.parquet'\n",
    "out_term_priors = parsed_dir / 'term_priors.parquet'\n",
    "out_train_taxa = parsed_dir / 'train_taxa.feather'\n",
    "out_test_taxa = parsed_dir / 'test_taxa.feather'\n",
    "expected = [out_train_seq, out_train_terms, out_term_counts, out_term_priors, out_train_taxa]\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    expected += [out_test_seq, out_test_taxa]\n",
    "missing = [p for p in expected if not p.exists()]\n",
    "if not missing:\n",
    "    print(\"Parsed artefacts already exist; skipping Phase 1 writes.\")\n",
    "else:\n",
    "    # ------------------------------------------\n",
    "    # A. Parse FASTA to Feather\n",
    "    # ------------------------------------------\n",
    "    def parse_fasta(path: Path) -> pd.DataFrame:\n",
    "        ids, seqs = [], []\n",
    "        cur_id, cur_seq = None, []\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    if cur_id:\n",
    "                        ids.append(cur_id)\n",
    "                        seqs.append(''.join(cur_seq))\n",
    "                    cur_id = line[1:].split()[0]\n",
    "                    cur_seq = []\n",
    "                else:\n",
    "                    cur_seq.append(line)\n",
    "            if cur_id:\n",
    "                ids.append(cur_id)\n",
    "                seqs.append(''.join(cur_seq))\n",
    "        return pd.DataFrame({'id': ids, 'sequence': seqs})\n",
    "    print(\"Parsing FASTA...\")\n",
    "    parse_fasta(PATH_TRAIN_FASTA).to_feather(out_train_seq)\n",
    "    if PATH_TEST_FASTA.exists():\n",
    "        parse_fasta(PATH_TEST_FASTA).to_feather(out_test_seq)\n",
    "    print(\"FASTA parsed and saved to artefacts.\")\n",
    "    # ------------------------------------------\n",
    "    # C. Process Terms & Priors\n",
    "    # ------------------------------------------\n",
    "    terms = pd.read_csv(PATH_TRAIN_TERMS, sep='\\t')\n",
    "    col_term = terms.columns[1]\n",
    "    terms['aspect'] = terms[col_term].map(lambda x: go_namespaces.get(x, 'UNK'))\n",
    "    # Plot Aspects\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    terms['aspect'].value_counts().plot(kind='bar', title='Annotations by Namespace')\n",
    "    plt.show()\n",
    "    # Save Priors\n",
    "    priors = (terms[col_term].value_counts() / terms.iloc[:, 0].nunique()).reset_index()\n",
    "    priors.columns = ['term', 'prior']\n",
    "    if PATH_IA.exists():\n",
    "        ia = pd.read_csv(PATH_IA, sep='\\t', names=['term', 'ia'])\n",
    "        priors = priors.merge(ia, on='term', how='left').fillna(0)\n",
    "    priors.to_parquet(out_term_priors)\n",
    "    print(\"Terms processed and priors saved.\")\n",
    "    # ------------------------------------------\n",
    "    # D. Process Taxonomy\n",
    "    # ------------------------------------------\n",
    "    print(\"Processing Taxonomy...\")\n",
    "    # Train Taxonomy\n",
    "    tax_train = pd.read_csv(PATH_TRAIN_TAXON, sep='\\t', header=None, names=['id', 'taxon_id'])\n",
    "    tax_train['taxon_id'] = tax_train['taxon_id'].astype(int)\n",
    "    tax_train.to_feather(out_train_taxa)\n",
    "    # Test Taxonomy (Extract from FASTA headers)\n",
    "    if PATH_TEST_FASTA.exists():\n",
    "        ids, taxons = [], []\n",
    "        with PATH_TEST_FASTA.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    parts = line[1:].split()\n",
    "                    ids.append(parts[0])\n",
    "                    # Assume second part is taxon if present\n",
    "                    if len(parts) > 1:\n",
    "                        try:\n",
    "                            taxons.append(int(parts[1]))\n",
    "                        except ValueError:\n",
    "                            taxons.append(0)\n",
    "                    else:\n",
    "                        taxons.append(0)\n",
    "        tax_test = pd.DataFrame({'id': ids, 'taxon_id': taxons})\n",
    "        tax_test.to_feather(out_test_taxa)\n",
    "        print(f\"Taxonomy processed. Train: {len(tax_train)}, Test: {len(tax_test)}\")\n",
    "    else:\n",
    "        print(f\"Taxonomy processed. Train: {len(tax_train)}\")\n",
    "    # ------------------------------------------\n",
    "    # E. Save Targets & Term List\n",
    "    # ------------------------------------------\n",
    "    print(\"Saving Targets & Term List...\")\n",
    "    # Save full terms list (long format)\n",
    "    terms.to_parquet(out_train_terms)\n",
    "    # Save unique term list with counts\n",
    "    term_counts = terms['term'].value_counts().reset_index()\n",
    "    term_counts.columns = ['term', 'count']\n",
    "    term_counts.to_parquet(out_term_counts)\n",
    "    print(\"Targets saved.\")\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.maybe_push('stage_01_parsed', [p for p in expected if p.exists()], note='parsed FASTA/taxa/terms/priors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2843d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db2843d4",
    "language": "python",
    "outputId": "3bbf582f-6c88-46d9-dcff-4cf30386fc99"
   },
   "outputs": [],
   "source": [
    "# CELL 05 - Inline: EntryID->text corpus builder\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _norm_uniprot_accession(raw_id: str) -> str:\n",
    "    s = str(raw_id).strip()\n",
    "    parts = s.split(\"|\")\n",
    "    # Common UniProt FASTA header: sp|P12345|NAME_HUMAN\n",
    "    if len(parts) >= 2 and parts[0] in {\"sp\", \"tr\"}:\n",
    "        return parts[1]\n",
    "    if len(parts) >= 3:\n",
    "        return parts[1]\n",
    "    return s\n",
    "def _detect_id_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [\"id\", \"EntryID\", \"entry_id\", \"accession\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return df.columns[0]\n",
    "def _read_ids(feather_path: Path) -> list[str]:\n",
    "    df = pd.read_feather(feather_path)\n",
    "    col = _detect_id_col(df)\n",
    "    return df[col].astype(str).tolist()\n",
    "def _strip_go_leakage(text: str) -> str:\n",
    "    # Remove explicit GO identifiers and obvious \"GO:\" tokens.\n",
    "    text = re.sub(r\"\\bGO:\\d{7}\\b\", \" \", text)\n",
    "    text = text.replace(\"GO:\", \" \")\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "def _make_session(total_retries: int = 5) -> requests.Session:\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    retry = Retry(\n",
    "        total=total_retries,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    s = requests.Session()\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    return s\n",
    "@dataclass\n",
    "\n",
    "class UniProtRow:\n",
    "    accession: str\n",
    "    protein_name: str\n",
    "    organism_name: str\n",
    "    keywords: str\n",
    "    protein_families: str\n",
    "    cc_function: str\n",
    "    cc_subcellular_location: str\n",
    "    pubmed_ids: list[str]\n",
    "def fetch_uniprot_tsv(\n",
    "    session: requests.Session,\n",
    "    accessions: list[str],\n",
    "    sleep_s: float,\n",
    "    fields: str,\n",
    ") -> dict[str, UniProtRow]:\n",
    "    \"\"\"Fetch UniProt rows for a small batch of accessions using the search endpoint (TSV).\"\"\"\n",
    "    # Query is a disjunction of accessions.\n",
    "    # Keep the batch modest to avoid URL length limits.\n",
    "    query = \" OR \".join([f\"accession:{a}\" for a in accessions])\n",
    "    url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "    params = {\n",
    "        \"query\": f\"({query})\",\n",
    "        \"format\": \"tsv\",\n",
    "        \"fields\": fields,\n",
    "        \"size\": 500,\n",
    "    }\n",
    "    resp = session.get(url, params=params, timeout=60)\n",
    "    # UniProt will return HTTP 400 for overly long/complex queries (often URL-length related).\n",
    "    # Fall back to a recursive split so large runs complete reliably.\n",
    "    if resp.status_code in {400, 414} and len(accessions) > 1:\n",
    "        mid = len(accessions) // 2\n",
    "        left = fetch_uniprot_tsv(session, accessions[:mid], sleep_s=sleep_s, fields=fields)\n",
    "        right = fetch_uniprot_tsv(session, accessions[mid:], sleep_s=sleep_s, fields=fields)\n",
    "        left.update(right)\n",
    "        return left\n",
    "    try:\n",
    "        resp.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        detail = (resp.text or '')[:1000] if resp is not None else ''\n",
    "        raise requests.HTTPError(f'{e} :: UniProt response: {detail}') from e\n",
    "    if sleep_s:\n",
    "        time.sleep(sleep_s)\n",
    "    lines = resp.text.splitlines()\n",
    "    if not lines:\n",
    "        return {}\n",
    "    reader = csv.DictReader(lines, delimiter=\"\\t\")\n",
    "    out: dict[str, UniProtRow] = {}\n",
    "    for row in reader:\n",
    "        acc = (row.get(\"Entry\") or row.get(\"accession\") or \"\").strip()\n",
    "        if not acc:\n",
    "            continue\n",
    "        def _get(k: str) -> str:\n",
    "            v = row.get(k)\n",
    "            return str(v).strip() if v is not None else \"\"\n",
    "        pmids_raw = (_get(\"PubMed ID\") or _get(\"lit_pubmed_id\")).strip()\n",
    "        pmids: list[str] = []\n",
    "        if pmids_raw:\n",
    "            pmids = [p.strip() for p in re.split(r\"[;\\s]+\", pmids_raw) if p.strip().isdigit()]\n",
    "        out[acc] = UniProtRow(\n",
    "            accession=acc,\n",
    "            protein_name=_get(\"Protein names\"),\n",
    "            organism_name=_get(\"Organism\"),\n",
    "            keywords=_get(\"Keywords\"),\n",
    "            protein_families=_get(\"Protein families\"),\n",
    "            cc_function=_get(\"Function [CC]\"),\n",
    "            cc_subcellular_location=_get(\"Subcellular location [CC]\"),\n",
    "            pubmed_ids=pmids,\n",
    "        )\n",
    "    return out\n",
    "def fetch_pubmed_abstracts(\n",
    "    session: requests.Session,\n",
    "    pmids: list[str],\n",
    "    sleep_s: float,\n",
    "    email: str,\n",
    "    api_key: str,\n",
    "    _depth: int = 0,\n",
    ") -> dict[str, tuple[str, str]]:\n",
    "    \"\"\"Fetch PubMed title+abstract via NCBI E-utilities (XML).\"\"\"\n",
    "    if not pmids:\n",
    "        return {}\n",
    "    pmids = [str(p).strip() for p in pmids if str(p).strip().isdigit()]\n",
    "    if not pmids:\n",
    "        return {}\n",
    "    pmids = list(dict.fromkeys(pmids))\n",
    "    def _looks_like_html(text: str) -> bool:\n",
    "        t = (text or \"\").lstrip().lower()\n",
    "        return t.startswith(\"<!doctype html\") or t.startswith(\"<html\") or \"<html\" in t[:2000]\n",
    "    def _sanitize_xml(text: str) -> str:\n",
    "        # Remove illegal XML control chars (keep \\t \\n \\r).\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\", \" \", text or \"\")\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": \",\".join(pmids),\n",
    "        \"retmode\": \"xml\",\n",
    "    }\n",
    "    if email:\n",
    "        params[\"email\"] = email\n",
    "    if api_key:\n",
    "        params[\"api_key\"] = api_key\n",
    "    root = None\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = session.get(url, params=params, timeout=60)\n",
    "            if resp.status_code in {429, 500, 502, 503, 504}:\n",
    "                raise requests.HTTPError(f\"PubMed transient HTTP {resp.status_code}\")\n",
    "            resp.raise_for_status()\n",
    "            if sleep_s:\n",
    "                time.sleep(sleep_s)\n",
    "            text = resp.text or \"\"\n",
    "            if _looks_like_html(text):\n",
    "                snippet = text[:300].replace(\"\\n\", \" \" )\n",
    "                raise ValueError(f\"PubMed returned HTML instead of XML: {snippet}\")\n",
    "            text = _sanitize_xml(text)\n",
    "            root = ET.fromstring(text)\n",
    "            break\n",
    "        except Exception:\n",
    "            if attempt < 2:\n",
    "                time.sleep(1.5 * (attempt + 1))\n",
    "            continue\n",
    "    if root is None:\n",
    "        if len(pmids) > 1 and _depth < 10:\n",
    "            mid = len(pmids) // 2\n",
    "            left = fetch_pubmed_abstracts(session, pmids[:mid], sleep_s=sleep_s, email=email, api_key=api_key, _depth=_depth + 1)\n",
    "            right = fetch_pubmed_abstracts(session, pmids[mid:], sleep_s=sleep_s, email=email, api_key=api_key, _depth=_depth + 1)\n",
    "            left.update(right)\n",
    "            return left\n",
    "        return {}\n",
    "    out: dict[str, tuple[str, str]] = {}\n",
    "    for article in root.findall(\".//PubmedArticle\"):\n",
    "        pmid_el = article.find(\".//PMID\")\n",
    "        if pmid_el is None or pmid_el.text is None:\n",
    "            continue\n",
    "        pmid = pmid_el.text.strip()\n",
    "        title_el = article.find(\".//ArticleTitle\")\n",
    "        abs_el = article.find(\".//Abstract\")\n",
    "        title = (\"\" if title_el is None or title_el.text is None else title_el.text.strip())\n",
    "        abstract = \"\"\n",
    "        if abs_el is not None:\n",
    "            parts = []\n",
    "            for t in abs_el.findall(\".//AbstractText\"):\n",
    "                if t.text:\n",
    "                    parts.append(t.text.strip())\n",
    "            abstract = \" \".join(parts).strip()\n",
    "        out[pmid] = (title, abstract)\n",
    "    return out\n",
    "def main() -> int:\n",
    "    p = argparse.ArgumentParser(description=\"Build EntryID->text corpus from UniProt (fields) + PubMed abstracts for --mode text.\")\n",
    "    p.add_argument(\"--artefacts-dir\", default=Path(\"artefacts_local\") / \"artefacts\", type=Path, help=\"Artefacts root\")\n",
    "    p.add_argument(\"--out-path\", default=Path(\"artefacts_local\") / \"artefacts\" / \"external\" / \"entryid_text.tsv\", type=Path, help=\"Output TSV path (default: artefacts_local/artefacts/external/entryid_text.tsv)\")\n",
    "    p.add_argument(\"--cache-dir\", default=Path(\"artefacts_local\") / \"artefacts\" / \"external\" / \"uniprot_pubmed_cache\", type=Path, help=\"Cache dir for UniProt+PubMed lookups\")\n",
    "    p.add_argument(\"--max-ids\", type=int, default=0, help=\"If >0, cap number of proteins (debug)\")\n",
    "    p.add_argument(\"--uniprot-batch-size\", type=int, default=80)\n",
    "    p.add_argument(\"--pubmed-batch-size\", type=int, default=50)\n",
    "    p.add_argument(\"--max-pubmed-per-protein\", type=int, default=3)\n",
    "    p.add_argument(\"--max-abstract-chars\", type=int, default=2000)\n",
    "    p.add_argument(\"--sleep-uniprot\", type=float, default=0.1)\n",
    "    p.add_argument(\"--sleep-pubmed\", type=float, default=0.34)\n",
    "    p.add_argument(\"--email\", type=str, default=os.environ.get(\"NCBI_EMAIL\", \"\"))\n",
    "    p.add_argument(\"--api-key\", type=str, default=os.environ.get(\"NCBI_API_KEY\", \"\"))\n",
    "    p.add_argument(\"--strip-go\", action=\"store_true\")\n",
    "    args = p.parse_args()\n",
    "    args.artefacts_dir = Path(args.artefacts_dir)\n",
    "    args.out_path = Path(args.out_path)\n",
    "    args.cache_dir = Path(args.cache_dir)\n",
    "    args.out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    args.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_feather = args.artefacts_dir / \"parsed\" / \"train_seq.feather\"\n",
    "    test_feather = args.artefacts_dir / \"parsed\" / \"test_seq.feather\"\n",
    "    ids = []\n",
    "    if train_feather.exists():\n",
    "        ids.extend(_read_ids(train_feather))\n",
    "    if test_feather.exists():\n",
    "        ids.extend(_read_ids(test_feather))\n",
    "    ids = [_norm_uniprot_accession(i) for i in ids]\n",
    "    uniq = sorted(set(ids))\n",
    "    if args.max_ids and args.max_ids > 0:\n",
    "        uniq = uniq[: int(args.max_ids)]\n",
    "    print(f\"Unique accessions: {len(uniq)}\")\n",
    "    fields = \",\".join(\n",
    "        [\n",
    "            \"accession\",\n",
    "            \"protein_name\",\n",
    "            \"organism_name\",\n",
    "            \"keyword\",\n",
    "            \"protein_families\",\n",
    "            \"cc_function\",\n",
    "            \"cc_subcellular_location\",\n",
    "            \"lit_pubmed_id\",\n",
    "        ]\n",
    "    )\n",
    "    session = _make_session()\n",
    "    # UniProt rows cache\n",
    "    uniprot_cache = args.cache_dir / \"uniprot_rows.tsv\"\n",
    "    uniprot_rows: dict[str, UniProtRow] = {}\n",
    "    if uniprot_cache.exists():\n",
    "        print(f\"Loading UniProt cache: {uniprot_cache}\")\n",
    "        df_u = pd.read_csv(uniprot_cache, sep=\"\\t\", dtype=str)\n",
    "        for _, r in df_u.iterrows():\n",
    "            acc = str(r[\"accession\"])\n",
    "            pmids = [p for p in str(r.get(\"pubmed_ids\", \"\")).split(\";\") if p]\n",
    "            uniprot_rows[acc] = UniProtRow(\n",
    "                accession=acc,\n",
    "                protein_name=str(r.get(\"protein_name\", \"\")) if r.get(\"protein_name\") is not None else \"\",\n",
    "                organism_name=str(r.get(\"organism_name\", \"\")) if r.get(\"organism_name\") is not None else \"\",\n",
    "                keywords=str(r.get(\"keywords\", \"\")) if r.get(\"keywords\") is not None else \"\",\n",
    "                protein_families=str(r.get(\"protein_families\", \"\")) if r.get(\"protein_families\") is not None else \"\",\n",
    "                cc_function=str(r.get(\"cc_function\", \"\")) if r.get(\"cc_function\") is not None else \"\",\n",
    "                cc_subcellular_location=str(r.get(\"cc_subcellular_location\", \"\")) if r.get(\"cc_subcellular_location\") is not None else \"\",\n",
    "                pubmed_ids=pmids,\n",
    "            )\n",
    "    missing = [a for a in uniq if a not in uniprot_rows]\n",
    "    if missing:\n",
    "        print(f\"Fetching UniProt rows (missing={len(missing)})...\")\n",
    "        write_header = not uniprot_cache.exists()\n",
    "        with uniprot_cache.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f,\n",
    "                delimiter=\"\\t\",\n",
    "                fieldnames=[\n",
    "                    \"accession\",\n",
    "                    \"protein_name\",\n",
    "                    \"organism_name\",\n",
    "                    \"keywords\",\n",
    "                    \"protein_families\",\n",
    "                    \"cc_function\",\n",
    "                    \"cc_subcellular_location\",\n",
    "                    \"pubmed_ids\",\n",
    "                ],\n",
    "            )\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            for i in tqdm(range(0, len(missing), args.uniprot_batch_size), desc=\"UniProt\"):\n",
    "                batch = missing[i : i + args.uniprot_batch_size]\n",
    "                got = fetch_uniprot_tsv(\n",
    "                    session=session,\n",
    "                    accessions=batch,\n",
    "                    sleep_s=args.sleep_uniprot,\n",
    "                    fields=fields,\n",
    "                )\n",
    "                for acc, r in got.items():\n",
    "                    uniprot_rows[acc] = r\n",
    "                    writer.writerow(\n",
    "                        {\n",
    "                            \"accession\": r.accession,\n",
    "                            \"protein_name\": r.protein_name,\n",
    "                            \"organism_name\": r.organism_name,\n",
    "                            \"keywords\": r.keywords,\n",
    "                            \"protein_families\": r.protein_families,\n",
    "                            \"cc_function\": r.cc_function,\n",
    "                            \"cc_subcellular_location\": r.cc_subcellular_location,\n",
    "                            \"pubmed_ids\": \";\".join(r.pubmed_ids),\n",
    "                        }\n",
    "                    )\n",
    "    # PubMed cache\n",
    "    pubmed_cache = args.cache_dir / \"pubmed_abstracts.tsv\"\n",
    "    pmid_sets: dict[str, list[str]] = {}\n",
    "    for acc, r in uniprot_rows.items():\n",
    "        pmids = [p for p in r.pubmed_ids if p]\n",
    "        if args.max_pubmed_per_protein and len(pmids) > args.max_pubmed_per_protein:\n",
    "            pmids = pmids[: int(args.max_pubmed_per_protein)]\n",
    "        pmid_sets[acc] = pmids\n",
    "    pmids_uniq = sorted({p for ps in pmid_sets.values() for p in ps})\n",
    "    pubmed_map: dict[str, tuple[str, str]] = {}\n",
    "    if pubmed_cache.exists():\n",
    "        print(f\"Loading PubMed cache: {pubmed_cache}\")\n",
    "        df_p = pd.read_csv(pubmed_cache, sep=\"\\t\", dtype=str)\n",
    "        for _, r in df_p.iterrows():\n",
    "            pmid = str(r[\"pmid\"])\n",
    "            pubmed_map[pmid] = (\n",
    "                str(r.get(\"title\", \"\")) if r.get(\"title\") is not None else \"\",\n",
    "                str(r.get(\"abstract\", \"\")) if r.get(\"abstract\") is not None else \"\",\n",
    "            )\n",
    "    missing_pmids = [p for p in pmids_uniq if p not in pubmed_map]\n",
    "    if missing_pmids:\n",
    "        print(f\"Fetching PubMed abstracts (missing={len(missing_pmids)})...\")\n",
    "        write_header = not pubmed_cache.exists()\n",
    "        with pubmed_cache.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=[\"pmid\", \"title\", \"abstract\"])\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            for i in tqdm(range(0, len(missing_pmids), args.pubmed_batch_size), desc=\"PubMed\"):\n",
    "                batch = missing_pmids[i : i + args.pubmed_batch_size]\n",
    "                got = fetch_pubmed_abstracts(\n",
    "                    session=session,\n",
    "                    pmids=batch,\n",
    "                    sleep_s=args.sleep_pubmed,\n",
    "                    email=args.email,\n",
    "                    api_key=args.api_key,\n",
    "                )\n",
    "                for pmid, (title, abstract) in got.items():\n",
    "                    if args.max_abstract_chars and abstract:\n",
    "                        abstract = abstract[: int(args.max_abstract_chars)]\n",
    "                    pubmed_map[pmid] = (title, abstract)\n",
    "                    writer.writerow({\"pmid\": pmid, \"title\": title, \"abstract\": abstract})\n",
    "    # Build final EntryID -> text\n",
    "    print(\"Writing entryid_text.tsv...\")\n",
    "    with args.out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f, delimiter=\"\\t\")\n",
    "        w.writerow([\"EntryID\", \"text\"])\n",
    "        n_with_uniprot = 0\n",
    "        n_with_pubmed = 0\n",
    "        for acc in tqdm(uniq, desc=\"Assemble\"):\n",
    "            r = uniprot_rows.get(acc)\n",
    "            if r is None:\n",
    "                w.writerow([acc, \"\"])\n",
    "                continue\n",
    "            parts: list[str] = []\n",
    "            # UniProt text (short, curated)\n",
    "            uniprot_bits = [\n",
    "                r.protein_name,\n",
    "                r.organism_name,\n",
    "                r.keywords,\n",
    "                r.protein_families,\n",
    "                r.cc_function,\n",
    "                r.cc_subcellular_location,\n",
    "            ]\n",
    "            uniprot_text = \" \".join([b for b in uniprot_bits if b])\n",
    "            if uniprot_text:\n",
    "                n_with_uniprot += 1\n",
    "                parts.append(uniprot_text)\n",
    "            # PubMed abstracts (richer)\n",
    "            pmids = pmid_sets.get(acc, [])\n",
    "            abs_parts: list[str] = []\n",
    "            for pmid in pmids:\n",
    "                title, abstract = pubmed_map.get(pmid, (\"\", \"\"))\n",
    "                if title or abstract:\n",
    "                    abs_parts.append(f\"{title}. {abstract}\".strip(\" .\"))\n",
    "            if abs_parts:\n",
    "                n_with_pubmed += 1\n",
    "                parts.append(\" \".join(abs_parts))\n",
    "            text = \" \".join(parts).strip()\n",
    "            if args.strip_go and text:\n",
    "                text = _strip_go_leakage(text)\n",
    "            w.writerow([acc, text])\n",
    "    print(f\"Saved: {args.out_path}\")\n",
    "    print(f\"Coverage: UniProt text {n_with_uniprot}/{len(uniq)} = {n_with_uniprot/ max(1,len(uniq)):.3f}\")\n",
    "    print(f\"Coverage: PubMed text  {n_with_pubmed}/{len(uniq)} = {n_with_pubmed/ max(1,len(uniq)):.3f}\")\n",
    "    return 0\n",
    "# Notebook safety:\n",
    "# Jupyter/IPython runs with __name__ == '__main__', which would trigger argparse via main() and fail\n",
    "# due to extra kernel arguments (e.g. '-f <kernel.json>').\n",
    "# This cell is intentionally library-only; run the next notebook cell to execute it.\n",
    "print('EntryIDtext corpus builder loaded. Run Notebook Cell 06 to build WORK_ROOT/external/entryid_text.tsv.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8602495",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8602495",
    "language": "python",
    "outputId": "12ae1de2-2a7c-4750-9f25-285d77f641df"
   },
   "outputs": [],
   "source": [
    "# CELL 06 - Run: build EntryID->text corpus (UniProt+PubMed)\n",
    "# PRODUCES: external/entryid_text.tsv\n",
    "# Milestone checkpoint: stage_02_external_text\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print('Stage stage_02_external_text: start')\n",
    "out_path = WORK_ROOT / 'external' / 'entryid_text.tsv'\n",
    "cache_dir = CACHE_ROOT / 'uniprot_pubmed_cache'\n",
    "print(f'  out_path: {out_path} (exists={out_path.exists()})')\n",
    "if out_path.exists():\n",
    "    print('  out_path already exists; skipping rebuild')\n",
    "else:\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sys.argv = [\n",
    "        '03_build_entryid_text_from_uniprot_pubmed.py',\n",
    "        '--artefacts-dir', str(WORK_ROOT),\n",
    "        '--out-path', str(out_path),\n",
    "        '--cache-dir', str(cache_dir),\n",
    "        '--max-ids', '0',\n",
    "        '--uniprot-batch-size', '80',\n",
    "        '--max-pubmed-per-protein', '3',\n",
    "        '--strip-go',\n",
    "        '--sleep-uniprot', '0.1',\n",
    "        '--sleep-pubmed', '0.34',\n",
    "    ]\n",
    "    _ = main()\n",
    "if out_path.exists():\n",
    "    STORE.maybe_push('stage_02_external_text', [out_path], note='entryid->text corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8a1fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28c8a1fc",
    "language": "python",
    "outputId": "4f497fcd-8a6b-4070-bc7a-c91b8bc9972c"
   },
   "outputs": [],
   "source": [
    "# CELL 07 - Inline: embeddings generator (used by TF-IDF; also supports extras)\n",
    "print('Cell 7: embeddings generator loaded')\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def _assert_all_finite_np(arr: np.ndarray, *, name: str) -> None:\n",
    "    if not np.isfinite(arr).all():\n",
    "        nan = int(np.isnan(arr).sum())\n",
    "        inf = int(np.isinf(arr).sum())\n",
    "        raise ValueError(f'{name}: non-finite values detected (nan={nan}, inf={inf}). Refusing to save.')\n",
    "\n",
    "def _read_sequences(feather_path: Path) -> tuple[list[str], list[str]]:\n",
    "    df = pd.read_feather(feather_path)\n",
    "    if \"id\" not in df.columns or \"sequence\" not in df.columns:\n",
    "        raise ValueError(f\"Expected columns id, sequence in {feather_path}; got {list(df.columns)}\")\n",
    "    ids = df[\"id\"].astype(str).tolist()\n",
    "    seqs = df[\"sequence\"].astype(str).tolist()\n",
    "    return ids, seqs\n",
    "def _smart_order(seqs: list[str]) -> np.ndarray:\n",
    "    lengths = np.fromiter((len(s) for s in seqs), dtype=np.int64)\n",
    "    return np.argsort(lengths)[::-1]\n",
    "def _mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    # last_hidden_state: (B, L, D), attention_mask: (B, L)\n",
    "    mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "    x = last_hidden_state * mask\n",
    "    denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "    return x.sum(dim=1) / denom\n",
    "@torch.no_grad()\n",
    "def embed_esm2(\n",
    "    seqs: list[str],\n",
    "    model_name: str,\n",
    "    batch_size: int,\n",
    "    max_len: int,\n",
    "    device: torch.device,\n",
    ") -> np.ndarray:\n",
    "    from transformers import EsmModel, EsmTokenizer\n",
    "    tok = EsmTokenizer.from_pretrained(model_name)\n",
    "    model = EsmModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    order = _smart_order(seqs)\n",
    "    seqs_sorted = [seqs[i] for i in order]\n",
    "    outs: list[np.ndarray] = []\n",
    "    use_amp = device.type == \"cuda\"\n",
    "    for i in range(0, len(seqs_sorted), batch_size):\n",
    "        batch = seqs_sorted[i : i + batch_size]\n",
    "        ids = tok.batch_encode_plus(\n",
    "            batch,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        ids = {k: v.to(device) for k, v in ids.items()}\n",
    "        ctx = torch.amp.autocast(\"cuda\") if use_amp else torch.autocast(\"cpu\", enabled=False)\n",
    "        with ctx:\n",
    "            out = model(**ids)\n",
    "        pooled = _mean_pool(out.last_hidden_state.float(), ids[\"attention_mask\"])\n",
    "        outs.append(pooled.cpu().numpy())\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    embs_sorted = np.vstack(outs).astype(np.float32)\n",
    "    embs = np.zeros_like(embs_sorted)\n",
    "    embs[order] = embs_sorted\n",
    "    return embs\n",
    "@torch.no_grad()\n",
    "def embed_ankh(\n",
    "    seqs: list[str],\n",
    "    model_name: str,\n",
    "    batch_size: int,\n",
    "    max_len: int,\n",
    "    device: torch.device,\n",
    "    trust_remote_code: bool,\n",
    ") -> np.ndarray:\n",
    "    # Ankh models are HuggingFace-hosted and may require trust_remote_code.\n",
    "    # We treat them as encoder-style models and mean-pool hidden states over attention_mask.\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=trust_remote_code).to(device)\n",
    "    model.eval()\n",
    "    order = _smart_order(seqs)\n",
    "    seqs_sorted = [seqs[i] for i in order]\n",
    "    outs: list[np.ndarray] = []\n",
    "    amp_enabled = device.type == \"cuda\"\n",
    "    for i in range(0, len(seqs_sorted), batch_size):\n",
    "        batch = seqs_sorted[i : i + batch_size]\n",
    "        # Many protein LMs expect space-separated amino acids; if the tokenizer has a small vocab,\n",
    "        # this typically helps. If it hurts, you can disable via --ankh-space-sep 0.\n",
    "        batch = [\" \".join(list(s.replace(\"U\", \"X\").replace(\"Z\", \"X\").replace(\"O\", \"X\").replace(\"B\", \"X\"))) for s in batch]\n",
    "        ids = tok(\n",
    "            batch,\n",
    "            add_special_tokens=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        ids = {k: v.to(device) for k, v in ids.items()}\n",
    "        if \"attention_mask\" not in ids:\n",
    "            raise RuntimeError(f\"Tokenizer for {model_name} did not return attention_mask\")\n",
    "        if (ids[\"attention_mask\"].sum(dim=1) == 0).any():\n",
    "            raise RuntimeError(f\"Ankh tokenisation produced an all-zero attention_mask (batch_start={i}).\")\n",
    "        def _forward(use_amp: bool):\n",
    "            ctx = (\n",
    "                torch.amp.autocast(\"cuda\")\n",
    "                if (use_amp and device.type == \"cuda\")\n",
    "                else torch.autocast(\"cpu\", enabled=False)\n",
    "            )\n",
    "            with ctx:\n",
    "                return model(**ids)\n",
    "        out = _forward(amp_enabled)\n",
    "        # HuggingFace convention: encoder outputs `last_hidden_state`\n",
    "        last = getattr(out, \"last_hidden_state\", None)\n",
    "        if last is None:\n",
    "            raise RuntimeError(f\"Model {model_name} did not return last_hidden_state\")\n",
    "        pooled = _mean_pool(last.float(), ids[\"attention_mask\"])\n",
    "        if not bool(torch.isfinite(pooled).all().item()):\n",
    "            if amp_enabled:\n",
    "                print(f\"WARNING: Non-finite Ankh embeddings under AMP at batch_start={i}; retrying without AMP.\")\n",
    "                amp_enabled = False\n",
    "                out = _forward(False)\n",
    "                last = getattr(out, \"last_hidden_state\", None)\n",
    "                if last is None:\n",
    "                    raise RuntimeError(f\"Model {model_name} did not return last_hidden_state\")\n",
    "                pooled = _mean_pool(last.float(), ids[\"attention_mask\"])\n",
    "            if not bool(torch.isfinite(pooled).all().item()):\n",
    "                raise RuntimeError(f\"Ankh produced non-finite embeddings even without AMP (batch_start={i}).\")\n",
    "        outs.append(pooled.cpu().numpy())\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    embs_sorted = np.vstack(outs).astype(np.float32)\n",
    "    _assert_all_finite_np(embs_sorted, name=\"ankh_embeds\")\n",
    "    embs = np.zeros_like(embs_sorted)\n",
    "    embs[order] = embs_sorted\n",
    "    return embs\n",
    "def main() -> int:\n",
    "    ap = argparse.ArgumentParser(description=\"Generate optional multimodal embeddings as .npy artefacts.\")\n",
    "    ap.add_argument(\n",
    "        \"--artefacts-dir\",\n",
    "        type=Path,\n",
    "        default=Path(\"artefacts_local\") / \"artefacts\",\n",
    "        help=\"Path containing parsed/ and features/ (default: artefacts_local/artefacts)\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--mode\",\n",
    "        choices=[\"esm2_3b\", \"ankh\", \"text\"],\n",
    "        required=True,\n",
    "        help=\"Which optional embedding to generate.\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=\"cuda\",\n",
    "        help=\"cuda|cpu (default: cuda)\",\n",
    "    )\n",
    "    ap.add_argument(\"--batch-size\", type=int, default=2)\n",
    "    ap.add_argument(\"--max-len\", type=int, default=1024)\n",
    "    ap.add_argument(\n",
    "        \"--esm2-3b-model\",\n",
    "        type=str,\n",
    "        default=\"facebook/esm2_t36_3B_UR50D\",\n",
    "        help=\"HF model id for ESM2-3B\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--ankh-model\",\n",
    "        type=str,\n",
    "        default=\"ElnaggarLab/ankh-large\",\n",
    "        help=\"HF model id for Ankh (large is typically 1536D)\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--trust-remote-code\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pass trust_remote_code=True for Ankh loading (often required).\",\n",
    "    )\n",
    "    # Text mode (fixed-width sparse->dense features; default dimension = 10279)\n",
    "    ap.add_argument(\n",
    "        \"--text-path\",\n",
    "        type=Path,\n",
    "        default=None,\n",
    "        help=\"Path to a 2-column TSV/CSV with EntryID and text (required for --mode text).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-sep\",\n",
    "        type=str,\n",
    "        default=\"\\t\",\n",
    "        help=\"Separator for --text-path (default: tab).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-id-col\",\n",
    "        type=str,\n",
    "        default=\"EntryID\",\n",
    "        help=\"ID column name in --text-path (default: EntryID).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-col\",\n",
    "        type=str,\n",
    "        default=\"text\",\n",
    "        help=\"Text column name in --text-path (default: text).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-dim\",\n",
    "        type=int,\n",
    "        default=10279,\n",
    "        help=\"Output feature dimension for text TF-IDF (default: 10279).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-dtype\",\n",
    "        choices=[\"float16\", \"float32\"],\n",
    "        default=\"float16\",\n",
    "        help=\"On-disk dtype for text .npy (default: float16 to keep size sane).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-ngram-max\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Max n-gram for TF-IDF (default: 2).\",\n",
    "    )\n",
    "    args = ap.parse_args()\n",
    "    artefacts_dir: Path = args.artefacts_dir\n",
    "    parsed_dir = artefacts_dir / \"parsed\"\n",
    "    feat_dir = artefacts_dir / \"features\"\n",
    "    feat_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_feather = parsed_dir / \"train_seq.feather\"\n",
    "    test_feather = parsed_dir / \"test_seq.feather\"\n",
    "    if not train_feather.exists() or not test_feather.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected {train_feather} and {test_feather}. Run Phase 1 parsing first.\"\n",
    "        )\n",
    "    # Make HF caches relocatable (handy on Colab/Kaggle)\n",
    "    os.environ.setdefault(\"HF_HOME\", str(artefacts_dir / \"hf_cache\"))\n",
    "    os.environ.setdefault(\"TRANSFORMERS_CACHE\", str(artefacts_dir / \"hf_cache\"))\n",
    "    os.environ.setdefault(\"TORCH_HOME\", str(artefacts_dir / \"torch_cache\"))\n",
    "    device = torch.device(\"cuda\" if (args.device == \"cuda\" and torch.cuda.is_available()) else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"Loading sequences...\")\n",
    "    train_ids, train_seqs = _read_sequences(train_feather)\n",
    "    test_ids, test_seqs = _read_sequences(test_feather)\n",
    "    if args.mode == \"esm2_3b\":\n",
    "        print(f\"Embedding ESM2-3B: {args.esm2_3b_model}\")\n",
    "        train_emb = embed_esm2(train_seqs, args.esm2_3b_model, args.batch_size, args.max_len, device)\n",
    "        test_emb = embed_esm2(test_seqs, args.esm2_3b_model, args.batch_size, args.max_len, device)\n",
    "        np.save(feat_dir / \"train_embeds_esm2_3b.npy\", train_emb)\n",
    "        np.save(feat_dir / \"test_embeds_esm2_3b.npy\", test_emb)\n",
    "        print(f\"Saved: {feat_dir / 'train_embeds_esm2_3b.npy'}\")\n",
    "        print(f\"Saved: {feat_dir / 'test_embeds_esm2_3b.npy'}\")\n",
    "        return 0\n",
    "    if args.mode == \"ankh\":\n",
    "        print(f\"Embedding Ankh: {args.ankh_model}\")\n",
    "        train_emb = embed_ankh(\n",
    "            train_seqs,\n",
    "            args.ankh_model,\n",
    "            args.batch_size,\n",
    "            args.max_len,\n",
    "            device,\n",
    "            trust_remote_code=args.trust_remote_code,\n",
    "        )\n",
    "        test_emb = embed_ankh(\n",
    "            test_seqs,\n",
    "            args.ankh_model,\n",
    "            args.batch_size,\n",
    "            args.max_len,\n",
    "            device,\n",
    "            trust_remote_code=args.trust_remote_code,\n",
    "        )\n",
    "        np.save(feat_dir / \"train_embeds_ankh.npy\", train_emb)\n",
    "        np.save(feat_dir / \"test_embeds_ankh.npy\", test_emb)\n",
    "        print(f\"Saved: {feat_dir / 'train_embeds_ankh.npy'}\")\n",
    "        print(f\"Saved: {feat_dir / 'test_embeds_ankh.npy'}\")\n",
    "        return 0\n",
    "    if args.mode == \"text\":\n",
    "        if args.text_path is None:\n",
    "            raise ValueError(\"--text-path is required for --mode text\")\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        import joblib\n",
    "        if not args.text_path.exists():\n",
    "            raise FileNotFoundError(f\"text-path not found: {args.text_path}\")\n",
    "        print(f\"Loading text corpus: {args.text_path}\")\n",
    "        df = pd.read_csv(args.text_path, sep=args.text_sep, dtype=str)\n",
    "        if args.text_id_col not in df.columns or args.text_col not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"Expected columns {args.text_id_col!r}, {args.text_col!r} in {args.text_path}; got {list(df.columns)}\"\n",
    "            )\n",
    "        # Many-to-one mapping is possible (multiple pubs per EntryID). Concatenate.\n",
    "        df = df[[args.text_id_col, args.text_col]].dropna()\n",
    "        df[args.text_id_col] = df[args.text_id_col].astype(str)\n",
    "        df[args.text_col] = df[args.text_col].astype(str)\n",
    "        grouped = df.groupby(args.text_id_col, sort=False)[args.text_col].apply(lambda x: \" \\n \".join(x.tolist()))\n",
    "        text_map = grouped.to_dict()\n",
    "        def _norm_uniprot_id(pid: str) -> str:\n",
    "            # Common UniProt FASTA headers: sp|P12345|NAME_HUMAN ...\n",
    "            parts = str(pid).split(\"|\")\n",
    "            if len(parts) >= 2 and parts[0] in {\"sp\", \"tr\"}:\n",
    "                return parts[1]\n",
    "            if len(parts) >= 3:\n",
    "                return parts[1]\n",
    "            return str(pid)\n",
    "        # Support both raw IDs and normalised accessions for joining.\n",
    "        text_map_norm: dict[str, str] = {}\n",
    "        for k, v in text_map.items():\n",
    "            text_map_norm[str(k)] = v\n",
    "            text_map_norm[_norm_uniprot_id(k)] = v\n",
    "        train_texts = [text_map_norm.get(str(pid), \"\") for pid in train_ids]\n",
    "        test_texts = [text_map_norm.get(str(pid), \"\") for pid in test_ids]\n",
    "        # TF-IDF gives a fixed-width, high-dimensional text modality with controllable size.\n",
    "        # This is the most practical way to realise a 10279D \"text embedding\" without a bespoke LLM pipeline.\n",
    "        print(f\"Fitting TF-IDF (dim={args.text_dim}, ngram<= {args.text_ngram_max})...\")\n",
    "        vec = TfidfVectorizer(\n",
    "            max_features=args.text_dim,\n",
    "            ngram_range=(1, max(1, int(args.text_ngram_max))),\n",
    "            min_df=2,\n",
    "            strip_accents=\"unicode\",\n",
    "            lowercase=True,\n",
    "        )\n",
    "        vec.fit(train_texts + test_texts)\n",
    "        n_features = len(vec.get_feature_names_out())\n",
    "        print(f\"TF-IDF vocab size: {n_features} (padded to {args.text_dim})\")\n",
    "        # Persist vectorizer for reproducibility\n",
    "        joblib.dump(vec, feat_dir / \"text_vectorizer.joblib\")\n",
    "        print(f\"Saved: {feat_dir / 'text_vectorizer.joblib'}\")\n",
    "        out_dtype = np.float16 if args.text_dtype == \"float16\" else np.float32\n",
    "        def _write_memmap(name: str, texts: list[str]):\n",
    "            path = feat_dir / name\n",
    "            mm = np.lib.format.open_memmap(path, mode=\"w+\", dtype=out_dtype, shape=(len(texts), args.text_dim))\n",
    "            bs = 2048\n",
    "            for i in range(0, len(texts), bs):\n",
    "                chunk = texts[i : i + bs]\n",
    "                X = vec.transform(chunk)  # sparse\n",
    "                # Convert per-chunk to dense to write to .npy\n",
    "                arr = X.toarray().astype(out_dtype, copy=False)\n",
    "                if arr.shape[1] == args.text_dim:\n",
    "                    mm[i : i + arr.shape[0], :] = arr\n",
    "                else:\n",
    "                    dense = np.zeros((arr.shape[0], args.text_dim), dtype=out_dtype)\n",
    "                    if arr.shape[1] > 0:\n",
    "                        dense[:, : arr.shape[1]] = arr\n",
    "                    mm[i : i + dense.shape[0], :] = dense\n",
    "            mm.flush()\n",
    "            return path\n",
    "        train_path = _write_memmap(\"train_embeds_text.npy\", train_texts)\n",
    "        test_path = _write_memmap(\"test_embeds_text.npy\", test_texts)\n",
    "        print(f\"Saved: {train_path}\")\n",
    "        print(f\"Saved: {test_path}\")\n",
    "        return 0\n",
    "    raise RuntimeError(\"unreachable\")\n",
    "# Notebook safety:\n",
    "# This cell is intentionally library-only (no auto-run).\n",
    "# Jupyter passes extra kernel args (e.g. '-f <kernel.json>') that break argparse-based mains.\n",
    "# Use the dedicated run cell that sets sys.argv explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8997aea9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "8997aea9",
    "language": "python",
    "outputId": "9396c63c-81e3-478e-86b0-24a6c91f503d"
   },
   "outputs": [],
   "source": [
    "# CELL 08 - Run: TF-IDF embeddings (10279D)\n",
    "# PRODUCES: features/text_vectorizer.joblib, features/train_embeds_text.npy, features/test_embeds_text.npy\n",
    "# Milestone checkpoint: stage_03_tfidf_text\n",
    "# Option B strictness: TF-IDF is required, but we allow building it in-place when missing.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "STRICT_OPTION_B = True\n",
    "USE_TFIDF_TEXT = True\n",
    "\n",
    "feat_dir = WORK_ROOT / 'features'\n",
    "text_vect = feat_dir / 'text_vectorizer.joblib'\n",
    "train_text_npy = feat_dir / 'train_embeds_text.npy'\n",
    "test_text_npy = feat_dir / 'test_embeds_text.npy'\n",
    "\n",
    "generated = False\n",
    "if USE_TFIDF_TEXT:\n",
    "    have_all = text_vect.exists() and train_text_npy.exists() and test_text_npy.exists()\n",
    "    if have_all:\n",
    "        print('TF-IDF artefacts already exist; skipping generation')\n",
    "    else:\n",
    "        text_path = WORK_ROOT / 'external' / 'entryid_text.tsv'\n",
    "        if not text_path.exists():\n",
    "            raise FileNotFoundError(f'Missing corpus file: {text_path}. Run the corpus milestone first.')\n",
    "        sys.argv = [\n",
    "            '02_generate_optional_embeddings.py',\n",
    "            '--artefacts-dir', str(WORK_ROOT),\n",
    "            '--mode', 'text',\n",
    "            '--text-path', str(text_path),\n",
    "            '--text-dim', '10279',\n",
    "        ]\n",
    "        _ = main()\n",
    "        generated = True\n",
    "\n",
    "if text_vect.exists() and train_text_npy.exists() and test_text_npy.exists():\n",
    "    # Avoid creating a new HF commit every time we re-run this cell.\n",
    "    # Publish if we just generated TF-IDF, or if the user explicitly allows pushing existing artefacts.\n",
    "    if generated:\n",
    "        STORE.maybe_push('stage_03_tfidf_text', [text_vect, train_text_npy, test_text_npy], note='tf-idf 10279D')\n",
    "    else:\n",
    "        if PUSH_EXISTING_CHECKPOINTS:\n",
    "            STORE.maybe_push('stage_03_tfidf_text', [text_vect, train_text_npy, test_text_npy], note='tf-idf 10279D (existing)')\n",
    "        else:\n",
    "            print('TF-IDF present; not pushing checkpoints (PUSH_EXISTING_CHECKPOINTS=False).')\n",
    "\n",
    "    # Diagnostics: TF-IDF sparsity + row norms (sampled)\n",
    "    try:\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "        def _sample_rows(x: np.ndarray, n: int = 20000) -> np.ndarray:\n",
    "            m = min(int(x.shape[0]), int(n))\n",
    "            if m <= 0:\n",
    "                return np.zeros((0, int(x.shape[1])), dtype=np.float32)\n",
    "            idx = np.linspace(0, int(x.shape[0]) - 1, num=m, dtype=np.int64)\n",
    "            return np.asarray(x[idx], dtype=np.float32)\n",
    "\n",
    "        x_tr = np.load(train_text_npy, mmap_mode='r')\n",
    "        x_te = np.load(test_text_npy, mmap_mode='r')\n",
    "        tr = _sample_rows(x_tr)\n",
    "        te = _sample_rows(x_te)\n",
    "\n",
    "        tr_nnz = (tr > 0).sum(axis=1)\n",
    "        te_nnz = (te > 0).sum(axis=1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(tr_nnz, bins=60, alpha=0.6, label=f'train nnz/row (n={len(tr_nnz)})')\n",
    "        plt.hist(te_nnz, bins=60, alpha=0.6, label=f'test nnz/row (n={len(te_nnz)})')\n",
    "        plt.title('TF-IDF: non-zeros per row (sampled)')\n",
    "        plt.xlabel('nnz per row')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        tr_norm = np.linalg.norm(tr, axis=1)\n",
    "        te_norm = np.linalg.norm(te, axis=1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(tr_norm, bins=60, alpha=0.6, label='train L2')\n",
    "        plt.hist(te_norm, bins=60, alpha=0.6, label='test L2')\n",
    "        plt.title('TF-IDF: L2 norm per row (sampled)')\n",
    "        plt.xlabel('L2 norm')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('TF-IDF diagnostics skipped:', repr(e))\n",
    "elif STRICT_OPTION_B:\n",
    "    missing = [p for p in [text_vect, train_text_npy, test_text_npy] if not p.exists()]\n",
    "    raise FileNotFoundError('Option B requires TF-IDF artefacts, but these are missing:\\n' + '\\n'.join([f' - {m}' for m in missing]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 08b - Manual publish: TF-IDF stage to Hugging Face (safe gating)\n",
    "# Use this only if you intentionally want to publish Stage 03 to the HF checkpoint repo.\n",
    "# This cell refuses to auto-enable pushes; you must enable via env/flags upstream (HF_PUSH / CAFA_HF_PUSH).\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "feat_dir = WORK_ROOT / 'features'\n",
    "train_text_npy = feat_dir / 'train_embeds_text.npy'\n",
    "test_text_npy = feat_dir / 'test_embeds_text.npy'\n",
    "text_vect = feat_dir / 'text_vectorizer.joblib'\n",
    "\n",
    "def _sample_rows(x: np.ndarray, n: int = 20000) -> np.ndarray:\n",
    "    m = min(int(x.shape[0]), int(n))\n",
    "    if m <= 0:\n",
    "        return np.zeros((0, int(x.shape[1])), dtype=np.float32)\n",
    "    idx = np.linspace(0, int(x.shape[0]) - 1, num=m, dtype=np.int64)\n",
    "    return np.asarray(x[idx], dtype=np.float32)\n",
    "\n",
    "def _looks_nonzero(path: Path, name: str) -> bool:\n",
    "    if not path.exists():\n",
    "        print(f'[CHECKPOINT] Missing: {path}')\n",
    "        return False\n",
    "    x = np.load(path, mmap_mode='r')\n",
    "    sub = _sample_rows(x)\n",
    "    if sub.size == 0:\n",
    "        print(f'[CHECKPOINT] Empty array sample for {name}')\n",
    "        return False\n",
    "    if not np.isfinite(sub).all():\n",
    "        print(f'[CHECKPOINT] Non-finite detected in {name} sample')\n",
    "        return False\n",
    "    if float(np.max(sub)) <= 0.0:\n",
    "        print(f'[CHECKPOINT] {name} sample max==0 (all-zero suspected)')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Sanity checks first\n",
    "if not (text_vect.exists() and train_text_npy.exists() and test_text_npy.exists()):\n",
    "    raise FileNotFoundError('TF-IDF artefacts missing; run Cell 10 first.')\n",
    "if not (_looks_nonzero(train_text_npy, 'train_embeds_text') and _looks_nonzero(test_text_npy, 'test_embeds_text')):\n",
    "    raise RuntimeError('Refusing to push: TF-IDF embeddings look broken (all-zero or non-finite).')\n",
    "\n",
    "if 'STAGE_REQUIRED_FILES' not in globals():\n",
    "    raise RuntimeError('Missing STAGE_REQUIRED_FILES. Run the earlier setup cell that defines stage requirements.')\n",
    "if 'STORE' not in globals() or STORE is None:\n",
    "    raise RuntimeError('Missing STORE. Run the checkpoint store setup cell first.')\n",
    "\n",
    "# Refuse to override safety switches\n",
    "if not bool(getattr(STORE, 'push_enabled', False)):\n",
    "    raise RuntimeError('Checkpoint pushing is disabled (STORE.push_enabled=False). Enable CAFA_HF_PUSH=1 (and set CAFA_HF_REPO_ID) if you really want to publish.')\n",
    "\n",
    "rels = STAGE_REQUIRED_FILES.get('stage_03_tfidf_text', [])\n",
    "if not rels:\n",
    "    raise RuntimeError(\"STAGE_REQUIRED_FILES['stage_03_tfidf_text'] is empty; refusing to push\")\n",
    "required_paths = [WORK_ROOT / r for r in rels]\n",
    "\n",
    "STORE.maybe_push('stage_03_tfidf_text', required_paths, note='TF-IDF Stage 03 (manual publish)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9903df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "8a9903df",
    "outputId": "4e04156e-f74b-41f9-ce75-45b98224ce20"
   },
   "outputs": [],
   "source": [
    "# CELL 09a - Legacy GOA download/upload helper (DISABLED by default)\n",
    "# This cell previously mixed in Kaggle dataset uploads and forced checkpoint pushing.\n",
    "# That behaviour is intentionally disabled: the project now treats Hugging Face as the canonical artefact store,\n",
    "# and expects GOA filtered artefacts to be provided via mounted datasets or precomputed files.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "EXTERNAL_DIR = WORK_ROOT / 'external'\n",
    "EXTERNAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "filtered_path = EXTERNAL_DIR / 'goa_filtered_iea.tsv.gz'\n",
    "print('External GOA (filtered) expected at:', filtered_path)\n",
    "print('  exists:', filtered_path.exists())\n",
    "\n",
    "if filtered_path.exists():\n",
    "    # Enable downstream external processing cells\n",
    "    os.environ['CAFA_PROCESS_EXTERNAL'] = '1'\n",
    "    print('CAFA_PROCESS_EXTERNAL=1 (external GOA enabled).')\n",
    "else:\n",
    "    os.environ['CAFA_PROCESS_EXTERNAL'] = '0'\n",
    "    print('CAFA_PROCESS_EXTERNAL=0 (external GOA disabled).')\n",
    "    print('To enable external GOA priors, provide goa_filtered_iea.tsv.gz (or goa_filtered_all.tsv.gz) via:')\n",
    "    print('  - a mounted Kaggle Dataset on Kaggle, or')\n",
    "    print('  - a precomputed file placed under WORK_ROOT/external/, or')\n",
    "    print('  - your offline pipeline (scripts/01_build_goa_features.py), then publish via HF checkpointing if needed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8090a",
   "metadata": {
    "id": "c3a8090a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 09 - Solution: 2.1 PHASE 1 (Step 3): EXTERNAL DATA & EVIDENCE CODES (ARTEFACT-FIRST)\n",
    "# 2.1 PHASE 1 (Step 3): EXTERNAL DATA & EVIDENCE CODES (ARTEFACT-FIRST)\n",
    "# ================================================================\n",
    "# Option B strictness: external GOA artefacts are REQUIRED on Kaggle/Colab.\n",
    "# Local runs default to best-effort (skip if missing) unless you opt-in.\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "\n",
    "def _detect_kaggle() -> bool:\n",
    "    return bool(\n",
    "        os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\")\n",
    "        or os.environ.get(\"KAGGLE_URL_BASE\")\n",
    "        or os.environ.get(\"KAGGLE_DATA_PROXY_URL\")\n",
    "    )\n",
    "\n",
    "\n",
    "def _detect_colab() -> bool:\n",
    "    return bool(\n",
    "        os.environ.get(\"COLAB_RELEASE_TAG\")\n",
    "        or os.environ.get(\"COLAB_GPU\")\n",
    "        or os.environ.get(\"COLAB_TPU_ADDR\")\n",
    "    )\n",
    "\n",
    "\n",
    "IS_KAGGLE = _detect_kaggle()\n",
    "IS_COLAB = (not IS_KAGGLE) and _detect_colab()\n",
    "IS_LOCAL = (not IS_KAGGLE) and (not IS_COLAB)\n",
    "\n",
    "# Defaults:\n",
    "# - Kaggle/Colab: strict + enabled\n",
    "# - Local: non-strict + disabled (opt-in)\n",
    "STRICT_OPTION_B = bool(int(os.getenv(\"CAFA_STRICT_OPTION_B\", \"0\" if IS_LOCAL else \"1\")))\n",
    "PROCESS_EXTERNAL = bool(int(os.getenv(\"CAFA_PROCESS_EXTERNAL\", \"0\" if IS_LOCAL else \"1\")))\n",
    "\n",
    "# Recommended (Kaggle): set CAFA_GOA_DATASET_DIR to your dataset folder,\n",
    "# e.g. /kaggle/input/goa-filtered-all-tsv-gz\n",
    "GOA_ENV_DIR = os.getenv(\"CAFA_GOA_DATASET_DIR\", \"\").strip()\n",
    "GOA_ARTEFACT_DIRS = [\n",
    "    Path(GOA_ENV_DIR) if GOA_ENV_DIR else None,\n",
    "    Path(\"/content/cafa6_data/external\"),\n",
    "    # Path('/kaggle/input/<dataset-folder>'),\n",
    "]\n",
    "\n",
    "if PROCESS_EXTERNAL:\n",
    "    EXT_DIR = WORK_ROOT / \"external\"\n",
    "    EXT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    def _existing(dirs: list[Path]) -> list[Path]:\n",
    "        return [\n",
    "            Path(d)\n",
    "            for d in dirs\n",
    "            if d is not None and str(d).strip() and Path(d).exists()\n",
    "        ]\n",
    "\n",
    "    def _discover(dirs: list[Path]) -> list[Path]:\n",
    "        patterns = (\"**/goa_filtered_*.tsv.gz\", \"**/goa_filtered_*.tsv\")\n",
    "        found: list[Path] = []\n",
    "        for d in dirs:\n",
    "            found += [\n",
    "                d / \"goa_filtered_all.tsv.gz\",\n",
    "                d / \"goa_filtered_iea.tsv.gz\",\n",
    "                d / \"goa_filtered_all.tsv\",\n",
    "                d / \"goa_filtered_iea.tsv\",\n",
    "            ]\n",
    "            for pat in patterns:\n",
    "                found += sorted(Path(d).glob(pat))\n",
    "        out: list[Path] = []\n",
    "        seen = set()\n",
    "        for p in found:\n",
    "            p = Path(p)\n",
    "            if p in seen:\n",
    "                continue\n",
    "            seen.add(p)\n",
    "            if p.exists() and p.is_file():\n",
    "                out.append(p)\n",
    "        return out\n",
    "\n",
    "    def _pick_best(paths: list[Path]) -> Path:\n",
    "        def score(p: Path) -> tuple[int, int]:\n",
    "            name = p.name.lower()\n",
    "            ext_rank = 0 if name.endswith(\".tsv.gz\") else 1\n",
    "            if \"goa_filtered_all\" in name:\n",
    "                return (0, ext_rank)\n",
    "            if \"goa_filtered_iea\" in name:\n",
    "                return (1, ext_rank)\n",
    "            if \"all\" in name:\n",
    "                return (2, ext_rank)\n",
    "            if \"iea\" in name:\n",
    "                return (3, ext_rank)\n",
    "            return (4, ext_rank)\n",
    "\n",
    "        return sorted(paths, key=score)[0]\n",
    "\n",
    "    roots = _existing(GOA_ARTEFACT_DIRS)\n",
    "    if not roots and Path(\"/kaggle/input\").exists():\n",
    "        # fallback discovery, but still strict if nothing found\n",
    "        roots = [Path(\"/kaggle/input\")]\n",
    "    elif not roots:\n",
    "        roots = [EXT_DIR, Path(\"artefacts_local/artefacts/external\")]\n",
    "\n",
    "    goa_paths = _discover(roots)\n",
    "    if not goa_paths:\n",
    "        msg = (\n",
    "            \"External GOA artefacts not found.\\n\"\n",
    "            \"Fix:\\n\"\n",
    "            \" - Publish goa_filtered_all.tsv.gz (and/or goa_filtered_iea.tsv.gz) as a Kaggle Dataset\\n\"\n",
    "            \" - Attach it to this notebook\\n\"\n",
    "            \" - Set env var CAFA_GOA_DATASET_DIR=/kaggle/input/<dataset-folder> (recommended)\\n\"\n",
    "            \" - Or place the file under artefacts/external locally\\n\"\n",
    "            \"\\n\"\n",
    "            f\"Mode: STRICT_OPTION_B={STRICT_OPTION_B} PROCESS_EXTERNAL={PROCESS_EXTERNAL}\"\n",
    "        )\n",
    "        if STRICT_OPTION_B:\n",
    "            raise FileNotFoundError(msg)\n",
    "        print(msg)\n",
    "        PROCESS_EXTERNAL = False\n",
    "    else:\n",
    "        GOA_FEATURE_PATH = _pick_best(goa_paths)\n",
    "        print(\"Using GOA artefact:\", GOA_FEATURE_PATH)\n",
    "        print(\"Format expected: EntryID<TAB>term<TAB>evidence (header included)\")\n",
    "else:\n",
    "    print(\"Skipping external GOA features (PROCESS_EXTERNAL=0).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac3bd0",
   "metadata": {
    "id": "16ac3bd0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 10 - Solution: 2.2 PHASE 1 (Step 4): HIERARCHY PROPAGATION FOR EXTERNAL GOA (NO-KAGGLE / IEA)\n",
    "# PRODUCES (when PROCESS_EXTERNAL=True): external/prop_train_no_kaggle.tsv, external/prop_test_no_kaggle.tsv\n",
    "# Milestone checkpoint: stage_04_external_goa_priors\n",
    "# Produces (when enabled):\n",
    "# - external/prop_train_no_kaggle.tsv.gz\n",
    "# - external/prop_test_no_kaggle.tsv.gz\n",
    "# Option B strictness: this step MUST be satisfied when PROCESS_EXTERNAL=True.\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "EXTERNAL_PRIOR_SCORE = 1.0  # binary prior (later we down-weight when injecting into models)\n",
    "\n",
    "if not PROCESS_EXTERNAL:\n",
    "    print(\"Skipping external GOA hierarchy propagation (PROCESS_EXTERNAL=0).\")\n",
    "else:\n",
    "    if \"GOA_FEATURE_PATH\" not in locals():\n",
    "        raise RuntimeError(\n",
    "            \"PROCESS_EXTERNAL=True but GOA_FEATURE_PATH is missing. Run the external GOA discovery cell first.\"\n",
    "        )\n",
    "\n",
    "    # Ensure we have GO parents (hierarchy)\n",
    "    if \"go_parents\" not in locals():\n",
    "        def parse_obo(path: Path):\n",
    "            parents = {}\n",
    "            namespaces = {}\n",
    "            cur_id, cur_ns = None, None\n",
    "            with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line == \"[Term]\":\n",
    "                        if cur_id and cur_ns:\n",
    "                            namespaces[cur_id] = cur_ns\n",
    "                        cur_id, cur_ns = None, None\n",
    "                    elif line.startswith(\"id: GO:\"):\n",
    "                        cur_id = line.split(\"id: \", 1)[1]\n",
    "                    elif line.startswith(\"namespace:\"):\n",
    "                        cur_ns = line.split(\"namespace: \", 1)[1]\n",
    "                    elif line.startswith(\"is_a:\") and cur_id:\n",
    "                        parent = line.split(\"is_a: \", 1)[1].split(\" ! \")[0]\n",
    "                        parents.setdefault(cur_id, set()).add(parent)\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "            return parents, namespaces\n",
    "\n",
    "        go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "\n",
    "    # Ensure train/test IDs are available\n",
    "    train_seq_path = WORK_ROOT / \"parsed\" / \"train_seq.feather\"\n",
    "    test_seq_path = WORK_ROOT / \"parsed\" / \"test_seq.feather\"\n",
    "    if not train_seq_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            \"Missing train_seq.feather. Run Phase 1 Step 2 (FASTA parse) first.\"\n",
    "        )\n",
    "    if not test_seq_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            \"Missing test_seq.feather. Run Phase 1 Step 2 (FASTA parse) first.\"\n",
    "        )\n",
    "\n",
    "    # IMPORTANT: GOA artefacts usually use UniProt accessions, but FASTA IDs may be like sp|P12345|NAME.\n",
    "    # Normalise for matching, but write the original IDs so downstream joins (reindex on train_ids/test_ids) still work.\n",
    "    def _norm_entry_id(raw: str) -> str:\n",
    "        s = str(raw).strip()\n",
    "        parts = s.split(\"|\")\n",
    "        if len(parts) >= 2 and parts[0] in {\"sp\", \"tr\"}:\n",
    "            return parts[1]\n",
    "        if len(parts) >= 3:\n",
    "            return parts[1]\n",
    "        return s\n",
    "\n",
    "    train_ids_raw = pd.read_feather(train_seq_path)[\"id\"].astype(str).tolist()\n",
    "    test_ids_raw = pd.read_feather(test_seq_path)[\"id\"].astype(str).tolist()\n",
    "    train_id_map = {_norm_entry_id(i): i for i in train_ids_raw}\n",
    "    test_id_map = {_norm_entry_id(i): i for i in test_ids_raw}\n",
    "    train_ids = set(train_id_map.keys())\n",
    "    test_ids = set(test_id_map.keys())\n",
    "\n",
    "    # Get top-K train terms (defines the external feature space)\n",
    "    # Champion Strategy: 13,500 terms (10k BP, 2k MF, 1.5k CC)\n",
    "    train_terms_path = WORK_ROOT / \"parsed\" / \"train_terms.parquet\"\n",
    "    if not train_terms_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            \"Missing train_terms.parquet. Run Phase 1 Step 2 (targets parse) first.\"\n",
    "        )\n",
    "    train_terms = pd.read_parquet(train_terms_path)\n",
    "\n",
    "    # Map terms to namespaces\n",
    "    term_counts = train_terms[\"term\"].value_counts()\n",
    "    # Ensure all terms have a namespace (fallback to unknown if missing in OBO)\n",
    "    term_ns = term_counts.index.map(lambda t: go_namespaces.get(t, \"unknown\"))\n",
    "\n",
    "    # Select top K per aspect\n",
    "    top_bp = term_counts[term_ns == \"biological_process\"].head(10000).index.tolist()\n",
    "    top_mf = term_counts[term_ns == \"molecular_function\"].head(2000).index.tolist()\n",
    "    top_cc = term_counts[term_ns == \"cellular_component\"].head(1500).index.tolist()\n",
    "\n",
    "    top_terms = list(set(top_bp + top_mf + top_cc))\n",
    "    top_terms_set = set(top_terms)\n",
    "    print(f\"External propagation restricted to top {len(top_terms)} train terms (BP={len(top_bp)}, MF={len(top_mf)}, CC={len(top_cc)}).\")\n",
    "\n",
    "    EXT_DIR = WORK_ROOT / \"external\"\n",
    "    EXT_DIR.mkdir(exist_ok=True)\n",
    "    out_train = EXT_DIR / \"prop_train_no_kaggle.tsv.gz\"\n",
    "    out_test = EXT_DIR / \"prop_test_no_kaggle.tsv.gz\"\n",
    "\n",
    "    if out_train.exists() and out_test.exists():\n",
    "        print(\"Propagated GOA priors already present; skipping propagation.\")\n",
    "    else:\n",
    "        # Ancestor closure with memoisation\n",
    "        _anc_cache: dict[str, set[str]] = {}\n",
    "\n",
    "        def ancestors(term: str) -> set[str]:\n",
    "            if term in _anc_cache:\n",
    "                return _anc_cache[term]\n",
    "            seen = {term}\n",
    "            stack = [term]\n",
    "            while stack:\n",
    "                t = stack.pop()\n",
    "                for p in go_parents.get(t, ()):\n",
    "                    if p not in seen:\n",
    "                        seen.add(p)\n",
    "                        stack.append(p)\n",
    "            _anc_cache[term] = seen\n",
    "            return seen\n",
    "\n",
    "        cols = [\"EntryID\", \"term\", \"evidence\"]\n",
    "        print(\"Streaming GOA artefact:\", GOA_FEATURE_PATH)\n",
    "        print(\"Writing:\", out_train)\n",
    "        print(\"Writing:\", out_test)\n",
    "        n_train = 0\n",
    "        n_test = 0\n",
    "        with gzip.open(out_train, \"wt\", encoding=\"utf-8\") as ftr, gzip.open(\n",
    "            out_test, \"wt\", encoding=\"utf-8\"\n",
    "        ) as fte:\n",
    "            ftr.write(\"EntryID\\tterm\\tscore\\n\")\n",
    "            fte.write(\"EntryID\\tterm\\tscore\\n\")\n",
    "            for chunk in pd.read_csv(\n",
    "                GOA_FEATURE_PATH,\n",
    "                sep=\"\\t\",\n",
    "                dtype=str,\n",
    "                usecols=lambda c: c in cols,\n",
    "                chunksize=500_000,\n",
    "            ):\n",
    "                missing_cols = [c for c in cols if c not in chunk.columns]\n",
    "                if missing_cols:\n",
    "                    raise ValueError(\n",
    "                        f\"GOA artefact missing columns: {missing_cols}. Found: {list(chunk.columns)}\"\n",
    "                    )\n",
    "                chunk = chunk[cols].dropna()\n",
    "                chunk = chunk[chunk[\"evidence\"] == \"IEA\"]\n",
    "                if chunk.empty:\n",
    "                    continue\n",
    "                chunk = chunk.drop_duplicates(subset=[\"EntryID\", \"term\"])\n",
    "                for entry_id_raw, term in zip(\n",
    "                    chunk[\"EntryID\"].tolist(), chunk[\"term\"].tolist()\n",
    "                ):\n",
    "                    entry_norm = _norm_entry_id(entry_id_raw)\n",
    "                    if entry_norm in train_ids:\n",
    "                        target = ftr\n",
    "                        out_entry = train_id_map[entry_norm]\n",
    "                    elif entry_norm in test_ids:\n",
    "                        target = fte\n",
    "                        out_entry = test_id_map[entry_norm]\n",
    "                    else:\n",
    "                        continue\n",
    "                    keep = ancestors(term) & top_terms_set\n",
    "                    if not keep:\n",
    "                        continue\n",
    "                    for t in keep:\n",
    "                        target.write(f\"{out_entry}\\t{t}\\t{EXTERNAL_PRIOR_SCORE}\\n\")\n",
    "                    if target is ftr:\n",
    "                        n_train += len(keep)\n",
    "                    else:\n",
    "                        n_test += len(keep)\n",
    "        print(f\"Wrote propagated IEA edges: train={n_train:,} test={n_test:,}\")\n",
    "        print(\n",
    "            \"Outputs are intentionally sparse priors (score=1.0) and will be down-weighted when injected.\"\n",
    "        )\n",
    "\n",
    "    if out_train.exists() and out_test.exists():\n",
    "        STORE.maybe_push(\n",
    "            \"stage_04_external_goa_priors\",\n",
    "            [out_train, out_test],\n",
    "            note=\"propagated IEA priors\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bed4d",
   "metadata": {
    "id": "886bed4d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 10b - Diagnostics: artefact manifest (existence + sizes)\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "def _mb(p: Path) -> float:\n",
    "    return p.stat().st_size / (1024**2)\n",
    "\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "\n",
    "# Minimal contract: Phase 1 outputs + Option B required artefacts + later model outputs\n",
    "paths = {\n",
    "    # Phase 1 parsed\n",
    "    'parsed/train_seq.feather': WORK_ROOT / 'parsed' / 'train_seq.feather',\n",
    "    'parsed/test_seq.feather': WORK_ROOT / 'parsed' / 'test_seq.feather',\n",
    "    'parsed/train_terms.parquet': WORK_ROOT / 'parsed' / 'train_terms.parquet',\n",
    "    'parsed/term_priors.parquet': WORK_ROOT / 'parsed' / 'term_priors.parquet',\n",
    "    'parsed/train_taxa.feather': WORK_ROOT / 'parsed' / 'train_taxa.feather',\n",
    "    'parsed/test_taxa.feather': WORK_ROOT / 'parsed' / 'test_taxa.feather',\n",
    "\n",
    "    # Text pipeline\n",
    "    'external/entryid_text.tsv': WORK_ROOT / 'external' / 'entryid_text.tsv',\n",
    "    'features/text_vectorizer.joblib': WORK_ROOT / 'features' / 'text_vectorizer.joblib',\n",
    "    'features/train_embeds_text.npy': WORK_ROOT / 'features' / 'train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy': WORK_ROOT / 'features' / 'test_embeds_text.npy',\n",
    "\n",
    "    # Sequence embeddings (core)\n",
    "    'features/train_embeds_t5.npy': WORK_ROOT / 'features' / 'train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy': WORK_ROOT / 'features' / 'test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy': WORK_ROOT / 'features' / 'train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy': WORK_ROOT / 'features' / 'test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy': WORK_ROOT / 'features' / 'train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy': WORK_ROOT / 'features' / 'test_embeds_esm2_3b.npy',\n",
    "    'features/train_embeds_ankh.npy': WORK_ROOT / 'features' / 'train_embeds_ankh.npy',\n",
    "    'features/test_embeds_ankh.npy': WORK_ROOT / 'features' / 'test_embeds_ankh.npy',\n",
    "\n",
    "    # External priors\n",
    "    'external/prop_train_no_kaggle.tsv.gz': WORK_ROOT / 'external' / 'prop_train_no_kaggle.tsv.gz',\n",
    "    'external/prop_test_no_kaggle.tsv.gz': WORK_ROOT / 'external' / 'prop_test_no_kaggle.tsv.gz',\n",
    "\n",
    "    # Term contract\n",
    "    'features/top_terms_13500.json': WORK_ROOT / 'features' / 'top_terms_13500.json',\n",
    "\n",
    "    # Level-1 predictions (canonical location)\n",
    "    'features/level1_preds/oof_pred_logreg.npy': PRED_DIR / 'oof_pred_logreg.npy',\n",
    "    'features/level1_preds/test_pred_logreg.npy': PRED_DIR / 'test_pred_logreg.npy',\n",
    "    'features/level1_preds/oof_pred_logreg_BP.npy': PRED_DIR / 'oof_pred_logreg_BP.npy',\n",
    "    'features/level1_preds/test_pred_logreg_BP.npy': PRED_DIR / 'test_pred_logreg_BP.npy',\n",
    "    'features/level1_preds/oof_pred_logreg_MF.npy': PRED_DIR / 'oof_pred_logreg_MF.npy',\n",
    "    'features/level1_preds/test_pred_logreg_MF.npy': PRED_DIR / 'test_pred_logreg_MF.npy',\n",
    "    'features/level1_preds/oof_pred_logreg_CC.npy': PRED_DIR / 'oof_pred_logreg_CC.npy',\n",
    "    'features/level1_preds/test_pred_logreg_CC.npy': PRED_DIR / 'test_pred_logreg_CC.npy',\n",
    "    'features/level1_preds/oof_pred_gbdt.npy': PRED_DIR / 'oof_pred_gbdt.npy',\n",
    "    'features/level1_preds/test_pred_gbdt.npy': PRED_DIR / 'test_pred_gbdt.npy',\n",
    "    'features/level1_preds/oof_pred_dnn.npy': PRED_DIR / 'oof_pred_dnn.npy',\n",
    "    'features/level1_preds/test_pred_dnn.npy': PRED_DIR / 'test_pred_dnn.npy',\n",
    "    'features/level1_preds/oof_pred_knn.npy': PRED_DIR / 'oof_pred_knn.npy',\n",
    "    'features/level1_preds/test_pred_knn.npy': PRED_DIR / 'test_pred_knn.npy',\n",
    "\n",
    "    # Stacker output\n",
    "    'features/test_pred_gcn.npy': WORK_ROOT / 'features' / 'test_pred_gcn.npy',\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, p in paths.items():\n",
    "    rows.append({'artefact': name, 'exists': p.exists(), 'mb': _mb(p) if p.exists() else 0.0, 'path': str(p)})\n",
    "df = pd.DataFrame(rows).sort_values(['exists', 'mb'], ascending=[True, False])\n",
    "print('WORK_ROOT:', WORK_ROOT)\n",
    "\n",
    "try:\n",
    "    from IPython.display import display  # type: ignore\n",
    "except Exception:\n",
    "    def display(x):\n",
    "        print(x)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Visual: top 25 largest artefacts\n",
    "df2 = df[df['exists']].sort_values('mb', ascending=False).head(25)\n",
    "if len(df2) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df2, y='artefact', x='mb')\n",
    "    plt.title('Largest artefacts (MB)')\n",
    "    plt.xlabel('MB')\n",
    "    plt.ylabel('artefact')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Option B strict check (external priors + embeddings + term contract)\n",
    "option_b_required = [\n",
    "    'external/prop_train_no_kaggle.tsv.gz',\n",
    "    'external/prop_test_no_kaggle.tsv.gz',\n",
    "    'features/text_vectorizer.joblib',\n",
    "    'features/train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy',\n",
    "    'features/train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy',\n",
    "    'features/train_embeds_ankh.npy',\n",
    "    'features/test_embeds_ankh.npy',\n",
    "    'parsed/train_taxa.feather',\n",
    "    'parsed/test_taxa.feather',\n",
    "    'features/top_terms_13500.json',\n",
    "]\n",
    "missing = [a for a in option_b_required if not paths[a].exists()]\n",
    "if missing:\n",
    "    print('\\nOption B missing artefacts:')\n",
    "    for m in missing:\n",
    "        print(' -', m)\n",
    "else:\n",
    "    print('\\nOption B artefacts OK: priors + embeddings + TF-IDF + taxonomy + top terms present.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c1751",
   "metadata": {
    "id": "c02c1751",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 11 - Solution: 3a. PHASE 1: EMBEDDINGS GENERATION (T5 only)\n",
    "# PRODUCES: features/train_embeds_t5.npy, features/test_embeds_t5.npy\n",
    "# 3a. PHASE 1: EMBEDDINGS GENERATION (T5 only)\n",
    "# ============================================\n",
    "# HARDWARE: GPU recommended\n",
    "# ============================================\n",
    "# Split from ESM2 so you can run each independently on Kaggle.\n",
    "import os\n",
    "\n",
    "# IMPORTANT: pushing checkpoints republishes a whole Kaggle Dataset version (but only the staged flat files we include),\n",
    "# which can mean re-uploading multi-GB zips even when embeddings already exist.\n",
    "t5_train_path = WORK_ROOT / 'features' / 'train_embeds_t5.npy'\n",
    "t5_test_path = WORK_ROOT / 'features' / 'test_embeds_t5.npy'\n",
    "has_test = (WORK_ROOT / 'parsed' / 'test_seq.feather').exists()\n",
    "train_ready = t5_train_path.exists()\n",
    "test_ready = (not has_test) or t5_test_path.exists()\n",
    "COMPUTE_T5 = True  # <--- enable/disable T5 run\n",
    "FORCE_REBUILD = False  # manual toggle; keep default strict\n",
    "train_needed = COMPUTE_T5 and (FORCE_REBUILD or (not train_ready))\n",
    "train_needed = COMPUTE_T5 and (FORCE_REBUILD or (not train_ready))\n",
    "test_needed = COMPUTE_T5 and has_test and (FORCE_REBUILD or (not t5_test_path.exists()))\n",
    "# If artefacts already exist, skip compute unless forced.\n",
    "if COMPUTE_T5 and (not train_needed) and (not test_needed):\n",
    "    COMPUTE_T5 = False\n",
    "    if not PUSH_EXISTING_CHECKPOINTS:\n",
    "        print('T5 embeddings already exist; skipping compute and not pushing checkpoints (PUSH_EXISTING_CHECKPOINTS=False).')\n",
    "    if PUSH_EXISTING_CHECKPOINTS and 'STORE' in globals() and STORE is not None:\n",
    "        # Granular checkpoints: allow resuming test without losing train.\n",
    "        try:\n",
    "            STORE.maybe_push('stage_05a_embeddings_t5_train', [t5_train_path], note='ProtT5 mean-pool embeddings (train only, existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_05a_embeddings_t5_train: {e}')\n",
    "        if t5_test_path.exists():\n",
    "            try:\n",
    "                STORE.maybe_push('stage_05b_embeddings_t5_test', [t5_test_path], note='ProtT5 mean-pool embeddings (test only, existing)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_05b_embeddings_t5_test: {e}')\n",
    "        # Backwards-compatible combined stage.\n",
    "        paths = [t5_train_path]\n",
    "        if t5_test_path.exists():\n",
    "            paths.append(t5_test_path)\n",
    "        try:\n",
    "            STORE.maybe_push('stage_05_embeddings_t5', paths, note='ProtT5 mean-pool embeddings (existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_05_embeddings_t5: {e}')\n",
    "if COMPUTE_T5:\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from transformers import T5Tokenizer, T5EncoderModel\n",
    "    from tqdm.auto import tqdm\n",
    "    import contextlib\n",
    "    # Optimise CUDA memory allocation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    # Fix Protobuf 'GetPrototype' error\n",
    "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    amp_ctx = torch.amp.autocast('cuda') if device.type == 'cuda' else contextlib.nullcontext()\n",
    "    def get_t5_model():\n",
    "        print(\"Loading T5 Model...\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\n",
    "            \"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case=False, legacy=True\n",
    "        )\n",
    "        model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "        model.eval()\n",
    "        return tokenizer, model\n",
    "    def generate_embeddings_t5(model, tokenizer, sequences, batch_size=4, max_len=1024):\n",
    "        seq_lens = [len(s) for s in sequences]\n",
    "        sort_idx = np.argsort(seq_lens)[::-1]\n",
    "        sorted_seqs = [sequences[i] for i in sort_idx]\n",
    "        embeddings_list = []\n",
    "        for i in tqdm(range(0, len(sorted_seqs), batch_size), desc=\"Embedding T5 (Smart Batch)\"):\n",
    "            batch_seqs = sorted_seqs[i : i + batch_size]\n",
    "            batch_seqs = [seq.replace('U', 'X').replace('Z', 'X').replace('O', 'X').replace('B', 'X') for seq in batch_seqs]\n",
    "            batch_seqs = [\" \".join(list(seq)) for seq in batch_seqs]\n",
    "            ids = tokenizer.batch_encode_plus(\n",
    "                batch_seqs,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                with amp_ctx:\n",
    "                    embedding_repr = model(input_ids=ids['input_ids'], attention_mask=ids['attention_mask'])\n",
    "            emb = embedding_repr.last_hidden_state.float().detach().cpu().numpy()\n",
    "            mask = ids['attention_mask'].detach().cpu().numpy()\n",
    "            for j in range(len(batch_seqs)):\n",
    "                seq_len = int(mask[j].sum())\n",
    "                valid_emb = emb[j, :seq_len]\n",
    "                embeddings_list.append(valid_emb.mean(axis=0))\n",
    "            del ids, embedding_repr, emb, mask\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        sorted_embeddings = np.vstack(embeddings_list)\n",
    "        original_order_embeddings = np.zeros_like(sorted_embeddings)\n",
    "        original_order_embeddings[sort_idx] = sorted_embeddings\n",
    "        return original_order_embeddings\n",
    "    tokenizer, model = get_t5_model()\n",
    "    if train_needed:\n",
    "        print(\"Loading train sequences for T5 embedding...\")\n",
    "        train_df = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')\n",
    "        print(f\"Generating Train Embeddings T5 ({len(train_df)})...\")\n",
    "        train_emb = generate_embeddings_t5(model, tokenizer, train_df['sequence'].tolist())\n",
    "        np.save(t5_train_path, train_emb)\n",
    "        # Push immediately after train completes so a crash during test doesn't lose progress.\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            try:\n",
    "                STORE.maybe_push('stage_05a_embeddings_t5_train', [t5_train_path], note='ProtT5 mean-pool embeddings (train only)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_05a_embeddings_t5_train: {e}')\n",
    "        del train_emb, train_df\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(f\"Train embeddings already exist at {t5_train_path}; skipping train compute.\")\n",
    "    if test_needed:\n",
    "        print(\"Loading test sequences for T5 embedding...\")\n",
    "        test_df = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')\n",
    "        print(f\"Generating Test Embeddings T5 ({len(test_df)})...\")\n",
    "        test_emb = generate_embeddings_t5(model, tokenizer, test_df['sequence'].tolist())\n",
    "        np.save(t5_test_path, test_emb)\n",
    "        # Push immediately after test completes as well.\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            try:\n",
    "                STORE.maybe_push('stage_05b_embeddings_t5_test', [t5_test_path], note='ProtT5 mean-pool embeddings (test only)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_05b_embeddings_t5_test: {e}')\n",
    "        del test_emb, test_df\n",
    "        gc.collect()\n",
    "    else:\n",
    "        if has_test:\n",
    "            print(f\"Test embeddings already exist at {t5_test_path}; skipping test compute.\")\n",
    "        else:\n",
    "            print(\"No test sequences found; skipping test embedding.\")\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"T5 embeddings ready.\")\n",
    "    # Backwards-compatible combined stage.\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        paths = [t5_train_path]\n",
    "        if t5_test_path.exists():\n",
    "            paths.append(t5_test_path)\n",
    "        try:\n",
    "            STORE.maybe_push('stage_05_embeddings_t5', paths, note='ProtT5 mean-pool embeddings')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_05_embeddings_t5: {e}')\n",
    "else:\n",
    "    print(\"Skipping T5 embedding generation (COMPUTE_T5=False).\")\n",
    "# Diagnostics: embedding norms (train vs test; sampled)\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    def _plot_embed_norms(name: str, train_path: Path, test_path: Path):\n",
    "        if not Path(train_path).exists():\n",
    "            print(f'[{name}] missing train embeddings: {train_path}')\n",
    "            return\n",
    "        x_tr = np.load(train_path, mmap_mode='r')\n",
    "        x_te = np.load(test_path, mmap_mode='r') if Path(test_path).exists() else None\n",
    "        def _sample_norms(x: np.ndarray, n: int = 20000) -> np.ndarray:\n",
    "            m = min(int(x.shape[0]), int(n))\n",
    "            if m <= 0:\n",
    "                return np.zeros((0,), dtype=np.float32)\n",
    "            idx = np.linspace(0, int(x.shape[0]) - 1, num=m, dtype=np.int64)\n",
    "            return np.linalg.norm(np.asarray(x[idx], dtype=np.float32), axis=1).astype(np.float32)\n",
    "        tr_norm = _sample_norms(x_tr)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(tr_norm, bins=60, alpha=0.6, label=f'train (n={len(tr_norm)})')\n",
    "        if x_te is not None:\n",
    "            te_norm = _sample_norms(x_te)\n",
    "            plt.hist(te_norm, bins=60, alpha=0.6, label=f'test (n={len(te_norm)})')\n",
    "        plt.title(f'{name}: L2 norm distribution (sampled)')\n",
    "        plt.xlabel('L2 norm')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        def _stats(a: np.ndarray) -> dict:\n",
    "            if a.size == 0:\n",
    "                return {}\n",
    "            return {\n",
    "                'min': float(np.min(a)),\n",
    "                'p50': float(np.median(a)),\n",
    "                'p95': float(np.quantile(a, 0.95)),\n",
    "                'max': float(np.max(a)),\n",
    "            }\n",
    "        print(f'[{name}] train norm stats:', _stats(tr_norm))\n",
    "        if x_te is not None:\n",
    "            print(f'[{name}] test norm stats:', _stats(te_norm))\n",
    "    _plot_embed_norms('ProtT5', t5_train_path, t5_test_path)\n",
    "except Exception as e:\n",
    "    print('T5 embedding diagnostics skipped:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d978323",
   "metadata": {
    "id": "1d978323",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 12 - Solution: 3b. PHASE 1: EMBEDDINGS GENERATION (ESM2 only)\n",
    "# PRODUCES: features/train_embeds_esm2.npy, features/test_embeds_esm2.npy, features/train_embeds_esm2_3b.npy, features/test_embeds_esm2_3b.npy\n",
    "\n",
    "# 3b. PHASE 1: EMBEDDINGS GENERATION (ESM2 only)\n",
    "\n",
    "# HARDWARE: GPU recommended\n",
    "\n",
    "# Split from Ankh so you can run each independently.\n",
    "\n",
    "import os\n",
    "\n",
    "FORCE_REBUILD = bool(globals().get('FORCE_REBUILD', False))\n",
    "\n",
    "esm2_train_path = WORK_ROOT / 'features' / 'train_embeds_esm2.npy'\n",
    "\n",
    "esm2_test_path = WORK_ROOT / 'features' / 'test_embeds_esm2.npy'\n",
    "\n",
    "esm2_3b_train_path = WORK_ROOT / 'features' / 'train_embeds_esm2_3b.npy'\n",
    "\n",
    "esm2_3b_test_path = WORK_ROOT / 'features' / 'test_embeds_esm2_3b.npy'\n",
    "\n",
    "has_test = (WORK_ROOT / 'parsed' / 'test_seq.feather').exists()\n",
    "\n",
    "COMPUTE_ESM2 = True\n",
    "\n",
    "COMPUTE_ESM2_3B = True\n",
    "\n",
    "esm2_train_needed = COMPUTE_ESM2 and (FORCE_REBUILD or (not esm2_train_path.exists()))\n",
    "\n",
    "esm2_test_needed = COMPUTE_ESM2 and has_test and (FORCE_REBUILD or (not esm2_test_path.exists()))\n",
    "\n",
    "esm2_3b_train_needed = COMPUTE_ESM2_3B and (FORCE_REBUILD or (not esm2_3b_train_path.exists()))\n",
    "\n",
    "esm2_3b_test_needed = COMPUTE_ESM2_3B and has_test and (FORCE_REBUILD or (not esm2_3b_test_path.exists()))\n",
    "\n",
    "if COMPUTE_ESM2 and (not esm2_train_needed) and (not esm2_test_needed):\n",
    "\n",
    "    COMPUTE_ESM2 = False\n",
    "\n",
    "    if PUSH_EXISTING_CHECKPOINTS and 'STORE' in globals() and STORE is not None:\n",
    "\n",
    "        try:\n",
    "\n",
    "            STORE.maybe_push('stage_06_embeddings_esm2_train', [esm2_train_path], note='ESM2-650M mean-pool embeddings (train only, existing)')\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f'WARN: failed to push stage_06_embeddings_esm2_train: {e}')\n",
    "\n",
    "        if esm2_test_path.exists():\n",
    "\n",
    "            try:\n",
    "\n",
    "                STORE.maybe_push('stage_06_embeddings_esm2_test', [esm2_test_path], note='ESM2-650M mean-pool embeddings (test only, existing)')\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f'WARN: failed to push stage_06_embeddings_esm2_test: {e}')\n",
    "\n",
    "        paths = [esm2_train_path]\n",
    "\n",
    "        if esm2_test_path.exists():\n",
    "\n",
    "            paths.append(esm2_test_path)\n",
    "\n",
    "        try:\n",
    "\n",
    "            STORE.maybe_push('stage_06_embeddings_esm2', paths, note='ESM2-650M mean-pool embeddings (existing)')\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f'WARN: failed to push stage_06_embeddings_esm2: {e}')\n",
    "\n",
    "if COMPUTE_ESM2_3B and (not esm2_3b_train_needed) and (not esm2_3b_test_needed):\n",
    "\n",
    "    COMPUTE_ESM2_3B = False\n",
    "\n",
    "    if PUSH_EXISTING_CHECKPOINTS and 'STORE' in globals() and STORE is not None:\n",
    "\n",
    "        try:\n",
    "\n",
    "            STORE.maybe_push('stage_06b_embeddings_esm2_3b_train', [esm2_3b_train_path], note='ESM2-3B mean-pool embeddings (train only, existing)')\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f'WARN: failed to push stage_06b_embeddings_esm2_3b_train: {e}')\n",
    "\n",
    "        if esm2_3b_test_path.exists():\n",
    "\n",
    "            try:\n",
    "\n",
    "                STORE.maybe_push('stage_06b_embeddings_esm2_3b_test', [esm2_3b_test_path], note='ESM2-3B mean-pool embeddings (test only, existing)')\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f'WARN: failed to push stage_06b_embeddings_esm2_3b_test: {e}')\n",
    "\n",
    "        paths = [esm2_3b_train_path]\n",
    "\n",
    "        if esm2_3b_test_path.exists():\n",
    "\n",
    "            paths.append(esm2_3b_test_path)\n",
    "\n",
    "        try:\n",
    "\n",
    "            STORE.maybe_push('stage_06b_embeddings_esm2_3b', paths, note='ESM2-3B mean-pool embeddings (existing)')\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f'WARN: failed to push stage_06b_embeddings_esm2_3b: {e}')\n",
    "\n",
    "if COMPUTE_ESM2 or COMPUTE_ESM2_3B:\n",
    "\n",
    "    import gc, numpy as np, pandas as pd, torch\n",
    "\n",
    "    from transformers import EsmTokenizer, EsmModel\n",
    "\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    import contextlib\n",
    "\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    amp_ctx = torch.amp.autocast('cuda') if device.type == 'cuda' else contextlib.nullcontext()\n",
    "\n",
    "    def _mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "\n",
    "        x = last_hidden_state * mask\n",
    "\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "\n",
    "        return x.sum(dim=1) / denom\n",
    "\n",
    "    def get_esm2_model(model_name: str):\n",
    "\n",
    "        print(f\"Loading ESM2 model: {model_name}\")\n",
    "\n",
    "        tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        model = EsmModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        return tokenizer, model\n",
    "\n",
    "    def generate_embeddings_esm2(model, tokenizer, sequences, batch_size=16, max_len=1024):\n",
    "\n",
    "        seq_lens = [len(s) for s in sequences]\n",
    "\n",
    "        sort_idx = np.argsort(seq_lens)[::-1]\n",
    "\n",
    "        sorted_seqs = [sequences[i] for i in sort_idx]\n",
    "\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i in tqdm(range(0, len(sorted_seqs), batch_size), desc=\"Embedding ESM2 (Smart Batch)\"):\n",
    "\n",
    "            batch_seqs = sorted_seqs[i : i + batch_size]\n",
    "\n",
    "            ids = tokenizer.batch_encode_plus(\n",
    "\n",
    "                batch_seqs,\n",
    "\n",
    "                add_special_tokens=True,\n",
    "\n",
    "                padding=\"longest\",\n",
    "\n",
    "                truncation=True,\n",
    "\n",
    "                max_length=max_len,\n",
    "\n",
    "                return_tensors=\"pt\",\n",
    "\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                with amp_ctx:\n",
    "\n",
    "                    output = model(input_ids=ids['input_ids'], attention_mask=ids['attention_mask'])\n",
    "\n",
    "            pooled = _mean_pool(output.last_hidden_state.float(), ids['attention_mask']).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "            embeddings_list.append(pooled)\n",
    "\n",
    "            del ids, output, pooled\n",
    "\n",
    "            if device.type == 'cuda':\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        sorted_embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "        original_order_embeddings = np.zeros_like(sorted_embeddings)\n",
    "\n",
    "        original_order_embeddings[sort_idx] = sorted_embeddings\n",
    "\n",
    "        return original_order_embeddings\n",
    "\n",
    "    if esm2_train_needed or esm2_test_needed or esm2_3b_train_needed or esm2_3b_test_needed:\n",
    "\n",
    "        print(\"Loading sequences...\")\n",
    "\n",
    "        train_df = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')\n",
    "\n",
    "        test_df = None\n",
    "\n",
    "        if (WORK_ROOT / 'parsed' / 'test_seq.feather').exists():\n",
    "\n",
    "            test_df = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')\n",
    "\n",
    "        train_seqs = train_df['sequence'].tolist()\n",
    "\n",
    "        test_seqs = test_df['sequence'].tolist() if test_df is not None else None\n",
    "\n",
    "    else:\n",
    "\n",
    "        train_df = None\n",
    "\n",
    "        test_df = None\n",
    "\n",
    "        train_seqs = None\n",
    "\n",
    "        test_seqs = None\n",
    "\n",
    "    if COMPUTE_ESM2:\n",
    "\n",
    "        tokenizer, model = get_esm2_model('facebook/esm2_t33_650M_UR50D')\n",
    "\n",
    "        if esm2_train_needed:\n",
    "\n",
    "            print(f\"Generating Train Embeddings ESM2-650M ({len(train_seqs)})...\")\n",
    "\n",
    "            train_emb = generate_embeddings_esm2(model, tokenizer, train_seqs, batch_size=16)\n",
    "\n",
    "            np.save(esm2_train_path, train_emb)\n",
    "\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    STORE.maybe_push('stage_06_embeddings_esm2_train', [esm2_train_path], note='ESM2-650M mean-pool embeddings (train only)')\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f'WARN: failed to push stage_06_embeddings_esm2_train: {e}')\n",
    "\n",
    "            del train_emb\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"Train embeddings already exist at {esm2_train_path}; skipping train compute.\")\n",
    "\n",
    "        if esm2_test_needed and test_seqs is not None:\n",
    "\n",
    "            print(f\"Generating Test Embeddings ESM2-650M ({len(test_seqs)})...\")\n",
    "\n",
    "            test_emb = generate_embeddings_esm2(model, tokenizer, test_seqs, batch_size=16)\n",
    "\n",
    "            np.save(esm2_test_path, test_emb)\n",
    "\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    STORE.maybe_push('stage_06_embeddings_esm2_test', [esm2_test_path], note='ESM2-650M mean-pool embeddings (test only)')\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f'WARN: failed to push stage_06_embeddings_esm2_test: {e}')\n",
    "\n",
    "            del test_emb\n",
    "\n",
    "        else:\n",
    "\n",
    "            if has_test:\n",
    "\n",
    "                print(f\"Test embeddings already exist at {esm2_test_path}; skipping test compute.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"No test sequences found; skipping test embedding.\")\n",
    "\n",
    "        del model, tokenizer\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"ESM2-650M embeddings ready.\")\n",
    "\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "\n",
    "            paths = [esm2_train_path]\n",
    "\n",
    "            if esm2_test_path.exists():\n",
    "\n",
    "                paths.append(esm2_test_path)\n",
    "\n",
    "            try:\n",
    "\n",
    "                STORE.maybe_push('stage_06_embeddings_esm2', paths, note='ESM2-650M mean-pool embeddings')\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f'WARN: failed to push stage_06_embeddings_esm2: {e}')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"Skipping ESM2-650M embedding generation (COMPUTE_ESM2=False).\")\n",
    "\n",
    "    if COMPUTE_ESM2_3B:\n",
    "\n",
    "        tokenizer, model = get_esm2_model('facebook/esm2_t36_3B_UR50D')\n",
    "\n",
    "        if esm2_3b_train_needed:\n",
    "\n",
    "            print(f\"Generating Train Embeddings ESM2-3B ({len(train_seqs)})...\")\n",
    "\n",
    "            train_emb = generate_embeddings_esm2(model, tokenizer, train_seqs, batch_size=4)\n",
    "\n",
    "            np.save(esm2_3b_train_path, train_emb)\n",
    "\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    STORE.maybe_push('stage_06b_embeddings_esm2_3b_train', [esm2_3b_train_path], note='ESM2-3B mean-pool embeddings (train only)')\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f'WARN: failed to push stage_06b_embeddings_esm2_3b_train: {e}')\n",
    "\n",
    "            del train_emb\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"Train embeddings already exist at {esm2_3b_train_path}; skipping train compute.\")\n",
    "\n",
    "        if esm2_3b_test_needed and test_seqs is not None:\n",
    "\n",
    "            print(f\"Generating Test Embeddings ESM2-3B ({len(test_seqs)})...\")\n",
    "\n",
    "            test_emb = generate_embeddings_esm2(model, tokenizer, test_seqs, batch_size=4)\n",
    "\n",
    "            np.save(esm2_3b_test_path, test_emb)\n",
    "\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "\n",
    "                try:\n",
    "\n",
    "                    STORE.maybe_push('stage_06b_embeddings_esm2_3b_test', [esm2_3b_test_path], note='ESM2-3B mean-pool embeddings (test only)')\n",
    "\n",
    "                except Exception as e:\n",
    "\n",
    "                    print(f'WARN: failed to push stage_06b_embeddings_esm2_3b_test: {e}')\n",
    "\n",
    "            del test_emb\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "        else:\n",
    "\n",
    "            if has_test:\n",
    "\n",
    "                print(f\"Test embeddings already exist at {esm2_3b_test_path}; skipping test compute.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"No test sequences found; skipping test embedding.\")\n",
    "\n",
    "        del model, tokenizer\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"ESM2-3B embeddings ready.\")\n",
    "\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "\n",
    "            paths = [esm2_3b_train_path]\n",
    "\n",
    "            if esm2_3b_test_path.exists():\n",
    "\n",
    "                paths.append(esm2_3b_test_path)\n",
    "\n",
    "            try:\n",
    "\n",
    "                STORE.maybe_push('stage_06b_embeddings_esm2_3b', paths, note='ESM2-3B mean-pool embeddings')\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f'WARN: failed to push stage_06b_embeddings_esm2_3b: {e}')\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(\"Skipping ESM2-3B embedding generation (COMPUTE_ESM2_3B=False).\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"All requested ESM2 embedding artefacts already exist; skipping compute.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedb58b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "49a1e7eeb59549f7bde6b5dc77a5443c",
      "94258441ba164dd6a791e659d0a3ea13",
      "8113820c64dd40118afafc8a56041910",
      "7360f7ccef5349cb816c1774a30ae7e9",
      "bd89145d91c54ed4be02ad48e7558271",
      "9a854f2e91cd4f56b0937e4f222abb5c",
      "7850656efc39421283d8f13684b311ac",
      "cde1407606464fd79d9aa61ef44428dd",
      "83a2b49fb4564294aa88469de130fda8",
      "9869069e7adc4fc29569093e9818faf8",
      "83d205cd553b4a419738077b6eea679c",
      "3bcee3914b8f472885d1edc1e5f062c7",
      "54aee4cf51a3433d8dde5e0ed1b6cb45",
      "388a3ca0d6d340ebb05e13044802ba91",
      "8aebce47ff0f40bd9ff677669c73cff2",
      "cf6a6078b24c4682b53b982ca82fef9a",
      "d5c40b93b1b24c4d87f504ad48568979",
      "0f0f149d523b42fc9c547995d844d0d1",
      "310911b6bd4d47c3958e3aea1a3bf2d0",
      "1ffca17f62e24e85a02c7a03d300f691",
      "e2061fb21a18495eab8241fa7bc8c6eb",
      "ee0a833bc86b459eba420ad0c96b939d",
      "088aab8fb2474be8907b4b93e1acb3ae",
      "4471863dc9c64f8982c8f84fbd0929b6",
      "a3afb8c80fe043ec90542de597a7b24c",
      "bbc67a4be0254994aa963f470e366c45",
      "a6938cdc756a4a449bbf1e4876a928dc",
      "2c31b96c78624ca8acb17e3e8cdf007f",
      "469cb5f1147446e4a7bca4d1b00e7f74",
      "8aa7f2081b26436dbb9b5f6a867a8c07",
      "6600ee0c54ee4903adb2a28a70d5950d",
      "f94a7c141ba1444f9070a9ffa788f447",
      "fce09728b0f0497992d9104932bfd2ea",
      "62fdfb366acf45ca95e556ab5ef6d483",
      "1b4eb7d199504354884cd3d1270267c4",
      "08e42bc79a764754a9c29ae767a73056",
      "c7e4de822c564d24a7c4bb58c107f980",
      "011a2984555b43408e4e1c5e163b88d1",
      "e5eccbaab4174e6db90be072660966d8",
      "dbf709b4d0654086b44622309fcd4b4f",
      "200129b724424cefba7eab313c236046",
      "e036cfd62a1148989a25c8455b8af964",
      "7308a11db52f4cc1884a94f457cc5d0a",
      "c784e4371d134289a029a1fefd7f8567",
      "86e8814ed91749c999feb3bec80e3404",
      "6439fbe2447e4f059904d7af4806aa65",
      "0b97f98e312a414693ae24c8e1adee8b",
      "ab3bb536171f47a98a4f98ee8f46ab39",
      "c744b89abd6e4c2183f432b840737a24",
      "663d716c63f042699b2b49e3c9aaea60",
      "7a9b1a8477cb437ea2dcd8ac38a52311",
      "b0ee2c338d3c4dd499bfd94b3e0d6b72",
      "725e70ff793949459a453ecc5cc47af2",
      "42f83c186d1a4fdd89cc9cb37943c9ec",
      "a4d42157de5744dcbba3a001dfbe9f86",
      "fc3e7727c4ab4e20a44014e13eebd610",
      "29a5e01835c645b8ada633c177d1a081",
      "15d93ab946bb49db9ac9e19e4e8dd772",
      "f87e7e87dc554744a85d6e75222b8410",
      "a709033de4a642b89430f2eb009d4a05",
      "78986fba9af248fb89683bb9ee1ca0f7",
      "9bb0e9eb949541ac8a4fbb64d0d19be4",
      "cfc5bd70a63845fb8f83fb4a0dbedb4a",
      "9032af986942492686d77fd438b65011",
      "d3edc963c4f2461197f769252c3bfee6",
      "69df82b9c6f14ddb99b8dee43fbd8d1a",
      "f3403693f736496f87a35cd05dbfb619",
      "1fc5cc8957c84d91bd3b6b26f6ad3bd8",
      "ab2d76783f3044be907ea3019858b1e9",
      "e8077f535fd048da929e5c39896fbbf8",
      "22ddcb4f9fc24df7b51f964662b38767",
      "c1fcdea1c4f24685ace973a7a7ad4b63",
      "c66898ee085b40c28df2cddac70c9f5c",
      "8fafe0b9758f45f182cd379c39ef8751",
      "cb52621d45004c1c869beb22ba10abf1",
      "ed9a029942704ac8ac66abf47546f0b2",
      "a6f8a8cb5d5945dd9cb65d836d91463d"
     ]
    },
    "id": "0eedb58b",
    "outputId": "df00dafb-a079-40ce-df1b-6945948a1afe"
   },
   "outputs": [],
   "source": [
    "# CELL 12b - Solution: 3c. PHASE 1: EMBEDDINGS GENERATION (ANKH only)\n",
    "\n",
    "# 3c. PHASE 1: EMBEDDINGS GENERATION (ANKH only)\n",
    "\n",
    "# HARDWARE: GPU strongly recommended\n",
    "\n",
    "# Split from ESM2 so you can run Ankh independently.\n",
    "\n",
    "import os\n",
    "\n",
    "DEFAULT_ANKH_MODEL = 'ElnaggarLab/ankh-large'\n",
    "ANKH_MODEL = os.environ.get('CAFA_ANKH_MODEL', DEFAULT_ANKH_MODEL)\n",
    "\n",
    "# BATCH SIZE CONFIGURATION\n",
    "# Local (8GB VRAM): 2\n",
    "# Colab L4/A100 (22.5GB+ VRAM): 8 (safe for Large), 32 (safe for Base)\n",
    "BATCH_SIZE = 8 if globals().get('IS_COLAB', False) else 2\n",
    "\n",
    "# Champion-aligned tokenisation: space-separate residues by default.\n",
    "# Set CAFA_ANKH_SPACE_SEP=0 to disable.\n",
    "ANKH_SPACE_SEP = bool(int(os.environ.get('CAFA_ANKH_SPACE_SEP', '1')))\n",
    "\n",
    "\n",
    "def _safe_model_slug(model_name: str) -> str:\n",
    "    return ''.join(ch if (ch.isalnum() or ch in ['-', '_']) else '_' for ch in model_name)\n",
    "\n",
    "\n",
    "ankh_suffix = '' if ANKH_MODEL == DEFAULT_ANKH_MODEL else f\"_{_safe_model_slug(ANKH_MODEL)}\"\n",
    "\n",
    "ankh_train_path = WORK_ROOT / 'features' / f'train_embeds_ankh{ankh_suffix}.npy'\n",
    "ankh_test_path = WORK_ROOT / 'features' / f'test_embeds_ankh{ankh_suffix}.npy'\n",
    "\n",
    "has_test = (WORK_ROOT / 'parsed' / 'test_seq.feather').exists()\n",
    "\n",
    "ankh_train_exists = ankh_train_path.exists()\n",
    "ankh_test_exists = ankh_test_path.exists()\n",
    "\n",
    "# Required behaviour: skip train if it already exists; otherwise build it.\n",
    "# Then do the same for test.\n",
    "ankh_train_needed = not ankh_train_exists\n",
    "ankh_test_needed = has_test and (not ankh_test_exists)\n",
    "\n",
    "print(f\"Ankh model: {ANKH_MODEL}\")\n",
    "print(f\"Train embeddings: {ankh_train_path} (exists={ankh_train_exists}, needed={ankh_train_needed})\")\n",
    "print(f\"Test embeddings : {ankh_test_path} (exists={ankh_test_exists}, needed={ankh_test_needed}, has_test={has_test})\")\n",
    "\n",
    "if (not ankh_train_needed) and (not ankh_test_needed):\n",
    "    if PUSH_EXISTING_CHECKPOINTS and 'STORE' in globals() and STORE is not None:\n",
    "        try:\n",
    "            STORE.maybe_push(\n",
    "                'stage_06c_embeddings_ankh_train',\n",
    "                [ankh_train_path],\n",
    "                note=f'Ankh mean-pool embeddings (train only, existing) {ANKH_MODEL}',\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06c_embeddings_ankh_train: {e}')\n",
    "\n",
    "        if ankh_test_path.exists():\n",
    "            try:\n",
    "                STORE.maybe_push(\n",
    "                    'stage_06c_embeddings_ankh_test',\n",
    "                    [ankh_test_path],\n",
    "                    note=f'Ankh mean-pool embeddings (test only, existing) {ANKH_MODEL}',\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06c_embeddings_ankh_test: {e}')\n",
    "\n",
    "        paths = [ankh_train_path]\n",
    "        if ankh_test_path.exists():\n",
    "            paths.append(ankh_test_path)\n",
    "\n",
    "        try:\n",
    "            STORE.maybe_push(\n",
    "                'stage_06c_embeddings_ankh',\n",
    "                paths,\n",
    "                note=f'Ankh mean-pool embeddings (existing) {ANKH_MODEL}',\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06c_embeddings_ankh: {e}')\n",
    "\n",
    "    print(\"Skipping Ankh embedding generation (all requested artefacts already exist).\")\n",
    "else:\n",
    "    import contextlib\n",
    "    import gc\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    from transformers import AutoTokenizer, T5EncoderModel\n",
    "\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device} with Batch Size: {BATCH_SIZE}\")\n",
    "    print(f\"ANKH_SPACE_SEP={ANKH_SPACE_SEP} (set CAFA_ANKH_SPACE_SEP=0 to disable)\")\n",
    "\n",
    "    # Disable AMP for Ankh to prevent instability/retries\n",
    "    # amp_ctx = torch.amp.autocast('cuda') if device.type == 'cuda' else contextlib.nullcontext()\n",
    "\n",
    "    def _mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        x = last_hidden_state * mask\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "        return x.sum(dim=1) / denom\n",
    "\n",
    "    def get_ankh_model(model_name: str):\n",
    "        print(f\"Loading Ankh model: {model_name}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Ankh is T5-based. AutoModel would load encoder-decoder and require decoder inputs.\n",
    "        # Use encoder-only model for embeddings.\n",
    "        model = T5EncoderModel.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        return tokenizer, model\n",
    "\n",
    "    def _sanitize_seq(seq: str) -> str:\n",
    "        # Replace rare/ambiguous residues with 'X'\n",
    "        seq = seq.replace('U', 'X').replace('Z', 'X').replace('O', 'X').replace('B', 'X')\n",
    "        # Champion-aligned: space-separated residues\n",
    "        if ANKH_SPACE_SEP:\n",
    "            seq = ' '.join(list(seq))\n",
    "        return seq\n",
    "\n",
    "    def generate_embeddings_ankh(model, tokenizer, sequences, batch_size=2, max_len=1024):\n",
    "        # Sort by *true residue length* (pre-space) for stable smart batching.\n",
    "        seq_lens = [len(s) for s in sequences]\n",
    "        sort_idx = np.argsort(seq_lens)[::-1]\n",
    "\n",
    "        sorted_seqs = [_sanitize_seq(sequences[i]) for i in sort_idx]\n",
    "\n",
    "        embeddings_list = []\n",
    "\n",
    "        for batch_start in tqdm(range(0, len(sorted_seqs), batch_size), desc=\"Embedding ANKH (Smart Batch)\"):\n",
    "            batch_seqs = sorted_seqs[batch_start : batch_start + batch_size]\n",
    "\n",
    "            ids = tokenizer.batch_encode_plus(\n",
    "                batch_seqs,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            # AMP DISABLED: Ankh is unstable in fp16, causing infinite retry loops.\n",
    "            # We run in full precision (float32) to ensure stability and avoid 2x runtime from retries.\n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids=ids['input_ids'], attention_mask=ids['attention_mask'])\n",
    "\n",
    "            last = output.last_hidden_state\n",
    "\n",
    "            # Force float32 for pooling/aggregation\n",
    "            pooled_t = _mean_pool(last.float(), ids['attention_mask'])\n",
    "\n",
    "            # Batch-level finite check\n",
    "            if torch.isfinite(pooled_t).all():\n",
    "                pooled = pooled_t.detach().cpu().numpy().astype(np.float32)\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    f\"Ankh produced non-finite embeddings even in float32 (batch_start={batch_start}).\"\n",
    "                )\n",
    "\n",
    "            embeddings_list.append(pooled)\n",
    "\n",
    "            del ids, output, pooled_t, pooled\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        sorted_embeddings = np.vstack(embeddings_list)\n",
    "\n",
    "        # Final guardrail: never return corrupted arrays.\n",
    "        if not np.isfinite(sorted_embeddings).all():\n",
    "            raise RuntimeError(\"Final embeddings contain non-finite values; refusing to proceed.\")\n",
    "\n",
    "        original_order_embeddings = np.zeros_like(sorted_embeddings)\n",
    "        original_order_embeddings[sort_idx] = sorted_embeddings\n",
    "        return original_order_embeddings\n",
    "\n",
    "    # Load only what we actually need.\n",
    "    train_seqs = None\n",
    "    test_seqs = None\n",
    "    if ankh_train_needed:\n",
    "        print(\"Loading train sequences...\")\n",
    "        train_df = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')\n",
    "        train_seqs = train_df['sequence'].tolist()\n",
    "    if ankh_test_needed:\n",
    "        if (WORK_ROOT / 'parsed' / 'test_seq.feather').exists():\n",
    "            print(\"Loading test sequences...\")\n",
    "            test_df = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')\n",
    "            test_seqs = test_df['sequence'].tolist()\n",
    "        else:\n",
    "            test_seqs = None\n",
    "\n",
    "    tokenizer, model = get_ankh_model(ANKH_MODEL)\n",
    "\n",
    "    if ankh_train_needed:\n",
    "        print(f\"Generating Train Embeddings ANKH ({len(train_seqs)}) -> {ankh_train_path}\")\n",
    "        train_emb = generate_embeddings_ankh(model, tokenizer, train_seqs, batch_size=BATCH_SIZE)\n",
    "\n",
    "        if not np.isfinite(train_emb).all():\n",
    "            raise RuntimeError(\"Refusing to save: train embeddings contain non-finite values.\")\n",
    "\n",
    "        np.save(ankh_train_path, train_emb)\n",
    "\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            try:\n",
    "                STORE.maybe_push(\n",
    "                    'stage_06c_embeddings_ankh_train',\n",
    "                    [ankh_train_path],\n",
    "                    note=f'Ankh mean-pool embeddings (train only) {ANKH_MODEL}',\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06c_embeddings_ankh_train: {e}')\n",
    "\n",
    "        del train_emb\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(f\"Train embeddings already exist at {ankh_train_path}; skipping train compute.\")\n",
    "\n",
    "    if ankh_test_needed and test_seqs is not None:\n",
    "        print(f\"Generating Test Embeddings ANKH ({len(test_seqs)}) -> {ankh_test_path}\")\n",
    "        test_emb = generate_embeddings_ankh(model, tokenizer, test_seqs, batch_size=BATCH_SIZE)\n",
    "\n",
    "        if not np.isfinite(test_emb).all():\n",
    "            raise RuntimeError(\"Refusing to save: test embeddings contain non-finite values.\")\n",
    "\n",
    "        np.save(ankh_test_path, test_emb)\n",
    "\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            try:\n",
    "                STORE.maybe_push(\n",
    "                    'stage_06c_embeddings_ankh_test',\n",
    "                    [ankh_test_path],\n",
    "                    note=f'Ankh mean-pool embeddings (test only) {ANKH_MODEL}',\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06c_embeddings_ankh_test: {e}')\n",
    "\n",
    "        del test_emb\n",
    "        gc.collect()\n",
    "    else:\n",
    "        if has_test:\n",
    "            print(f\"Test embeddings already exist at {ankh_test_path}; skipping test compute.\")\n",
    "        else:\n",
    "            print(\"No test sequences found; skipping test embedding.\")\n",
    "\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"ANKH embeddings ready.\")\n",
    "\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        paths = [ankh_train_path]\n",
    "        if ankh_test_path.exists():\n",
    "            paths.append(ankh_test_path)\n",
    "\n",
    "        try:\n",
    "            STORE.maybe_push('stage_06c_embeddings_ankh', paths, note=f'Ankh mean-pool embeddings {ANKH_MODEL}')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06c_embeddings_ankh: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "547acee5",
   "metadata": {
    "id": "547acee5",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUDITOR] GPU Detected: NVIDIA GeForce RTX 2070\n",
      "[AUDITOR] VRAM: 8.59 GB\n",
      "Loading targets...\n",
      "Applying ID cleaning fix...\n",
      "Selecting Top-K terms per aspect (Champion Strategy)...\n",
      "Global PATH_GO_OBO set to: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\cafa6_data\\Train\\go-basic.obo\n",
      "Loading OBO from c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\cafa6_data\\Train\\go-basic.obo...\n",
      "  Selected: 10000 BP + 2000 MF + 1500 CC\n",
      "Loaded existing top_terms_13500.json (n=13500)\n",
      "Loaded existing stable_terms_1585.json (n=1585)\n",
      "Stable targets ready: n=1585 (expected 1585)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.23 GiB for an array with shape (13500, 81865) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31100\\844363175.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[0mtrain_terms_top\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_terms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_terms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"term\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m     \u001b[0mY_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_terms_top\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"EntryID\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"term\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m     \u001b[0mY_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ids_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m   9507\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpivot_table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9509\u001b[1;33m         return pivot_table(\n\u001b[0m\u001b[0;32m   9510\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9511\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36mpivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pivot_table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     table = __internal_pivot_table(\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\pivot.py\u001b[0m in \u001b[0;36m__internal_pivot_table\u001b[1;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msort\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   7377\u001b[0m         \u001b[0md\u001b[0m  \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7378\u001b[0m         \"\"\"\n\u001b[1;32m-> 7379\u001b[1;33m         return super().sort_index(\n\u001b[0m\u001b[0;32m   7380\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7381\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msort_index\u001b[1;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[0;32m   5315\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5316\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5317\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5319\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6809\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6810\u001b[0m         \"\"\"\n\u001b[1;32m-> 6811\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6812\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6813\u001b[0m         return self._constructor_from_mgr(data, axes=data.axes).__finalize__(\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"copy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m                 \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[0mrefs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBlockValuesRefs\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m             \u001b[0mrefs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.23 GiB for an array with shape (13500, 81865) and data type int64"
     ]
    }
   ],
   "source": [
    "# CELL 13a - Setup & Data Loading (Phase 2 canonical)\n",
    "# =============================================\n",
    "# 4. PHASE 2: LEVEL-1 MODELS (DIVERSE ENSEMBLE)\n",
    "# =============================================\n",
    "# Target selection source-of-truth: Colab_04b_first_submission_no_ankh.ipynb (aspect-split Top-K)\n",
    "\n",
    "TRAIN_LEVEL1 = True\n",
    "\n",
    "if TRAIN_LEVEL1:\n",
    "    import gc\n",
    "    import json\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import psutil\n",
    "\n",
    "    # AUDITOR: Hardware Check\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"[AUDITOR] GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "            print(\n",
    "                f\"[AUDITOR] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"[AUDITOR] WARNING: No GPU detected.\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    def log_mem(tag: str = \"\") -> None:\n",
    "        try:\n",
    "            mem = psutil.virtual_memory()\n",
    "            print(\n",
    "                f\"[MEM] {tag:<30} | Used: {mem.used/1e9:.2f}GB / {mem.total/1e9:.2f}GB ({mem.percent}%)\"\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # WORK_ROOT recovery (safety)\n",
    "    # Prefer canonical dataset root (cafa6_data/) and validate by presence of parsed artefacts.\n",
    "    if \"WORK_ROOT\" not in locals() and \"WORK_ROOT\" not in globals():\n",
    "        candidates = [\n",
    "            Path(\"/content/cafa6_data\"),\n",
    "            Path(\"/content/work\"),\n",
    "            Path(\"/kaggle/working/work\"),\n",
    "            Path.cwd() / \"cafa6_data\",\n",
    "            Path.cwd() / \"artefacts_local\" / \"work\",\n",
    "        ]\n",
    "\n",
    "        WORK_ROOT = None\n",
    "        for c in candidates:\n",
    "            if (c / \"parsed\" / \"train_terms.parquet\").exists():\n",
    "                WORK_ROOT = c\n",
    "                break\n",
    "\n",
    "        if WORK_ROOT is None:\n",
    "            for c in candidates:\n",
    "                if c.exists():\n",
    "                    WORK_ROOT = c\n",
    "                    break\n",
    "\n",
    "        if WORK_ROOT is None:\n",
    "            WORK_ROOT = Path.cwd() / \"cafa6_data\"\n",
    "\n",
    "        print(f\"WORK_ROOT recovered: {WORK_ROOT}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load targets + ids\n",
    "    # -----------------------------\n",
    "    print(\"Loading targets...\")\n",
    "    train_terms = pd.read_parquet(WORK_ROOT / \"parsed\" / \"train_terms.parquet\")\n",
    "    train_ids = pd.read_feather(WORK_ROOT / \"parsed\" / \"train_seq.feather\")[\"id\"].astype(str)\n",
    "    test_ids = pd.read_feather(WORK_ROOT / \"parsed\" / \"test_seq.feather\")[\"id\"].astype(str)\n",
    "\n",
    "    # FIX: Clean IDs in train_ids to match EntryID format\n",
    "    print(\"Applying ID cleaning fix...\")\n",
    "    train_ids_clean = train_ids.str.extract(r\"\\|(.*?)\\|\")[0]\n",
    "    train_ids_clean = train_ids_clean.fillna(train_ids)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Target Matrix Construction (Champion Strategy: 13,500 Terms)\n",
    "    # 10,000 BP + 2,000 MF + 1,500 CC\n",
    "    # -----------------------------\n",
    "    print(\"Selecting Top-K terms per aspect (Champion Strategy)...\")\n",
    "\n",
    "    try:\n",
    "        import obonet\n",
    "\n",
    "        # Robust OBO Path Search\n",
    "        possible_paths = [\n",
    "            WORK_ROOT / \"go-basic.obo\",\n",
    "            WORK_ROOT / \"Train\" / \"go-basic.obo\",\n",
    "            WORK_ROOT.parent / \"go-basic.obo\",\n",
    "            Path(\"go-basic.obo\"),\n",
    "            Path(\"Train/go-basic.obo\"),\n",
    "            Path(\"../Train/go-basic.obo\"),\n",
    "            Path(\"/content/cafa6_data/Train/go-basic.obo\"),\n",
    "        ]\n",
    "\n",
    "        obo_path = None\n",
    "        for p in possible_paths:\n",
    "            if p.exists():\n",
    "                obo_path = p\n",
    "                break\n",
    "\n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"CRITICAL: go-basic.obo not found. Searched: {[str(p) for p in possible_paths]}\"\n",
    "            )\n",
    "\n",
    "        global PATH_GO_OBO\n",
    "        PATH_GO_OBO = obo_path\n",
    "        print(f\"Global PATH_GO_OBO set to: {PATH_GO_OBO}\")\n",
    "\n",
    "        print(f\"Loading OBO from {obo_path}...\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        term_to_ns = {\n",
    "            node: data.get(\"namespace\", \"unknown\") for node, data in graph.nodes(data=True)\n",
    "        }\n",
    "\n",
    "        # Keep compatibility with downstream code that expects go_namespaces\n",
    "        go_namespaces = term_to_ns\n",
    "\n",
    "        ns_map = {\n",
    "            \"biological_process\": \"BP\",\n",
    "            \"molecular_function\": \"MF\",\n",
    "            \"cellular_component\": \"CC\",\n",
    "        }\n",
    "\n",
    "        # Normalise any existing aspect column (some artefacts store full namespace strings)\n",
    "        aspect_aliases = {\n",
    "            \"biological_process\": \"BP\",\n",
    "            \"molecular_function\": \"MF\",\n",
    "            \"cellular_component\": \"CC\",\n",
    "            \"BP\": \"BP\",\n",
    "            \"MF\": \"MF\",\n",
    "            \"CC\": \"CC\",\n",
    "        }\n",
    "        if \"aspect\" in train_terms.columns:\n",
    "            train_terms[\"aspect\"] = train_terms[\"aspect\"].map(\n",
    "                lambda a: aspect_aliases.get(str(a), \"UNK\")\n",
    "            )\n",
    "        else:\n",
    "            train_terms[\"aspect\"] = train_terms[\"term\"].map(\n",
    "                lambda t: ns_map.get(term_to_ns.get(t), \"UNK\")\n",
    "            )\n",
    "\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"obonet not installed. Please install it.\") from e\n",
    "\n",
    "    # Canonical aspect split (04b)\n",
    "    term_counts = train_terms.groupby([\"aspect\", \"term\"]).size().reset_index(name=\"count\")\n",
    "    targets_bp = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"BP\"].nlargest(10000, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "    targets_mf = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"MF\"].nlargest(2000, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "    targets_cc = (\n",
    "        term_counts[term_counts[\"aspect\"] == \"CC\"].nlargest(1500, \"count\")[\"term\"].tolist()\n",
    "    )\n",
    "\n",
    "    # Guardrail: avoid silently switching target strategy due to aspect encoding mismatch\n",
    "    ALLOW_GLOBAL_FALLBACK = False\n",
    "    if len(targets_bp) == 0 and len(targets_mf) == 0 and len(targets_cc) == 0:\n",
    "        aspect_vc = train_terms[\"aspect\"].value_counts().to_dict() if \"aspect\" in train_terms.columns else {}\n",
    "        msg = (\n",
    "            \"No BP/MF/CC aspect split found after normalisation. \"\n",
    "            f\"aspect_vc={aspect_vc}. This would fall back to global Top-13,500; \"\n",
    "            \"set ALLOW_GLOBAL_FALLBACK=True to override.\"\n",
    "        )\n",
    "        if ALLOW_GLOBAL_FALLBACK:\n",
    "            print(\"  [WARNING] \" + msg)\n",
    "            top_terms = train_terms[\"term\"].value_counts().head(13500).index.tolist()\n",
    "        else:\n",
    "            raise RuntimeError(msg)\n",
    "    else:\n",
    "        # Stable, deterministic ordering: BP then MF then CC with de-dup preserving order\n",
    "        top_terms = []\n",
    "        seen = set()\n",
    "        for t in (targets_bp + targets_mf + targets_cc):\n",
    "            if t not in seen:\n",
    "                top_terms.append(t)\n",
    "                seen.add(t)\n",
    "        print(f\"  Selected: {len(targets_bp)} BP + {len(targets_mf)} MF + {len(targets_cc)} CC\")\n",
    "\n",
    "    # Persist label contract for downstream stages\n",
    "    top_terms_path = WORK_ROOT / \"features\" / \"top_terms_13500.json\"\n",
    "    top_terms_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if top_terms_path.exists():\n",
    "        try:\n",
    "            with open(top_terms_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                top_terms_disk = json.load(f)\n",
    "            if isinstance(top_terms_disk, list) and len(top_terms_disk) > 0:\n",
    "                top_terms = [str(x) for x in top_terms_disk]\n",
    "                print(f\"Loaded existing top_terms_13500.json (n={len(top_terms)})\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to load existing top_terms_13500.json: {e}\")\n",
    "    else:\n",
    "        with open(top_terms_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(list(top_terms), f)\n",
    "        print(\"Saved: top_terms_13500.json\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Stable target contract (audited: 1,585 terms)\n",
    "    # Definition: GO terms with >= 50 positives AND valid namespace (BP/MF/CC)\n",
    "    # Stored separately from top_terms_13500.json (do not mix contracts).\n",
    "    # -----------------------------\n",
    "    stable_terms_path = WORK_ROOT / \"features\" / \"stable_terms_1585.json\"\n",
    "    stable_meta_path = WORK_ROOT / \"features\" / \"stable_terms_1585_meta.json\"\n",
    "    noise_floor = 50\n",
    "\n",
    "    if stable_terms_path.exists():\n",
    "        try:\n",
    "            stable_terms = json.loads(stable_terms_path.read_text(encoding=\"utf-8\"))\n",
    "            stable_terms = [str(t) for t in stable_terms]\n",
    "            print(f\"Loaded existing stable_terms_1585.json (n={len(stable_terms)})\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load {stable_terms_path}: {e}\")\n",
    "    else:\n",
    "        # Compute from Phase-1 truth (train_terms.parquet) and OBO namespace mapping already loaded above.\n",
    "        stable_bp = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"BP\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_mf = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"MF\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_cc = (\n",
    "            term_counts[(term_counts[\"aspect\"] == \"CC\") & (term_counts[\"count\"] >= noise_floor)]\n",
    "            .sort_values([\"count\", \"term\"], ascending=[False, True])[\"term\"]\n",
    "            .astype(str)\n",
    "            .tolist()\n",
    "        )\n",
    "        stable_terms = stable_bp + stable_mf + stable_cc\n",
    "        stable_terms_path.write_text(json.dumps(stable_terms), encoding=\"utf-8\")\n",
    "        stable_meta_path.write_text(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"noise_floor\": noise_floor,\n",
    "                    \"counts\": {\"BP\": len(stable_bp), \"MF\": len(stable_mf), \"CC\": len(stable_cc)},\n",
    "                    \"total\": len(stable_terms),\n",
    "                },\n",
    "                indent=2,\n",
    "            ),\n",
    "            encoding=\"utf-8\",\n",
    "        )\n",
    "        print(f\"Saved: stable_terms_1585.json (n={len(stable_terms)})\")\n",
    "\n",
    "    if len(stable_terms) != 1585:\n",
    "        raise RuntimeError(f\"Stable term contract mismatch: expected 1585, got {len(stable_terms)}\")\n",
    "\n",
    "    top_term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "    missing_stable = [t for t in stable_terms if t not in top_term_to_idx]\n",
    "    if missing_stable:\n",
    "        raise RuntimeError(\n",
    "            \"Stable terms contain items not present in top_terms_13500.json. \"\n",
    "            f\"Missing={len(missing_stable)} (example: {missing_stable[:10]})\"\n",
    "        )\n",
    "\n",
    "    stable_idx = np.asarray([top_term_to_idx[t] for t in stable_terms], dtype=np.int64)\n",
    "    print(f\"Stable targets ready: n={int(stable_idx.shape[0])} (expected 1585)\")\n",
    "\n",
    "    train_terms_top = train_terms[train_terms[\"term\"].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index=\"EntryID\", columns=\"term\", aggfunc=\"size\", fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids_clean, fill_value=0)\n",
    "    Y = Y_df.values.astype(np.float32)\n",
    "    print(f\"Targets: Y={Y.shape}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Feature loading helper (Memory Optimised)\n",
    "    # -----------------------------\n",
    "    FEAT_DIR = WORK_ROOT / \"features\"\n",
    "\n",
    "    def load_features_dict(split: str = \"both\"):\n",
    "        log_mem(f\"Start load_features_dict({split})\")\n",
    "        print(f\"Loading multimodal features (mode={split})...\")\n",
    "\n",
    "        ft_train = {}\n",
    "        ft_test = {}\n",
    "\n",
    "        def _load_pair(stem: str):\n",
    "            tr = FEAT_DIR / f\"train_embeds_{stem}.npy\"\n",
    "            te = FEAT_DIR / f\"test_embeds_{stem}.npy\"\n",
    "            return tr, te\n",
    "\n",
    "        # All modalities are mandatory.\n",
    "        stems = [\n",
    "            (\"t5\", \"t5\"),\n",
    "            (\"esm2\", \"esm2_650m\"),\n",
    "            (\"esm2_3b\", \"esm2_3b\"),\n",
    "            (\"ankh\", \"ankh\"),\n",
    "            (\"text\", \"text\"),\n",
    "        ]\n",
    "\n",
    "        for stem, key in stems:\n",
    "            tr_path, te_path = _load_pair(stem)\n",
    "            if not (tr_path.exists() and te_path.exists()):\n",
    "                raise FileNotFoundError(f\"Missing mandatory embeddings for {stem}: {tr_path} or {te_path}\")\n",
    "\n",
    "            if split in [\"both\", \"train\"]:\n",
    "                ft_train[key] = np.load(tr_path, mmap_mode=\"r\")\n",
    "            if split in [\"both\", \"test\"]:\n",
    "                ft_test[key] = np.load(te_path, mmap_mode=\"r\")\n",
    "\n",
    "        taxa_train_path = WORK_ROOT / \"parsed\" / \"train_taxa.feather\"\n",
    "        taxa_test_path = WORK_ROOT / \"parsed\" / \"test_taxa.feather\"\n",
    "\n",
    "        if not (taxa_train_path.exists() and taxa_test_path.exists()):\n",
    "            raise FileNotFoundError(f\"Missing mandatory taxa features: {taxa_train_path} or {taxa_test_path}\")\n",
    "\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "        tax_tr = pd.read_feather(taxa_train_path).astype({\"id\": str})\n",
    "        tax_te = pd.read_feather(taxa_test_path).astype({\"id\": str})\n",
    "        enc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, dtype=np.float32)\n",
    "        enc.fit(pd.concat([tax_tr[[\"taxon_id\"]], tax_te[[\"taxon_id\"]]], axis=0))\n",
    "\n",
    "        if split in [\"both\", \"train\"]:\n",
    "            tax_tr = tax_tr.set_index(\"id\").reindex(train_ids, fill_value=0).reset_index()\n",
    "            ft_train[\"taxa\"] = enc.transform(tax_tr[[\"taxon_id\"]]).astype(np.float32)\n",
    "        if split in [\"both\", \"test\"]:\n",
    "            tax_te = tax_te.set_index(\"id\").reindex(test_ids, fill_value=0).reset_index()\n",
    "            ft_test[\"taxa\"] = enc.transform(tax_te[[\"taxon_id\"]]).astype(np.float32)\n",
    "\n",
    "        log_mem(f\"End load_features_dict({split})\")\n",
    "        if split == \"train\":\n",
    "            return ft_train\n",
    "        if split == \"test\":\n",
    "            return ft_test\n",
    "        return ft_train, ft_test\n",
    "\n",
    "    # Materialise feature dicts (mmap arrays where possible)\n",
    "    features_train, features_test = load_features_dict(split=\"both\")\n",
    "\n",
    "    # Flat concatenation order for classical models (LR/GBDT)\n",
    "    FLAT_KEYS = [k for k in [\"t5\", \"esm2_650m\", \"esm2_3b\", \"ankh\", \"text\", \"taxa\"] if k in features_train]\n",
    "    if \"ankh\" not in FLAT_KEYS:\n",
    "        raise RuntimeError(\"Ankh is mandatory but was not loaded into features_train.\")\n",
    "    print(f\"Flat X keys={FLAT_KEYS}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Disk-backed X / X_test (for RAM-safe downstream cells)\n",
    "    # -----------------------------\n",
    "    X_train_path = FEAT_DIR / \"X_train_mmap.npy\"\n",
    "    X_test_path = FEAT_DIR / \"X_test_mmap.npy\"\n",
    "\n",
    "    def _build_X_memmaps(chunk_size: int = 10000) -> None:\n",
    "        dims = {k: int(features_train[k].shape[1]) for k in FLAT_KEYS}\n",
    "        total_dim = int(sum(dims.values()))\n",
    "        n_tr = int(len(train_ids))\n",
    "        n_te = int(len(test_ids))\n",
    "\n",
    "        print(f\"Building X memmaps: train=({n_tr}, {total_dim}) test=({n_te}, {total_dim})\")\n",
    "        X_mm = np.lib.format.open_memmap(\n",
    "            str(X_train_path), mode=\"w+\", dtype=np.float32, shape=(n_tr, total_dim)\n",
    "        )\n",
    "        Xte_mm = np.lib.format.open_memmap(\n",
    "            str(X_test_path), mode=\"w+\", dtype=np.float32, shape=(n_te, total_dim)\n",
    "        )\n",
    "\n",
    "        col = 0\n",
    "        for k in FLAT_KEYS:\n",
    "            d = dims[k]\n",
    "            print(f\"  Streaming {k} into cols {col}:{col + d}\")\n",
    "            for i in range(0, n_tr, chunk_size):\n",
    "                j = min(i + chunk_size, n_tr)\n",
    "                X_mm[i:j, col : col + d] = np.asarray(features_train[k][i:j], dtype=np.float32)\n",
    "            for i in range(0, n_te, chunk_size):\n",
    "                j = min(i + chunk_size, n_te)\n",
    "                Xte_mm[i:j, col : col + d] = np.asarray(features_test[k][i:j], dtype=np.float32)\n",
    "            col += d\n",
    "\n",
    "        X_mm.flush()\n",
    "        Xte_mm.flush()\n",
    "\n",
    "    if X_train_path.exists() and X_test_path.exists():\n",
    "        print(\"X memmaps already exist; skipping build.\")\n",
    "    else:\n",
    "        _build_X_memmaps(chunk_size=5000)\n",
    "\n",
    "    X = np.load(X_train_path, mmap_mode=\"r\")\n",
    "    X_test = np.load(X_test_path, mmap_mode=\"r\")\n",
    "\n",
    "    log_mem(\"Phase 2 setup done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13b-DEBUG - Pre-flight checks for GBDT\n",
    "# ==========================================\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "print(\"--- GBDT Pre-flight Checks ---\")\n",
    "\n",
    "# 1. Check Globals\n",
    "required_globals = ['X', 'Y', 'X_test', 'stable_idx']\n",
    "missing = [g for g in required_globals if g not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required globals: {missing}. Did you run Cell 13a?\")\n",
    "\n",
    "# 2. Shape Checks\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"stable_idx count: {len(stable_idx)}\")\n",
    "\n",
    "if X.shape[0] != Y.shape[0]:\n",
    "    raise ValueError(f\"Mismatch: X has {X.shape[0]} rows, Y has {Y.shape[0]} rows\")\n",
    "\n",
    "if len(stable_idx) != 1585:\n",
    "    print(f\"WARNING: stable_idx has {len(stable_idx)} terms (expected 1585). Proceeding, but verify this is intended.\")\n",
    "\n",
    "if np.max(stable_idx) >= Y.shape[1]:\n",
    "    raise ValueError(f\"Invalid stable_idx: max index {np.max(stable_idx)} >= Y columns {Y.shape[1]}\")\n",
    "\n",
    "# 3. NaN/Inf Checks (Sampled for speed)\n",
    "sample_size = min(5000, X.shape[0])\n",
    "print(f\"Checking first {sample_size} rows for NaNs/Infs...\")\n",
    "\n",
    "if np.isnan(X[:sample_size]).any():\n",
    "    raise ValueError(\"NaNs detected in X (training features)!\")\n",
    "if np.isinf(X[:sample_size]).any():\n",
    "    raise ValueError(\"Infs detected in X (training features)!\")\n",
    "\n",
    "if np.isnan(X_test[:sample_size]).any():\n",
    "    raise ValueError(\"NaNs detected in X_test (test features)!\")\n",
    "\n",
    "# 4. Target Checks (Elite subset)\n",
    "Y_elite_sample = Y[:sample_size, stable_idx]\n",
    "if np.isnan(Y_elite_sample).any():\n",
    "    raise ValueError(\"NaNs detected in Y (elite targets)!\")\n",
    "\n",
    "# Check for dead targets in the elite set (using full Y if possible, or warn)\n",
    "# Since Y might be large, we'll do a quick sum check if it's in memory, otherwise skip or use sample\n",
    "if isinstance(Y, np.ndarray) and not isinstance(Y, np.memmap):\n",
    "    # In-memory, fast enough\n",
    "    positives = Y[:, stable_idx].sum(axis=0)\n",
    "    dead_targets = np.where(positives == 0)[0]\n",
    "    if len(dead_targets) > 0:\n",
    "        print(f\"WARNING: {len(dead_targets)} elite targets have 0 positive samples!\")\n",
    "else:\n",
    "    print(\"Skipping full target sum check (Y is memmap or large).\")\n",
    "\n",
    "# 5. Library Check\n",
    "try:\n",
    "    import py_boost\n",
    "    print(f\"py_boost version: {py_boost.__version__ if hasattr(py_boost, '__version__') else 'installed'}\")\n",
    "except ImportError:\n",
    "    print(\"WARNING: py_boost not found! Cell 13b will fail.\")\n",
    "\n",
    "print(\"--- CHECKS PASSED. Ready for GBDT. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13b-DIAGNOSTIC - Inspect py_boost + GPU backend (CuPy) pre-flight\n",
    "# Combines library inspection and the CuPy/py_boost GPU backend patch.\n",
    "#\n",
    "# Recommended setup (do this in the TERMINAL, not here): docs/ENV_GPU_PYBOOST_PY311.md\n",
    "#\n",
    "import importlib\n",
    "\n",
    "# --- Inspect py_boost ---\n",
    "import py_boost\n",
    "import py_boost.multioutput\n",
    "print(\"py_boost version:\", py_boost.__version__)\n",
    "print(\"py_boost contents:\", dir(py_boost))\n",
    "print(\"py_boost.multioutput contents:\", dir(py_boost.multioutput))\n",
    "try:\n",
    "    from py_boost import GradientBoosting\n",
    "    print(\"GradientBoosting imported successfully\")\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(f\"GradientBoosting import failed: {repr(e)}\") from e\n",
    "\n",
    "# --- CuPy pre-flight for py_boost GPU path ---\n",
    "# py_boost GPU path expects CuPy as `cp`. If CuPy isn't installed, py_boost can crash with: NameError: name 'cp' is not defined.\n",
    "try:\n",
    "    import cupy as cp  # type: ignore\n",
    "except Exception as e:\n",
    "    import sys\n",
    "    msg = [\n",
    "        \"CuPy is required for py_boost GPU training but is not importable in this kernel.\",\n",
    "        f\"Root cause: {repr(e)}\",\n",
    "        \"Preferred fix: follow docs/ENV_GPU_PYBOOST_PY311.md (Python 3.11 + CUDA-matched CuPy wheel).\",\n",
    "    ]\n",
    "    raise RuntimeError(\"\\n\".join(msg)) from e\n",
    "\n",
    "# Sanity-check CuPy can allocate on GPU\n",
    "try:\n",
    "    _ = cp.zeros((1,), dtype=cp.float32)\n",
    "    cp.cuda.runtime.deviceSynchronize()\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"CuPy imported but GPU allocation failed: {repr(e)}\") from e\n",
    "\n",
    "# Patch py_boost's GPU module to have `cp` in scope (workaround for some builds)\n",
    "try:\n",
    "    pb_boosting = importlib.import_module(\"py_boost.gpu.boosting\")\n",
    "    setattr(pb_boosting, \"cp\", cp)\n",
    "    print(\"[OK] CuPy available and patched into py_boost.gpu.boosting as `cp`.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to patch py_boost GPU backend: {repr(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04382edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP CELL  GBDT tiny smoke-run (fold1 only)\n",
    "# Run this on a GPU VM before the GBDT training cell to make the run cheap and audit-friendly.\n",
    "\n",
    "# Behaviour: GBDT cell will run only the first fold, then save checkpoints and stop.\n",
    "GBDT_TINY_MAX_FOLDS = 1\n",
    "print(f\"[GBDT tiny] Enabled: will run at most {GBDT_TINY_MAX_FOLDS} fold(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67aaabb",
   "metadata": {
    "id": "d67aaabb",
    "language": "python"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_LEVEL1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_47280\\204067296.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# - On restart, loads the arrays + state and skips completed folds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTRAIN_LEVEL1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Skipping GBDT (TRAIN_LEVEL1=False).'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TRAIN_LEVEL1' is not defined"
     ]
    }
   ],
   "source": [
    "# CELL 13b - Level 1: GBDT (py-boost)  Elite 1,585 stable terms + checkpoint push\n",
    "# =====================================================================\n",
    "# Track A (auditor): train GBDT only on the stable 1,585 terms (>=50 positives),\n",
    "# but emit full 13,500-wide predictions so the stacker contract stays unchanged.\n",
    "\n",
    "# Resume behaviour (added):\n",
    "# - Saves `oof_pred_gbdt.npy` + `test_pred_gbdt.npy` after EACH fold.\n",
    "# - Writes a small state file `gbdt_state_elite1585.json` listing completed folds.\n",
    "# - On restart, loads the arrays + state and skips completed folds.\n",
    "\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping GBDT (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import json\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import KFold\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gbdt_oof_path = PRED_DIR / 'oof_pred_gbdt.npy'\n",
    "    gbdt_test_path = PRED_DIR / 'test_pred_gbdt.npy'\n",
    "    gbdt_state_path = PRED_DIR / 'gbdt_state_elite1585.json'\n",
    "\n",
    "    if 'stable_idx' not in globals():\n",
    "        raise RuntimeError(\n",
    "            \"Missing stable_idx. Run Phase 2 setup (Cell 13a) first to generate stable_terms_1585.json.\"\n",
    "        )\n",
    "\n",
    "    elite_cols = np.asarray(stable_idx, dtype=np.int64)\n",
    "    if elite_cols.ndim != 1 or elite_cols.size != 1585:\n",
    "        raise RuntimeError(f\"Expected elite_cols to have shape (1585,), got {elite_cols.shape}\")\n",
    "\n",
    "    n_splits = 5\n",
    "    completed_folds: set[int] = set()\n",
    "\n",
    "    if gbdt_state_path.exists():\n",
    "        try:\n",
    "            state = json.loads(gbdt_state_path.read_text(encoding='utf-8'))\n",
    "            completed_folds = {int(x) for x in state.get('completed_folds', [])}\n",
    "        except Exception:\n",
    "            completed_folds = set()\n",
    "\n",
    "    have_arrays = gbdt_oof_path.exists() and gbdt_test_path.exists()\n",
    "    if have_arrays:\n",
    "        oof_pred_gbdt = np.load(gbdt_oof_path)\n",
    "        test_pred_gbdt = np.load(gbdt_test_path)\n",
    "        if len(completed_folds) >= n_splits:\n",
    "            print('Loaded existing GBDT preds (complete):', gbdt_oof_path.name, gbdt_test_path.name)\n",
    "        else:\n",
    "            print('Loaded partial GBDT preds; will resume.')\n",
    "            print('  completed folds:', sorted(completed_folds) if completed_folds else 'none')\n",
    "    else:\n",
    "        oof_pred_gbdt = np.zeros((X.shape[0], Y.shape[1]), dtype=np.float32)\n",
    "        test_pred_gbdt = np.zeros((X_test.shape[0], Y.shape[1]), dtype=np.float32)\n",
    "\n",
    "    if len(completed_folds) < n_splits:\n",
    "        # Auditor-aligned: manual CuPy injection into py_boost before training.\n",
    "        try:\n",
    "            import py_boost\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                'py_boost is REQUIRED for this pipeline (GBDT is mandatory). Install it with: pip install py-boost',\n",
    "            ) from e\n",
    "\n",
    "        try:\n",
    "            import cupy as cp\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                'CuPy is required for py_boost GPU training but is not importable in this kernel. '\n",
    "                'Install a CUDA-matched wheel (e.g. `pip install cupy-cuda12x`) and re-run.',\n",
    "            ) from e\n",
    "\n",
    "        try:\n",
    "            import py_boost.gpu.boosting as gpu_boosting\n",
    "            gpu_boosting.cp = cp\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f'Failed to inject CuPy into py_boost.gpu.boosting: {repr(e)}') from e\n",
    "\n",
    "        # Robustness: patch cp into all py_boost submodules (some builds reference `cp` in other modules too).\n",
    "        try:\n",
    "            import importlib\n",
    "            import pkgutil\n",
    "\n",
    "            def _patch_pyboost_cp() -> int:\n",
    "                patched = 0\n",
    "                for modinfo in pkgutil.walk_packages(py_boost.__path__, prefix=py_boost.__name__ + '.'):\n",
    "                    name = modinfo.name\n",
    "                    try:\n",
    "                        m = importlib.import_module(name)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    try:\n",
    "                        setattr(m, 'cp', cp)\n",
    "                        patched += 1\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                return patched\n",
    "\n",
    "            _n_patched = _patch_pyboost_cp()\n",
    "            print(f'[GBDT] Patched CuPy alias `cp` into {_n_patched} py_boost modules.')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f'Failed to patch py_boost modules with CuPy alias `cp`: {repr(e)}') from e\n",
    "\n",
    "        from py_boost import GradientBoosting\n",
    "\n",
    "        # GPU kernel sanity: py_boost sometimes silently fails to compile/init CuPy kernels\n",
    "        # (then later crashes with: TypeError: 'NoneType' object is not callable).\n",
    "        try:\n",
    "            import py_boost.gpu.utils as gpu_utils\n",
    "            if hasattr(gpu_utils, 'init_kernels'):\n",
    "                gpu_utils.init_kernels()\n",
    "            _k = getattr(gpu_utils, 'feature_grouper_kernel', None)\n",
    "            if _k is None:\n",
    "                raise RuntimeError(\n",
    "                    'py_boost GPU kernels are not initialised (feature_grouper_kernel is None). '\n",
    "                    'This is usually a broken CUDA/CuPy toolchain on this VM (common on Python 3.12 or '\n",
    "                    'a CUDA wheel/driver mismatch). Recommended: create a fresh Python 3.11 env and reinstall '\n",
    "                    'py-boost + a CUDA-matched CuPy wheel (e.g. cupy-cuda11x or cupy-cuda12x), then restart the kernel.',\n",
    "                )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f'py_boost GPU backend is not usable: {repr(e)}') from e\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        # TEMP: allow a tiny audit run (e.g. fold-1 only) by setting GBDT_TINY_MAX_FOLDS in a prior cell.\n",
    "        tiny_max_folds = int(globals().get('GBDT_TINY_MAX_FOLDS', 0) or 0)\n",
    "        if tiny_max_folds > 0:\n",
    "            print(f'[GBDT tiny] Limiting run to first {tiny_max_folds} fold(s).')\n",
    "\n",
    "        # Train only elite targets\n",
    "        Y_elite = Y[:, elite_cols].astype(np.float32, copy=False)\n",
    "        print(f\"[GBDT] Training elite targets: {Y_elite.shape[1]} / {Y.shape[1]}\")\n",
    "\n",
    "        base_params = dict(\n",
    "            loss='logloss',\n",
    "            ntrees=150,\n",
    "            lr=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample=0.8,\n",
    "        )\n",
    "\n",
    "        # Cheap sanity check (seconds): validates the API before a long run.\n",
    "        try:\n",
    "            mini_n = int(min(100, X.shape[0]))\n",
    "            if mini_n >= 50:\n",
    "                base_check = GradientBoosting(loss='logloss', ntrees=1, seed=123)\n",
    "                X_mini = np.ascontiguousarray(np.asarray(X[:mini_n], dtype=np.float32).copy())\n",
    "                y_mini = np.ascontiguousarray(np.asarray(Y_elite[:mini_n, 0], dtype=np.float32).copy())\n",
    "                base_check.fit(X_mini, y_mini)\n",
    "                _p = np.asarray(base_check.predict(X_mini[: min(64, mini_n)]))\n",
    "                if not np.isfinite(_p).all():\n",
    "                    raise RuntimeError('Non-finite predictions in mini-fit (NaN/Inf).')\n",
    "                del base_check, X_mini, y_mini, _p\n",
    "                print('[GBDT] Mini-fit OK (API + finite preds).')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"GBDT mini-fit failed; aborting before long training. Root cause: {repr(e)}\") from e\n",
    "\n",
    "        # Performance: avoid re-reading disk-backed memmaps on every target.\n",
    "        # Materialise X_test once into RAM (best effort; safe fallback if RAM is tight).\n",
    "        try:\n",
    "            print('[GBDT] Loading X_test into RAM (one-off) ...')\n",
    "            X_test_ram = np.ascontiguousarray(np.asarray(X_test, dtype=np.float32))\n",
    "            print(f'[GBDT] X_test_ram shape={X_test_ram.shape} dtype={X_test_ram.dtype}')\n",
    "        except MemoryError:\n",
    "            X_test_ram = X_test\n",
    "            print('[GBDT] WARNING: insufficient RAM to materialise X_test; using X_test as-is (may be slower).')\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X), start=1):\n",
    "            if tiny_max_folds > 0 and fold > tiny_max_folds:\n",
    "                print(f'[GBDT tiny] Stop after fold {tiny_max_folds}.')\n",
    "                break\n",
    "            if fold in completed_folds:\n",
    "                print(f'Fold {fold}/{n_splits} (GBDT elite)  SKIP (resume)')\n",
    "                continue\n",
    "\n",
    "            print(f'Fold {fold}/{n_splits} (GBDT elite)')\n",
    "\n",
    "            # py_boost 0.5.2 uses 'seed' (auditor guidance)\n",
    "            fold_seed = 42 + fold\n",
    "\n",
    "            # Ensure fold slices are contiguous RAM arrays once per fold (numba/memmap safety).\n",
    "            # Also avoids repeated random I/O into disk-backed memmaps.\n",
    "            print('   [System] Pulling fold X into RAM ...')\n",
    "            X_tr = np.ascontiguousarray(np.asarray(X[tr_idx], dtype=np.float32))\n",
    "            X_va = np.ascontiguousarray(np.asarray(X[va_idx], dtype=np.float32))\n",
    "            # Multi-output training: single model predicts ALL 1,585 targets per fold.\n",
    "            # This removes the dominant bottleneck: calling predict(X_test) 1,585.\n",
    "            print('   [System] Pulling fold Y_elite into RAM ...')\n",
    "            Y_tr = np.ascontiguousarray(np.asarray(Y_elite[tr_idx], dtype=np.float32))\n",
    "            Y_va = np.ascontiguousarray(np.asarray(Y_elite[va_idx], dtype=np.float32))\n",
    "            if Y_tr.ndim != 2 or Y_tr.shape[1] != int(len(elite_cols)):\n",
    "                raise RuntimeError(f'Unexpected Y_tr shape: {Y_tr.shape} (expected (n_tr, {int(len(elite_cols))}))')\n",
    "            try:\n",
    "                from py_boost.multioutput.sketching import TopOutputsSketch\n",
    "                _sketch = TopOutputsSketch(topk=int(len(elite_cols)))\n",
    "            except Exception:\n",
    "                _sketch = None\n",
    "            model = GradientBoosting(\n",
    "                **base_params,\n",
    "                seed=fold_seed,\n",
    "                target_splitter='OneVsAll',\n",
    "                multioutput_sketch=_sketch,\n",
    "            )\n",
    "            t_fit = time.time()\n",
    "            # Important: pass a validation set so `es=` early stopping can actually trigger.\n",
    "            model.fit(X_tr, Y_tr, eval_sets=[{'X': X_va, 'y': Y_va}])\n",
    "            print(f'   [GBDT] Fit done in {time.time() - t_fit:.1f}s')\n",
    "            def _fix_pred_shape(p: np.ndarray, n_rows: int, n_cols: int, name: str) -> np.ndarray:\n",
    "                a = np.asarray(p, dtype=np.float32)\n",
    "                if a.ndim == 3 and a.shape[-1] == 1:\n",
    "                    a = a[..., 0]\n",
    "                if a.ndim == 1:\n",
    "                    if n_cols == 1:\n",
    "                        a = a.reshape(-1, 1)\n",
    "                    else:\n",
    "                        raise RuntimeError(f'{name} prediction is 1D {a.shape}, expected 2D (n_rows, n_cols)')\n",
    "                if a.ndim != 2:\n",
    "                    raise RuntimeError(f'{name} prediction has unexpected ndim={a.ndim}, shape={a.shape}')\n",
    "                # Some backends return transposed (n_cols, n_rows)\n",
    "                if a.shape == (n_cols, n_rows):\n",
    "                    a = a.T\n",
    "                if a.shape != (n_rows, n_cols):\n",
    "                    raise RuntimeError(f'{name} prediction shape mismatch: got {a.shape}, expected {(n_rows, n_cols)}')\n",
    "                return a\n",
    "            t_pred = time.time()\n",
    "            # GPU memory telemetry (best-effort): CuPy memory pool retains peak allocations.\n",
    "            try:\n",
    "                import cupy as cp\n",
    "                pool = cp.get_default_memory_pool()\n",
    "                pinned = cp.get_default_pinned_memory_pool()\n",
    "                free0, total0 = cp.cuda.runtime.memGetInfo()\n",
    "                pool_total0 = int(pool.total_bytes())\n",
    "                pool_used0 = int(pool.used_bytes())\n",
    "                pinned_total0 = int(pinned.total_bytes())\n",
    "            except Exception:\n",
    "                cp = None\n",
    "                free0 = total0 = pool_total0 = pool_used0 = pinned_total0 = -1\n",
    "            va = _fix_pred_shape(model.predict(X_va), n_rows=int(len(va_idx)), n_cols=int(len(elite_cols)), name='val')\n",
    "            te = _fix_pred_shape(model.predict(X_test_ram), n_rows=int(X_test_ram.shape[0]), n_cols=int(len(elite_cols)), name='test')\n",
    "            pred_s = time.time() - t_pred\n",
    "            print(f'   [GBDT] Predict done in {pred_s:.1f}s')\n",
    "            try:\n",
    "                if cp is not None:\n",
    "                    free1, total1 = cp.cuda.runtime.memGetInfo()\n",
    "                    pool_total1 = int(pool.total_bytes())\n",
    "                    pool_used1 = int(pool.used_bytes())\n",
    "                    pinned_total1 = int(pinned.total_bytes())\n",
    "                    # Note: pool_total_bytes is effectively a 'peak' allocation watermark.\n",
    "                    print(\n",
    "                        '   [GBDT][GPU mem] VRAM free/total (GiB):',\n",
    "                        f'{free1 / (1024**3):.2f}/{total1 / (1024**3):.2f}',\n",
    "                    )\n",
    "                    print(\n",
    "                        '   [GBDT][GPU mem] CuPy pool total/used (GiB):',\n",
    "                        f'{pool_total1 / (1024**3):.2f}/{pool_used1 / (1024**3):.2f}',\n",
    "                        '(total is peak-ish)',\n",
    "                    )\n",
    "                    print(\n",
    "                        '   [GBDT][GPU mem] CuPy pinned pool total (GiB):',\n",
    "                        f'{pinned_total1 / (1024**3):.2f}',\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                print('   [GBDT][GPU mem] telemetry skipped:', repr(e))\n",
    "            va = np.clip(va, 0.0, 1.0)\n",
    "            te = np.clip(te, 0.0, 1.0)\n",
    "            oof_pred_gbdt[np.ix_(va_idx, elite_cols)] = va\n",
    "            test_pred_gbdt[:, elite_cols] += te / float(n_splits)\n",
    "\n",
    "            if 'ia_weighted_f1' in globals():\n",
    "                try:\n",
    "                    va_elite = oof_pred_gbdt[np.ix_(va_idx, elite_cols)]\n",
    "                    print('  IA-F1 (elite only):', ia_weighted_f1(Y_elite[va_idx], va_elite, thr=0.3))\n",
    "                except Exception as e:\n",
    "                    print('  IA-F1 skipped:', repr(e))\n",
    "\n",
    "            # Fold checkpoint (resume)\n",
    "            completed_folds.add(fold)\n",
    "            np.save(gbdt_oof_path, oof_pred_gbdt)\n",
    "            np.save(gbdt_test_path, test_pred_gbdt)\n",
    "            gbdt_state_path.write_text(\n",
    "                json.dumps(\n",
    "                    {\n",
    "                        'n_splits': n_splits,\n",
    "                        'completed_folds': sorted(completed_folds),\n",
    "                    },\n",
    "                    indent=2,\n",
    "                ),\n",
    "                encoding='utf-8',\n",
    "            )\n",
    "            print(f'[GBDT] Saved checkpoint after fold {fold}/{n_splits}: {gbdt_oof_path.name}, {gbdt_test_path.name}')\n",
    "\n",
    "        # Final save (also covers the no-resume path)\n",
    "        np.save(gbdt_oof_path, oof_pred_gbdt)\n",
    "        np.save(gbdt_test_path, test_pred_gbdt)\n",
    "        gbdt_state_path.write_text(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    'n_splits': n_splits,\n",
    "                    'completed_folds': sorted(completed_folds) if completed_folds else list(range(1, n_splits + 1)),\n",
    "                },\n",
    "                indent=2,\n",
    "            ),\n",
    "            encoding='utf-8',\n",
    "        )\n",
    "        print('Saved:', gbdt_oof_path)\n",
    "        print('Saved:', gbdt_test_path)\n",
    "\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07b_level1_gbdt',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str((WORK_ROOT / 'features' / 'stable_terms_1585.json').as_posix()),\n",
    "            str(gbdt_oof_path.as_posix()),\n",
    "            str(gbdt_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 GBDT predictions (OOF + test). Trained on elite 1,585 stable targets; emitted as 13,500-wide matrices.',\n",
    "    )\n",
    "\n",
    "    # Diagnostics: probability histograms + IA-F1 vs threshold (sampled)\n",
    "    try:\n",
    "        import os\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "\n",
    "        def _sub(y_true: np.ndarray, y_score: np.ndarray):\n",
    "            n = int(y_true.shape[0])\n",
    "            m = min(n, int(DIAG_N))\n",
    "            if m <= 0:\n",
    "                return y_true[:0], y_score[:0]\n",
    "            idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "            return y_true[idx], y_score[idx]\n",
    "\n",
    "        y_t, y_s = _sub(Y, oof_pred_gbdt)\n",
    "        row_max_oof = y_s.max(axis=1)\n",
    "        row_mean_oof = y_s.mean(axis=1)\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_max_oof, bins=60, alpha=0.7)\n",
    "        plt.title('GBDT OOF: max probability per protein (sampled)')\n",
    "        plt.xlabel('max prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_mean_oof, bins=60, alpha=0.7)\n",
    "        plt.title('GBDT OOF: mean probability per protein (sampled)')\n",
    "        plt.xlabel('mean prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        if test_pred_gbdt is not None:\n",
    "            te_s = test_pred_gbdt\n",
    "            te_m = min(int(te_s.shape[0]), int(DIAG_N))\n",
    "            te_idx = (\n",
    "                np.linspace(0, int(te_s.shape[0]) - 1, num=te_m, dtype=np.int64)\n",
    "                if te_m > 0\n",
    "                else np.array([], dtype=np.int64)\n",
    "            )\n",
    "            row_max_te = te_s[te_idx].max(axis=1) if te_m > 0 else np.array([], dtype=np.float32)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(row_max_oof, bins=60, alpha=0.5, label='OOF')\n",
    "            plt.hist(row_max_te, bins=60, alpha=0.5, label='test')\n",
    "            plt.title('GBDT: max probability per protein (OOF vs test; sampled)')\n",
    "            plt.xlabel('max prob')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(y_t, y_s, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('GBDT OOF: IA-weighted F1 vs threshold (sampled)')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('GBDT diagnostics skipped:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33519337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP CELL  Verify GBDT artefacts + elite-only writes\n",
    "# Run AFTER the GBDT cell finishes (even in fold1 mode).\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "gbdt_oof_path = PRED_DIR / 'oof_pred_gbdt.npy'\n",
    "gbdt_test_path = PRED_DIR / 'test_pred_gbdt.npy'\n",
    "gbdt_state_path = PRED_DIR / 'gbdt_state_elite1585.json'\n",
    "\n",
    "assert gbdt_oof_path.exists(), f\"Missing {gbdt_oof_path}\"\n",
    "assert gbdt_test_path.exists(), f\"Missing {gbdt_test_path}\"\n",
    "print('[GBDT verify] Found:', gbdt_oof_path.name, gbdt_test_path.name)\n",
    "if gbdt_state_path.exists():\n",
    "    print('[GBDT verify] State:', gbdt_state_path.read_text(encoding='utf-8')[:500])\n",
    "\n",
    "oof = np.load(gbdt_oof_path, mmap_mode='r')\n",
    "te = np.load(gbdt_test_path, mmap_mode='r')\n",
    "\n",
    "print('[GBDT verify] oof shape:', tuple(oof.shape), 'expected:', (int(X.shape[0]), int(Y.shape[1])))\n",
    "print('[GBDT verify] test shape:', tuple(te.shape), 'expected:', (int(X_test.shape[0]), int(Y.shape[1])))\n",
    "assert oof.shape == (X.shape[0], Y.shape[1])\n",
    "assert te.shape == (X_test.shape[0], Y.shape[1])\n",
    "\n",
    "elite_cols = np.asarray(stable_idx, dtype=np.int64)\n",
    "elite_mask = np.zeros((Y.shape[1],), dtype=bool)\n",
    "elite_mask[elite_cols] = True\n",
    "outside_cols = np.flatnonzero(~elite_mask)\n",
    "print('[GBDT verify] elite cols:', int(elite_cols.size), 'outside cols:', int(outside_cols.size))\n",
    "\n",
    "# Cheap check: sample outside entries and confirm they are all exactly zero.\n",
    "# This avoids a full scan of a multi-GB matrix while still catching accidental writes.\n",
    "rng = np.random.default_rng(123)\n",
    "n_check_rows = int(min(2048, oof.shape[0]))\n",
    "n_check_cols = int(min(2048, outside_cols.size))\n",
    "rows = rng.choice(oof.shape[0], size=n_check_rows, replace=False) if n_check_rows < oof.shape[0] else np.arange(oof.shape[0])\n",
    "cols = rng.choice(outside_cols, size=n_check_cols, replace=False) if n_check_cols < outside_cols.size else outside_cols\n",
    "block = np.asarray(oof[np.ix_(rows, cols)])\n",
    "max_abs = float(np.max(np.abs(block))) if block.size else 0.0\n",
    "print(f\"[GBDT verify] sampled outside max|value|={max_abs:.3g} (expect 0)\")\n",
    "assert max_abs == 0.0, 'Non-zero detected outside elite_cols (sampled). Investigate writes.'\n",
    "\n",
    "# Sanity: elite slice should contain some non-zeros after at least one fold (unless model failed silently).\n",
    "elite_sample = np.asarray(oof[np.ix_(rows, elite_cols[: min(64, elite_cols.size)])])\n",
    "elite_max = float(np.max(elite_sample)) if elite_sample.size else 0.0\n",
    "print(f\"[GBDT verify] sampled elite max={elite_max:.3g} (expect > 0 if fold ran)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae005d",
   "metadata": {
    "id": "79ae005d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 13c - Level 1: Logistic Regression  Long tail 13,500 (Aspect Split BP/MF/CC)\n",
    "# ==============================================================================\n",
    "# Track B (auditor): regularised linear model over the full 13,500-term space.\n",
    "# Goal: Train LR per GO aspect (BP/MF/CC) with RAM-safe target handling.\n",
    "#\n",
    "# Critical fix: Avoid NumPy fancy-indexing on memmaps.\n",
    "# - DO NOT do: Y_aspect = Y_full[:, aspect_indices]  (this materialises a huge dense copy in host RAM)\n",
    "# - Instead: slice Y_full per fold+target-chunk via np.ix_(idx_tr, cols)\n",
    "#\n",
    "# Protocol (recommended for stability): run ONE aspect per fresh runtime.\n",
    "# If `TARGET_ASPECT` is not set, we default to BP.\n",
    "# ==============================================================================\n",
    "\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping LogReg (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import os\n",
    "    import sys\n",
    "    import time\n",
    "    import gc\n",
    "    import warnings\n",
    "    import psutil\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import torch\n",
    "    from pathlib import Path\n",
    "    from tqdm.auto import tqdm\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "    warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "    def _stage(msg: str) -> None:\n",
    "        print(msg)\n",
    "        try:\n",
    "            sys.stdout.flush()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # -----------------------------\n",
    "    # WORK_ROOT recovery (safety)\n",
    "    # -----------------------------\n",
    "    if 'WORK_ROOT' not in locals() and 'WORK_ROOT' not in globals():\n",
    "        candidates = [\n",
    "            Path('/content/cafa6_data'),\n",
    "            Path('/content/work'),\n",
    "            Path('/kaggle/working/work'),\n",
    "            Path.cwd() / 'cafa6_data',\n",
    "            Path.cwd() / 'artefacts_local' / 'work',\n",
    "        ]\n",
    "        WORK_ROOT = None\n",
    "        for c in candidates:\n",
    "            if (c / 'parsed' / 'train_terms.parquet').exists():\n",
    "                WORK_ROOT = c\n",
    "                break\n",
    "        if WORK_ROOT is None:\n",
    "            for c in candidates:\n",
    "                if c.exists():\n",
    "                    WORK_ROOT = c\n",
    "                    break\n",
    "        if WORK_ROOT is None:\n",
    "            WORK_ROOT = Path.cwd() / 'cafa6_data'\n",
    "        _stage(f\"[AUDITOR] Recovered WORK_ROOT: {WORK_ROOT}\")\n",
    "\n",
    "    FEAT_DIR = Path(WORK_ROOT) / 'features'\n",
    "    PARSED_DIR = Path(WORK_ROOT) / 'parsed'\n",
    "\n",
    "    PRED_DIR = FEAT_DIR / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load top_terms + term_to_ns\n",
    "    # -----------------------------\n",
    "    top_terms_path = FEAT_DIR / 'top_terms_13500.json'\n",
    "    if not top_terms_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {top_terms_path}. Run the Phase 2 setup cell first.\")\n",
    "    top_terms = json.loads(top_terms_path.read_text(encoding='utf-8'))\n",
    "    top_terms = [str(t) for t in top_terms]\n",
    "\n",
    "    # Build / reuse term_to_ns mapping\n",
    "    if 'term_to_ns' in globals():\n",
    "        term_to_ns = globals()['term_to_ns']\n",
    "    else:\n",
    "        try:\n",
    "            import obonet\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('obonet is required for aspect split (term namespaces).') from e\n",
    "\n",
    "        possible_paths = []\n",
    "        if 'PATH_GO_OBO' in globals():\n",
    "            try:\n",
    "                possible_paths.append(Path(globals()['PATH_GO_OBO']))\n",
    "            except Exception:\n",
    "                pass\n",
    "        possible_paths += [\n",
    "            Path(WORK_ROOT) / 'Train' / 'go-basic.obo',\n",
    "            Path(WORK_ROOT) / 'go-basic.obo',\n",
    "            Path('/content/cafa6_data/Train/go-basic.obo'),\n",
    "            Path('Train/go-basic.obo'),\n",
    "            Path('go-basic.obo'),\n",
    "        ]\n",
    "        obo_path = None\n",
    "        for p in possible_paths:\n",
    "            if p is not None and Path(p).exists():\n",
    "                obo_path = Path(p)\n",
    "                break\n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError(f\"go-basic.obo not found. Candidates: {[str(p) for p in possible_paths]}\")\n",
    "\n",
    "        _stage(f\"[AUDITOR] Loading GO OBO for namespaces: {obo_path}\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        term_to_ns = {node: data.get('namespace', 'unknown') for node, data in graph.nodes(data=True)}\n",
    "\n",
    "    aspect_map = {\n",
    "        'biological_process': 'BP',\n",
    "        'molecular_function': 'MF',\n",
    "        'cellular_component': 'CC',\n",
    "    }\n",
    "\n",
    "    def _aspect_of_term(term: str) -> str:\n",
    "        return aspect_map.get(term_to_ns.get(term), 'UNK')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load X / X_test (memmap)\n",
    "    # -----------------------------\n",
    "    x_path = FEAT_DIR / 'X_train_mmap.npy'\n",
    "    xt_path = FEAT_DIR / 'X_test_mmap.npy'\n",
    "    if not x_path.exists() or not xt_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing X memmaps ({x_path} / {xt_path}). Run the Phase 2 setup cell first.\")\n",
    "\n",
    "    X = np.load(x_path, mmap_mode='r')\n",
    "    X_test = np.load(xt_path, mmap_mode='r')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load / build Y (full 13,500) as memmap\n",
    "    # -----------------------------\n",
    "    y_full_path = FEAT_DIR / 'Y_target_13500.npy'\n",
    "    if y_full_path.exists():\n",
    "        Y_full = np.load(y_full_path, mmap_mode='r')\n",
    "    else:\n",
    "        _stage('[AUDITOR] Building Y_target_13500.npy (disk-backed) ...')\n",
    "        train_terms = pd.read_parquet(PARSED_DIR / 'train_terms.parquet')\n",
    "        train_ids_raw = pd.read_feather(PARSED_DIR / 'train_seq.feather')['id'].astype(str)\n",
    "        train_ids = train_ids_raw.str.extract(r\"\\|(.*?)\\|\")[0].fillna(train_ids_raw)\n",
    "\n",
    "        train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "        Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "        Y_df = Y_df.reindex(train_ids, fill_value=0)\n",
    "        Y_df = Y_df.reindex(columns=top_terms, fill_value=0)\n",
    "        np.save(y_full_path, Y_df.values.astype(np.float32))\n",
    "        del train_terms, train_ids_raw, train_ids, train_terms_top, Y_df\n",
    "        gc.collect()\n",
    "        Y_full = np.load(y_full_path, mmap_mode='r')\n",
    "\n",
    "    # -----------------------------\n",
    "    # IA weights (optional)\n",
    "    # -----------------------------\n",
    "    weights_full = None\n",
    "    try:\n",
    "        ia_candidates = [Path(WORK_ROOT) / 'IA.tsv', FEAT_DIR / 'IA.tsv', Path('IA.tsv')]\n",
    "        ia_path = next((p for p in ia_candidates if p.exists()), None)\n",
    "        if ia_path is not None:\n",
    "            ia = pd.read_csv(ia_path, sep='\\t')\n",
    "            cols = [c.lower() for c in ia.columns]\n",
    "            term_col = ia.columns[cols.index('term')] if 'term' in cols else ia.columns[0]\n",
    "            if 'ia' in cols:\n",
    "                ia_col = ia.columns[cols.index('ia')]\n",
    "            elif 'information_accretion' in cols:\n",
    "                ia_col = ia.columns[cols.index('information_accretion')]\n",
    "            else:\n",
    "                ia_col = ia.columns[1] if len(ia.columns) > 1 else ia.columns[0]\n",
    "            ia_map = dict(zip(ia[term_col].astype(str), ia[ia_col].astype(np.float32)))\n",
    "            weights_full = np.asarray([ia_map.get(t, np.float32(1.0)) for t in top_terms], dtype=np.float32)\n",
    "            _stage(f\"[AUDITOR] Loaded IA weights from {ia_path}\")\n",
    "    except Exception as e:\n",
    "        _stage(f\"[AUDITOR] IA weights unavailable (continuing): {e}\")\n",
    "        weights_full = None\n",
    "\n",
    "    def _log_mem(msg: str = ''):\n",
    "        try:\n",
    "            proc = psutil.Process(os.getpid())\n",
    "            ram = proc.memory_info().rss / (1024**3)\n",
    "            avail = psutil.virtual_memory().available / (1024**3)\n",
    "            if torch.cuda.is_available():\n",
    "                alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "                res = torch.cuda.memory_reserved() / (1024**3)\n",
    "            else:\n",
    "                alloc = res = 0.0\n",
    "            print(f\"[MEM] {msg:22s} | RSS: {ram:6.2f}GB | RAM_avail: {avail:6.2f}GB | torch_alloc: {alloc:5.2f}GB torch_res: {res:5.2f}GB\")\n",
    "            sys.stdout.flush()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Placeholder; replaced after RAPIDS discovery.\n",
    "    def _log_gpu_mem(msg: str = ''):\n",
    "        return\n",
    "\n",
    "    # -----------------------------\n",
    "    # RAPIDS discovery\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        import cuml  # noqa: F401\n",
    "        from cuml.linear_model import LogisticRegression as cuLogReg\n",
    "        from cuml.multiclass import OneVsRestClassifier as cuOVR\n",
    "        import cupy as cp\n",
    "        import rmm\n",
    "\n",
    "        try:\n",
    "            rmm.reinitialize(managed_memory=True)\n",
    "            _stage('[AUDITOR] RAPIDS (cuML) detected. RMM Managed Memory ENABLED.')\n",
    "        except Exception as e:\n",
    "            _stage(f'[AUDITOR] RAPIDS detected but RMM init failed ({e}); proceeding with default memory.')\n",
    "\n",
    "        HAS_RAPIDS = True\n",
    "        _stage(\n",
    "            f\"[AUDITOR] versions: cupy={getattr(cp, '__version__', '?')} cuml={getattr(cuml, '__version__', '?')} rmm={getattr(rmm, '__version__', '?')}\"\n",
    "        )\n",
    "\n",
    "        def _maybe_bytes(obj, method_name: str):\n",
    "            fn = getattr(obj, method_name, None)\n",
    "            if callable(fn):\n",
    "                try:\n",
    "                    return int(fn())\n",
    "                except Exception:\n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "        def _pinned_used_bytes(pinned_pool):\n",
    "            used = _maybe_bytes(pinned_pool, 'used_bytes')\n",
    "            if used is not None:\n",
    "                return used\n",
    "            # Older CuPy versions sometimes expose total/free but not used.\n",
    "            total = _maybe_bytes(pinned_pool, 'total_bytes')\n",
    "            free = _maybe_bytes(pinned_pool, 'free_bytes')\n",
    "            if total is not None and free is not None:\n",
    "                return int(total - free)\n",
    "            return None\n",
    "\n",
    "        def _log_gpu_mem(msg: str = ''):\n",
    "            try:\n",
    "                free_b, total_b = cp.cuda.runtime.memGetInfo()\n",
    "                pool = cp.get_default_memory_pool()\n",
    "                pinned = cp.get_default_pinned_memory_pool()\n",
    "\n",
    "                pool_b = _maybe_bytes(pool, 'used_bytes')\n",
    "                pinned_b = _pinned_used_bytes(pinned)\n",
    "\n",
    "                pool_txt = f\"{pool_b/1e9:6.2f}GB\" if pool_b is not None else \"   n/a\"\n",
    "                pinned_txt = f\"{pinned_b/1e9:6.2f}GB\" if pinned_b is not None else \"   n/a\"\n",
    "\n",
    "                print(\n",
    "                    f\"[GPU] {msg:22s} | free {free_b/1e9:6.2f}GB / {total_b/1e9:6.2f}GB | \"\n",
    "                    f\"cupy_pool {pool_txt} | pinned {pinned_txt}\"\n",
    "                )\n",
    "                try:\n",
    "                    res = rmm.mr.get_current_device_resource()\n",
    "                    print(f\"[GPU] rmm resource: {res}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "                sys.stdout.flush()\n",
    "            except Exception as e:\n",
    "                print(f\"[GPU] {msg:22s} | unavailable ({e})\")\n",
    "                sys.stdout.flush()\n",
    "\n",
    "    except Exception:\n",
    "        HAS_RAPIDS = False\n",
    "        cp = None  # type: ignore\n",
    "        _stage('[AUDITOR] RAPIDS NOT detected. Falling back to CPU (slow).')\n",
    "\n",
    "    def _gpu_mem_okay(n_rows: int, n_cols: int, safety: float = 1.35) -> bool:\n",
    "        if not (HAS_RAPIDS and torch.cuda.is_available()):\n",
    "            return False\n",
    "        try:\n",
    "            free_b, _total_b = cp.cuda.runtime.memGetInfo()\n",
    "            need_b = int(n_rows) * int(n_cols) * 4\n",
    "            ok = free_b > int(safety * need_b)\n",
    "            if not ok:\n",
    "                _stage(f\"[AUDITOR] VRAM insufficient: free={free_b/1e9:.1f}GB need{need_b/1e9:.1f}GB\")\n",
    "            return bool(ok)\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "    # -----------------------------\n",
    "    # Aspect selection (default BP)\n",
    "    # -----------------------------\n",
    "    target_aspect = (os.environ.get('TARGET_ASPECT') or '').strip().upper()\n",
    "    if not target_aspect:\n",
    "        target_aspect = (globals().get('TARGET_ASPECT') or '').strip().upper()\n",
    "    if not target_aspect:\n",
    "        target_aspect = 'BP'\n",
    "        _stage('[AUDITOR] TARGET_ASPECT not set -> defaulting to BP (recommended: run one aspect per fresh runtime).')\n",
    "\n",
    "    if target_aspect not in {'BP', 'MF', 'CC'}:\n",
    "        raise RuntimeError(\n",
    "            f\"Invalid TARGET_ASPECT={target_aspect!r}. Must be one of: BP, MF, CC.\"\n",
    "        )\n",
    "\n",
    "    aspects = [target_aspect]\n",
    "    _stage(f\"[AUDITOR] Training only aspect: {target_aspect}\")\n",
    "\n",
    "    # Granularity knobs (auditor-aligned)\n",
    "    TARGET_CHUNK = 100\n",
    "    VAL_BS = 256\n",
    "    TEST_BS = 256\n",
    "\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    tmp_dir = FEAT_DIR / 'tmp_folds_logreg'\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    oof_pred_logreg_by_aspect = {}\n",
    "    test_pred_logreg_by_aspect = {}\n",
    "    aspect_indices_map = {}\n",
    "\n",
    "    t0_all = time.time()\n",
    "    _log_mem('start')\n",
    "    _log_gpu_mem('start')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Performance: materialise X_test into RAM (best-effort)\n",
    "    # -----------------------------\n",
    "    # X_test is used repeatedly across folds and target-chunks; keeping it disk-backed can become the bottleneck.\n",
    "    try:\n",
    "        _stage('[AUDITOR] LogReg: materialising X_test into RAM for faster inference...')\n",
    "        X_test_ram = np.ascontiguousarray(np.asarray(X_test, dtype=np.float32))\n",
    "        _stage(f'[AUDITOR] LogReg: X_test_ram shape={X_test_ram.shape} dtype={X_test_ram.dtype}')\n",
    "    except MemoryError:\n",
    "        X_test_ram = X_test\n",
    "        _stage('[AUDITOR] WARNING: could not materialise X_test (MemoryError); using memmap (slower).')\n",
    "\n",
    "    # -----------------------------\n",
    "    # Fold streaming helpers\n",
    "    # -----------------------------\n",
    "    def _iter_indexed_rows(src_mm, idx: np.ndarray, bs: int, desc: str):\n",
    "        for i in tqdm(range(0, int(len(idx)), int(bs)), desc=desc, unit='batch', leave=False):\n",
    "            j = min(i + int(bs), int(len(idx)))\n",
    "            rows = idx[i:j]\n",
    "            xb = np.asarray(src_mm[rows], dtype=np.float32)\n",
    "            yield i, j, xb\n",
    "\n",
    "    def _fit_scaler_from_indexed_rows(src_mm, idx: np.ndarray, bs: int, desc: str):\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        for _i, _j, xb in _iter_indexed_rows(src_mm, idx, bs=bs, desc=desc):\n",
    "            scaler.partial_fit(xb)\n",
    "        return {'mean': scaler.mean_.astype(np.float32), 'scale': scaler.scale_.astype(np.float32)}\n",
    "\n",
    "    def _alloc_and_fill_gpu_from_indexed_rows(src_mm, idx: np.ndarray, mean: np.ndarray, scale: np.ndarray, bs: int, desc: str):\n",
    "        n_rows = int(len(idx))\n",
    "        n_cols = int(src_mm.shape[1])\n",
    "        out = cp.empty((n_rows, n_cols), dtype=cp.float32)\n",
    "        mean_gpu = cp.asarray(mean)\n",
    "        scale_gpu = cp.asarray(scale)\n",
    "        for i, j, xb in _iter_indexed_rows(src_mm, idx, bs=bs, desc=desc):\n",
    "            xg = cp.asarray(xb)\n",
    "            xg = (xg - mean_gpu) / (scale_gpu + 1e-12)\n",
    "            out[i:j, :] = xg\n",
    "            del xb, xg\n",
    "            try:\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "            except Exception:\n",
    "                pass\n",
    "        return out, mean_gpu, scale_gpu\n",
    "\n",
    "    def _predict_proba_gpu_batched(clf, src_mm, idx: np.ndarray, mean_gpu, scale_gpu, bs: int, out_np: np.ndarray, out_rows: np.ndarray, col_slice: slice, desc: str):\n",
    "        for i, j, xb in _iter_indexed_rows(src_mm, idx, bs=bs, desc=desc):\n",
    "            xg = cp.asarray(xb)\n",
    "            xg = (xg - mean_gpu) / (scale_gpu + 1e-12)\n",
    "            p = clf.predict_proba(xg)\n",
    "            if hasattr(p, 'get'):\n",
    "                p = p.get()\n",
    "            elif hasattr(p, 'to_numpy'):\n",
    "                p = p.to_numpy()\n",
    "            out_np[out_rows[i:j], col_slice] = np.asarray(p, dtype=np.float32)\n",
    "            del xb, xg, p\n",
    "            try:\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "                cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    for aspect in aspects:\n",
    "        _stage(f\"\\n=== LogReg Aspect: {aspect} ===\")\n",
    "\n",
    "        aspect_indices = [i for i, t in enumerate(top_terms) if _aspect_of_term(t) == aspect]\n",
    "        if not aspect_indices:\n",
    "            raise RuntimeError(f\"No terms found for aspect {aspect} within top_terms_13500.json\")\n",
    "\n",
    "        aspect_indices = np.asarray(aspect_indices, dtype=np.int64)\n",
    "        aspect_indices_map[aspect] = aspect_indices\n",
    "        n_targets = int(aspect_indices.shape[0])\n",
    "        aspect_terms = [top_terms[i] for i in aspect_indices.tolist()]\n",
    "\n",
    "        # Slice IA weights for diagnostics only\n",
    "        weights_aspect = weights_full[aspect_indices] if weights_full is not None else None\n",
    "\n",
    "        # Persist per-aspect term contract\n",
    "        (PRED_DIR / f'top_terms_{aspect}.json').write_text(json.dumps(aspect_terms), encoding='utf-8')\n",
    "\n",
    "        # Per-aspect artefacts\n",
    "        lr_oof_path = PRED_DIR / f'oof_pred_logreg_{aspect}.npy'\n",
    "        lr_test_path = PRED_DIR / f'test_pred_logreg_{aspect}.npy'\n",
    "        marker_dir = PRED_DIR / f'fold_markers_logreg_{aspect}'\n",
    "        marker_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Done check\n",
    "        if lr_oof_path.exists() and lr_test_path.exists() and (len(list(marker_dir.glob('fold_*_done.flag'))) == n_splits):\n",
    "            _stage(f\"[AUDITOR] {aspect}: all folds completed; loading from disk\")\n",
    "            oof_pred_logreg_by_aspect[aspect] = np.load(lr_oof_path, mmap_mode='r')\n",
    "            test_pred_logreg_by_aspect[aspect] = np.load(lr_test_path, mmap_mode='r')\n",
    "            continue\n",
    "\n",
    "        # Prepare memmaps for this aspect\n",
    "        mode_oof = 'r+' if lr_oof_path.exists() else 'w+'\n",
    "        mode_test = 'r+' if lr_test_path.exists() else 'w+'\n",
    "\n",
    "        oof_pred = np.lib.format.open_memmap(str(lr_oof_path), mode=mode_oof, dtype=np.float32, shape=(X.shape[0], n_targets))\n",
    "        test_pred = np.lib.format.open_memmap(str(lr_test_path), mode=mode_test, dtype=np.float32, shape=(X_test.shape[0], n_targets))\n",
    "        if mode_oof == 'w+':\n",
    "            oof_pred[:] = 0.0\n",
    "            oof_pred.flush()\n",
    "        if mode_test == 'w+':\n",
    "            test_pred[:] = 0.0\n",
    "            test_pred.flush()\n",
    "\n",
    "        fold_iter = tqdm(kf.split(np.arange(X.shape[0])), total=kf.get_n_splits(), desc=f'LogReg {aspect} folds', unit='fold')\n",
    "\n",
    "        for fold, (idx_tr, idx_val) in enumerate(fold_iter):\n",
    "            marker_path = marker_dir / f'fold_{fold}_done.flag'\n",
    "            if marker_path.exists():\n",
    "                _stage(f\"{aspect} Fold {fold+1}/{n_splits} already completed (marker found). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            t0_fold = time.time()\n",
    "            _stage(f\"{aspect} Fold {fold+1}/{n_splits}\")\n",
    "            _log_mem(f\"{aspect} fold {fold+1} start\")\n",
    "            _log_gpu_mem(f\"{aspect} fold {fold+1} start\")\n",
    "\n",
    "            idx_tr = np.asarray(idx_tr, dtype=np.int64)\n",
    "            idx_val = np.asarray(idx_val, dtype=np.int64)\n",
    "\n",
    "            # Performance: materialise fold validation rows into RAM once.\n",
    "            # This avoids re-reading X (disk-backed) for every target-chunk during predict_proba.\n",
    "            try:\n",
    "                X_val_ram = np.ascontiguousarray(np.asarray(X[idx_val], dtype=np.float32))\n",
    "                val_src = X_val_ram\n",
    "                val_rows = np.arange(int(len(idx_val)), dtype=np.int64)\n",
    "                _stage(f'[AUDITOR] {aspect} Fold {fold+1}: X_val_ram in RAM: {X_val_ram.shape}')\n",
    "            except MemoryError:\n",
    "                val_src = X\n",
    "                val_rows = idx_val\n",
    "                _stage(f'[AUDITOR] {aspect} Fold {fold+1}: WARNING: could not materialise X_val (MemoryError); using memmap (slower).')\n",
    "\n",
    "            # Fit scaler by streaming fold rows from the global memmap (no fold copies).\n",
    "            scaler_path = PRED_DIR / f'logreg_scaler_{aspect}_fold{fold}.pkl'\n",
    "            if scaler_path.exists():\n",
    "                _stage(f\"[AUDIT] {aspect} Fold {fold+1}: Found existing scaler. Loading.\")\n",
    "                scaler_state = joblib.load(scaler_path)\n",
    "            else:\n",
    "                t0 = time.time()\n",
    "                scaler_state = _fit_scaler_from_indexed_rows(X, idx_tr, bs=2048, desc=f'{aspect} Fold {fold+1} Fit Scaler')\n",
    "                joblib.dump(scaler_state, scaler_path)\n",
    "                _stage(f\"[AUDIT] {aspect} Fold {fold+1}: Scaler fit in {time.time() - t0:.1f}s\")\n",
    "\n",
    "            mean = scaler_state['mean']\n",
    "            scale = scaler_state['scale']\n",
    "            _log_mem(f\"{aspect} fold {fold+1} post scaler\")\n",
    "            _log_gpu_mem(f\"{aspect} fold {fold+1} post scaler\")\n",
    "\n",
    "            # GPU selection: only if we can afford the full training fold on GPU.\n",
    "            use_gpu = bool(HAS_RAPIDS and _gpu_mem_okay(int(len(idx_tr)), int(X.shape[1])))\n",
    "            gpu_success = False\n",
    "\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    _stage(f\"[AUDITOR] {aspect} Fold {fold+1}: Using RAPIDS/cuML\")\n",
    "\n",
    "                    _stage(f\"[AUDIT] {aspect} Fold {fold+1}: PRE X_tr_gpu build\")\n",
    "                    _log_mem(f\"{aspect} pre X_tr_gpu\")\n",
    "                    _log_gpu_mem(f\"{aspect} pre X_tr_gpu\")\n",
    "\n",
    "                    # Build X_tr on GPU by streaming scaled batches (avoid full host materialisation).\n",
    "                    X_tr_gpu, mean_gpu, scale_gpu = _alloc_and_fill_gpu_from_indexed_rows(\n",
    "                        X,\n",
    "                        idx_tr,\n",
    "                        mean=mean,\n",
    "                        scale=scale,\n",
    "                        bs=1024,\n",
    "                        desc=f'{aspect} Fold {fold+1} ->GPU X_tr',\n",
    "                    )\n",
    "\n",
    "                    _stage(f\"[AUDIT] {aspect} Fold {fold+1}: POST X_tr_gpu build\")\n",
    "                    _log_mem(f\"{aspect} post X_tr_gpu\")\n",
    "                    _log_gpu_mem(f\"{aspect} post X_tr_gpu\")\n",
    "\n",
    "                    for start in tqdm(\n",
    "                        range(0, n_targets, TARGET_CHUNK),\n",
    "                        total=(n_targets + TARGET_CHUNK - 1) // TARGET_CHUNK,\n",
    "                        desc=f'{aspect} Fold {fold+1} target chunks',\n",
    "                        unit='chunk',\n",
    "                        leave=False,\n",
    "                    ):\n",
    "                        end = min(start + TARGET_CHUNK, n_targets)\n",
    "\n",
    "                        if start == 0:\n",
    "                            _stage(f\"[AUDIT] {aspect} Fold {fold+1} chunk0 PRE Y_tr_chunk\")\n",
    "                            _log_mem(f\"{aspect} chunk0 pre Y\")\n",
    "                            _log_gpu_mem(f\"{aspect} chunk0 pre Y\")\n",
    "\n",
    "                        cols = aspect_indices[start:end]\n",
    "                        y_host = np.asarray(Y_full[np.ix_(idx_tr, cols)], dtype=np.float32)\n",
    "                        Y_tr_chunk = cp.asarray(y_host)\n",
    "\n",
    "                        if start == 0:\n",
    "                            _stage(f\"[AUDIT] {aspect} Fold {fold+1} chunk0 PRE fit\")\n",
    "                            _log_mem(f\"{aspect} chunk0 pre fit\")\n",
    "                            _log_gpu_mem(f\"{aspect} chunk0 pre fit\")\n",
    "\n",
    "                        clf_chunk = cuOVR(cuLogReg(solver='qn', penalty='l2', C=1.0, max_iter=2000, tol=1e-3))\n",
    "                        clf_chunk.fit(X_tr_gpu, Y_tr_chunk)\n",
    "\n",
    "                        if start == 0:\n",
    "                            _stage(f\"[AUDIT] {aspect} Fold {fold+1} chunk0 POST fit\")\n",
    "                            _log_mem(f\"{aspect} chunk0 post fit\")\n",
    "                            _log_gpu_mem(f\"{aspect} chunk0 post fit\")\n",
    "\n",
    "                        # Validation probs (batched) -> write directly into oof memmap\n",
    "                        _predict_proba_gpu_batched(\n",
    "                            clf_chunk,\n",
    "                            val_src,\n",
    "                            val_rows,\n",
    "                            mean_gpu,\n",
    "                            scale_gpu,\n",
    "                            bs=VAL_BS,\n",
    "                            out_np=oof_pred,\n",
    "                            out_rows=idx_val,\n",
    "                            col_slice=slice(start, end),\n",
    "                            desc=f'{aspect} Fold {fold+1} val proba',\n",
    "                        )\n",
    "\n",
    "                        # Test probs (batched) -> accumulate\n",
    "                        for b0 in range(0, int(X_test_ram.shape[0]), TEST_BS):\n",
    "                            b1 = min(b0 + TEST_BS, int(X_test_ram.shape[0]))\n",
    "                            xb = np.asarray(X_test_ram[b0:b1], dtype=np.float32)\n",
    "                            xb_gpu = cp.asarray(xb)\n",
    "                            xb_gpu = (xb_gpu - mean_gpu) / (scale_gpu + 1e-12)\n",
    "\n",
    "                            p_te = clf_chunk.predict_proba(xb_gpu)\n",
    "                            if hasattr(p_te, 'get'):\n",
    "                                p_te = p_te.get()\n",
    "                            elif hasattr(p_te, 'to_numpy'):\n",
    "                                p_te = p_te.to_numpy()\n",
    "\n",
    "                            test_pred[b0:b1, start:end] += (np.asarray(p_te, dtype=np.float32) / float(n_splits))\n",
    "                            del xb, xb_gpu, p_te\n",
    "\n",
    "                        del y_host, Y_tr_chunk, clf_chunk\n",
    "                        try:\n",
    "                            cp.get_default_memory_pool().free_all_blocks()\n",
    "                            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        gc.collect()\n",
    "\n",
    "                    del X_tr_gpu, mean_gpu, scale_gpu\n",
    "                    try:\n",
    "                        cp.get_default_memory_pool().free_all_blocks()\n",
    "                        cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    gc.collect()\n",
    "                    gpu_success = True\n",
    "\n",
    "                except Exception as e:\n",
    "                    _stage(f\"[CRITICAL] {aspect} Fold {fold+1}: GPU Training Failed (likely OOM): {e}\")\n",
    "                    _stage('[AUDITOR] Cleaning up GPU memory and falling back to CPU...')\n",
    "                    gpu_success = False\n",
    "                    if HAS_RAPIDS and cp is not None:\n",
    "                        try:\n",
    "                            cp.get_default_memory_pool().free_all_blocks()\n",
    "                            cp.get_default_pinned_memory_pool().free_all_blocks()\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    gc.collect()\n",
    "\n",
    "            if (not use_gpu) or (not gpu_success):\n",
    "                _stage(f\"[AUDITOR] {aspect} Fold {fold+1}: Using CPU SGD (fallback)\")\n",
    "\n",
    "                # CPU fallback: build one scaled X_tr memmap, then fit/predict target chunks.\n",
    "                X_trs_path = tmp_dir / f'X_tr_scaled_{aspect}_fold{fold}.npy'\n",
    "                if not X_trs_path.exists():\n",
    "                    mm = np.lib.format.open_memmap(str(X_trs_path), mode='w+', dtype=np.float32, shape=(len(idx_tr), X.shape[1]))\n",
    "                    for i, j, xb in _iter_indexed_rows(X, idx_tr, bs=2048, desc=f'{aspect} Fold {fold+1} scale X_tr (CPU)'):\n",
    "                        xb = (xb - mean) / (scale + 1e-12)\n",
    "                        mm[i:j, :] = xb\n",
    "                    mm.flush()\n",
    "                    del mm\n",
    "                    gc.collect()\n",
    "\n",
    "                X_trs = np.load(X_trs_path, mmap_mode='r')\n",
    "\n",
    "                for start in tqdm(\n",
    "                    range(0, n_targets, TARGET_CHUNK),\n",
    "                    total=(n_targets + TARGET_CHUNK - 1) // TARGET_CHUNK,\n",
    "                    desc=f'{aspect} Fold {fold+1} CPU target chunks',\n",
    "                    unit='chunk',\n",
    "                    leave=False,\n",
    "                ):\n",
    "                    end = min(start + TARGET_CHUNK, n_targets)\n",
    "                    cols = aspect_indices[start:end]\n",
    "                    Y_tr_chunk = np.asarray(Y_full[np.ix_(idx_tr, cols)], dtype=np.float32)\n",
    "\n",
    "                    clf_logreg = OneVsRestClassifier(\n",
    "                        SGDClassifier(loss='log_loss', penalty='l2', alpha=0.0001, max_iter=1, tol=None, n_jobs=4),\n",
    "                        n_jobs=-1,\n",
    "                    )\n",
    "                    clf_logreg.fit(X_trs, Y_tr_chunk)\n",
    "\n",
    "                    # Validation preds (batched, scale on the fly)\n",
    "                    for i, j, xb in _iter_indexed_rows(val_src, val_rows, bs=VAL_BS, desc=f'{aspect} Fold {fold+1} val predict (CPU)'):\n",
    "                        xb = (xb - mean) / (scale + 1e-12)\n",
    "                        pb = clf_logreg.predict_proba(xb).astype(np.float32)\n",
    "                        oof_pred[idx_val[i:j], start:end] = pb\n",
    "\n",
    "                    # Test preds (batched)\n",
    "                    for b0 in tqdm(range(0, int(X_test_ram.shape[0]), TEST_BS), desc=f'{aspect} Fold {fold+1} test predict (CPU)', unit='batch', leave=False):\n",
    "                        b1 = min(b0 + TEST_BS, int(X_test_ram.shape[0]))\n",
    "                        xb = np.asarray(X_test_ram[b0:b1], dtype=np.float32)\n",
    "                        xb = (xb - mean) / (scale + 1e-12)\n",
    "                        pb = clf_logreg.predict_proba(xb).astype(np.float32)\n",
    "                        test_pred[b0:b1, start:end] += pb / float(n_splits)\n",
    "\n",
    "                    del Y_tr_chunk, clf_logreg\n",
    "                    gc.collect()\n",
    "\n",
    "                del X_trs\n",
    "                gc.collect()\n",
    "\n",
    "                # Cleanup temp scaled fold\n",
    "                try:\n",
    "                    os.remove(X_trs_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            # Fold diagnostics (sampled, capped columns to avoid huge allocations)\n",
    "            try:\n",
    "                sample_n = int(min(20000, len(idx_val)))\n",
    "                sample_k = int(min(2000, n_targets))\n",
    "                if sample_n > 0 and sample_k > 0:\n",
    "                    sample_probs = np.asarray(oof_pred[idx_val[:sample_n], :sample_k], dtype=np.float32)\n",
    "                    cols = aspect_indices[:sample_k]\n",
    "                    sample_true = np.asarray(Y_full[np.ix_(idx_val[:sample_n], cols)], dtype=np.float32)\n",
    "\n",
    "                    best_f1 = 0.0\n",
    "                    best_thr = 0.0\n",
    "                    for thr in np.linspace(0.01, 0.20, 20):\n",
    "                        vp = (sample_probs > thr).astype(np.int8)\n",
    "                        score = f1_score(sample_true, vp, average='micro')\n",
    "                        if score > best_f1:\n",
    "                            best_f1, best_thr = score, float(thr)\n",
    "\n",
    "                    _stage(f\"  >> {aspect} Fold {fold+1} (sample) micro-F1={best_f1:.4f} best_thr={best_thr:.2f} (k={sample_k})\")\n",
    "            except Exception as e:\n",
    "                _stage('  [WARNING] Diagnostics skipped: ' + repr(e))\n",
    "\n",
    "            oof_pred.flush()\n",
    "            test_pred.flush()\n",
    "            marker_path.touch()\n",
    "            _stage(f\"{aspect} Fold {fold+1} completed and flushed.\")\n",
    "            _stage(f\"[TIMER] {aspect} Fold {fold+1} wall: {time.time() - t0_fold:.1f}s\")\n",
    "            _log_mem(f\"{aspect} fold {fold+1} end\")\n",
    "            _log_gpu_mem(f\"{aspect} fold {fold+1} end\")\n",
    "\n",
    "        oof_pred.flush()\n",
    "        test_pred.flush()\n",
    "        del oof_pred, test_pred\n",
    "        gc.collect()\n",
    "\n",
    "        oof_pred_logreg_by_aspect[aspect] = np.load(lr_oof_path, mmap_mode='r')\n",
    "        test_pred_logreg_by_aspect[aspect] = np.load(lr_test_path, mmap_mode='r')\n",
    "        _stage(f\"[AUDITOR] {aspect}: saved {lr_oof_path.name}, {lr_test_path.name}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Backwards compatibility: combined outputs are only assembled once BP+MF+CC exist.\n",
    "    # -----------------------------\n",
    "    if set(['BP', 'MF', 'CC']).issubset(set(oof_pred_logreg_by_aspect.keys())):\n",
    "        lr_oof_full_path = PRED_DIR / 'oof_pred_logreg.npy'\n",
    "        lr_test_full_path = PRED_DIR / 'test_pred_logreg.npy'\n",
    "\n",
    "        mode_oof = 'r+' if lr_oof_full_path.exists() else 'w+'\n",
    "        mode_test = 'r+' if lr_test_full_path.exists() else 'w+'\n",
    "        oof_full = np.lib.format.open_memmap(str(lr_oof_full_path), mode=mode_oof, dtype=np.float32, shape=(X.shape[0], len(top_terms)))\n",
    "        te_full = np.lib.format.open_memmap(str(lr_test_full_path), mode=mode_test, dtype=np.float32, shape=(X_test.shape[0], len(top_terms)))\n",
    "        if mode_oof == 'w+':\n",
    "            oof_full[:] = 0.0\n",
    "        if mode_test == 'w+':\n",
    "            te_full[:] = 0.0\n",
    "\n",
    "        for asp in ['BP', 'MF', 'CC']:\n",
    "            idx = aspect_indices_map[asp]\n",
    "            oof_full[:, idx] = np.asarray(oof_pred_logreg_by_aspect[asp], dtype=np.float32)\n",
    "            te_full[:, idx] = np.asarray(test_pred_logreg_by_aspect[asp], dtype=np.float32)\n",
    "\n",
    "        oof_full.flush()\n",
    "        te_full.flush()\n",
    "        del oof_full, te_full\n",
    "        gc.collect()\n",
    "\n",
    "        oof_pred_logreg = np.load(lr_oof_full_path, mmap_mode='r')\n",
    "        test_pred_logreg = np.load(lr_test_full_path, mmap_mode='r')\n",
    "        _stage(f\"[AUDITOR] Combined preds saved: {lr_oof_full_path.name}, {lr_test_full_path.name}\")\n",
    "    else:\n",
    "        oof_pred_logreg = None\n",
    "        test_pred_logreg = None\n",
    "\n",
    "    # Final checkpoint push (per-aspect; combined only if present)\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        required = [str(top_terms_path.as_posix())]\n",
    "        for asp in oof_pred_logreg_by_aspect.keys():\n",
    "            required += [\n",
    "                str((PRED_DIR / f'oof_pred_logreg_{asp}.npy').as_posix()),\n",
    "                str((PRED_DIR / f'test_pred_logreg_{asp}.npy').as_posix()),\n",
    "                str((PRED_DIR / f'top_terms_{asp}.json').as_posix()),\n",
    "            ]\n",
    "        if oof_pred_logreg is not None and test_pred_logreg is not None:\n",
    "            required += [\n",
    "                str((PRED_DIR / 'oof_pred_logreg.npy').as_posix()),\n",
    "                str((PRED_DIR / 'test_pred_logreg.npy').as_posix()),\n",
    "            ]\n",
    "\n",
    "        try:\n",
    "            STORE.maybe_push(\n",
    "                stage='stage_07a_level1_logreg_aspect_split',\n",
    "                required_paths=required,\n",
    "                note='Level-1 Logistic Regression predictions (OOF + test), split by GO aspect (BP/MF/CC).',\n",
    "            )\n",
    "        except Exception as e:\n",
    "            _stage(f\"[WARN] STORE push failed: {e}\")\n",
    "\n",
    "    _stage(f\"[TIMER] LogReg total wall: {time.time() - t0_all:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933b226",
   "metadata": {
    "id": "3933b226",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 13D - DNN (PyTorch, multi-branch per modality) + checkpoint push\n",
    "# ===================================================================\n",
    "# Rank-1 style correction: each modality gets its own head before fusion.\n",
    "# Extreme ensembling correction: 5 seeds  5 folds = 25 models.\n",
    "# Implementation guardrails:\n",
    "#   - IA-weighted BCE is mandatory (class_weight per term).\n",
    "#   - Outputs must remain full 13,500 columns for the Phase 3 GCN contract.\n",
    "\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping DNN (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import gc\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    WORK_ROOT = Path(WORK_ROOT)\n",
    "    FEAT_DIR = WORK_ROOT / 'features'\n",
    "    PRED_DIR = FEAT_DIR / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dnn_oof_path = PRED_DIR / 'oof_pred_dnn.npy'\n",
    "    dnn_test_path = PRED_DIR / 'test_pred_dnn.npy'\n",
    "\n",
    "    if dnn_oof_path.exists() and dnn_test_path.exists():\n",
    "        oof_pred_dnn = np.load(dnn_oof_path)\n",
    "        test_pred_dnn = np.load(dnn_test_path)\n",
    "        print('Loaded existing DNN preds:', dnn_oof_path.name, dnn_test_path.name)\n",
    "    else:\n",
    "        if 'features_train' not in globals() or 'features_test' not in globals():\n",
    "            raise RuntimeError('Missing `features_train`/`features_test`. Run Cell 13a first (Phase 2 setup).')\n",
    "        if 'Y' not in globals():\n",
    "            raise RuntimeError('Missing Y. Run Cell 13a first (targets).')\n",
    "\n",
    "        # ---- IA class weights (per-term) ----\n",
    "        top_terms_path = FEAT_DIR / 'top_terms_13500.json'\n",
    "        if not top_terms_path.exists():\n",
    "            raise FileNotFoundError(f'Missing {top_terms_path}. Run Cell 13a first.')\n",
    "        top_terms = [str(t) for t in json.loads(top_terms_path.read_text(encoding='utf-8'))]\n",
    "\n",
    "        ia_path = WORK_ROOT / 'IA.tsv'\n",
    "        if not ia_path.exists():\n",
    "            raise FileNotFoundError(f'Missing IA.tsv at {ia_path}')\n",
    "        ia_df = pd.read_csv(ia_path, sep='\\t')\n",
    "        # robust column naming (some IA files use 'term' or '#term')\n",
    "        term_col = 'term' if 'term' in ia_df.columns else ('#term' if '#term' in ia_df.columns else ia_df.columns[0])\n",
    "        ia_col = 'ia' if 'ia' in ia_df.columns else (ia_df.columns[1] if len(ia_df.columns) > 1 else ia_df.columns[0])\n",
    "        ia_map = dict(zip(ia_df[term_col].astype(str).values, ia_df[ia_col].astype(np.float32).values))\n",
    "        weights = np.asarray([ia_map.get(t, np.float32(1.0)) for t in top_terms], dtype=np.float32)\n",
    "        # weights broadcast over classes: (1, L)\n",
    "        w_t = torch.from_numpy(weights).view(1, -1)\n",
    "        print(f'[DNN] IA weights ready: shape={weights.shape} min={float(weights.min()):.4f} max={float(weights.max()):.4f}')\n",
    "\n",
    "        # ---- label contract (must stay 13,500-wide) ----\n",
    "        out_dim = int(Y.shape[1])\n",
    "        if out_dim != len(top_terms):\n",
    "            raise RuntimeError(f'DNN label contract mismatch: Y has {out_dim} cols but top_terms has {len(top_terms)}')\n",
    "        if out_dim != 13500:\n",
    "            raise RuntimeError(f'DNN expects 13,500 labels; got out_dim={out_dim}')\n",
    "\n",
    "        # ---- modality inputs ----\n",
    "        # Use per-modality arrays directly (no flat X copy).\n",
    "        required_keys = ['t5', 'esm2_650m', 'esm2_3b', 'ankh', 'text', 'taxa']\n",
    "        missing = [k for k in required_keys if k not in features_train]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(f'Missing mandatory DNN modalities: {missing}. Run embeddings/text/taxa stages to materialise them.')\n",
    "\n",
    "        # Optional 7th branch: PB/GBDT probabilities (teacher features).\n",
    "        # If Cell 13b already produced OOF/test predictions, treat them as an additional modality input.\n",
    "        # This is leakage-safe because we use OOF for train rows and a proper test_pred for test rows.\n",
    "        gbdt_oof_path = PRED_DIR / 'oof_pred_gbdt.npy'\n",
    "        gbdt_test_path = PRED_DIR / 'test_pred_gbdt.npy'\n",
    "        use_pb = bool(gbdt_oof_path.exists() and gbdt_test_path.exists())\n",
    "\n",
    "        # Build local feature dicts for the DNN (may include pb)\n",
    "        dnn_train = dict(features_train)\n",
    "        dnn_test = dict(features_test)\n",
    "        dnn_keys = list(required_keys)\n",
    "\n",
    "        if use_pb:\n",
    "            pb_oof = np.load(gbdt_oof_path, mmap_mode='r')\n",
    "            pb_test = np.load(gbdt_test_path, mmap_mode='r')\n",
    "            if int(pb_oof.shape[0]) != int(Y.shape[0]):\n",
    "                raise RuntimeError(f'PB/GBDT OOF rows mismatch: {pb_oof.shape[0]} vs Y rows {Y.shape[0]}')\n",
    "            if int(pb_oof.shape[1]) != out_dim or int(pb_test.shape[1]) != out_dim:\n",
    "                raise RuntimeError(f'PB/GBDT pred cols mismatch: expected {out_dim}, got oof={pb_oof.shape}, test={pb_test.shape}')\n",
    "            dnn_train['pb'] = pb_oof\n",
    "            dnn_test['pb'] = pb_test\n",
    "            dnn_keys.append('pb')\n",
    "            print('[DNN] Using 7th modality branch: pb (= GBDT OOF/test probabilities)')\n",
    "        else:\n",
    "            print('[DNN] PB/GBDT branch not available (missing GBDT OOF/test preds); proceeding with 6 modalities.')\n",
    "\n",
    "        print(f'DNN modality heads: {dnn_keys} (n={len(dnn_keys)})')\n",
    "        dims = {k: int(dnn_train[k].shape[1]) for k in dnn_keys}\n",
    "\n",
    "        class MultiModalDataset(Dataset):\n",
    "            def __init__(self, X_dict, y, keys, idx):\n",
    "                self.X_dict = X_dict\n",
    "                self.y = y\n",
    "                self.keys = keys\n",
    "                self.idx = np.asarray(idx, dtype=np.int64)\n",
    "            def __len__(self):\n",
    "                return int(self.idx.shape[0])\n",
    "            def __getitem__(self, i):\n",
    "                j = int(self.idx[i])\n",
    "                xs = [np.asarray(self.X_dict[k][j], dtype=np.float32) for k in self.keys]\n",
    "                if self.y is None:\n",
    "                    return xs\n",
    "                yy = np.asarray(self.y[j], dtype=np.float32)\n",
    "                return xs, yy\n",
    "\n",
    "        def _collate(batch):\n",
    "            # batch: list of (xs, y) or xs\n",
    "            if isinstance(batch[0], tuple):\n",
    "                xs_list, ys = zip(*batch)\n",
    "                xs_by_key = list(zip(*xs_list))\n",
    "                xs_t = [torch.from_numpy(np.stack(v, axis=0)) for v in xs_by_key]\n",
    "                y_t = torch.from_numpy(np.stack(ys, axis=0))\n",
    "                return xs_t, y_t\n",
    "            else:\n",
    "                xs_by_key = list(zip(*batch))\n",
    "                xs_t = [torch.from_numpy(np.stack(v, axis=0)) for v in xs_by_key]\n",
    "                return xs_t\n",
    "\n",
    "        class ModalityHead(nn.Module):\n",
    "            def __init__(self, in_dim: int, p: float = 0.2):\n",
    "                super().__init__()\n",
    "                # Keep heads uniform but avoid huge parameter blow-ups for very wide modalities\n",
    "                if int(in_dim) >= 8000:\n",
    "                    hidden1, hidden2 = 1024, 1024\n",
    "                elif int(in_dim) >= 2000:\n",
    "                    hidden1, hidden2 = 2048, 1024\n",
    "                else:\n",
    "                    hidden1, hidden2 = 1024, 512\n",
    "                self.out_dim = int(hidden2)\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(int(in_dim), int(hidden1)),\n",
    "                    nn.BatchNorm1d(int(hidden1)),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p),\n",
    "                    nn.Linear(int(hidden1), int(hidden2)),\n",
    "                    nn.BatchNorm1d(int(hidden2)),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p),\n",
    "                )\n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "\n",
    "        class MultiBranchDNN(nn.Module):\n",
    "            def __init__(self, dims_by_key: dict, out_dim: int, p: float = 0.2):\n",
    "                super().__init__()\n",
    "                self.keys = list(dims_by_key.keys())\n",
    "                self.heads = nn.ModuleDict({k: ModalityHead(in_dim=int(dims_by_key[k]), p=p) for k in self.keys})\n",
    "                fused_dim = int(sum(self.heads[k].out_dim for k in self.keys))\n",
    "                self.trunk = nn.Sequential(\n",
    "                    nn.Linear(fused_dim, 2048),\n",
    "                    nn.BatchNorm1d(2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p),\n",
    "                    nn.Linear(2048, out_dim),\n",
    "                )\n",
    "            def forward(self, xs):\n",
    "                hs = []\n",
    "                for k, x in zip(self.keys, xs):\n",
    "                    hs.append(self.heads[k](x))\n",
    "                h = torch.cat(hs, dim=1)\n",
    "                return self.trunk(h)\n",
    "\n",
    "        def train_one_seed_fold(train_idx, val_idx, seed: int, epochs: int, batch_size: int, lr: float, device: torch.device):\n",
    "            torch.manual_seed(42 + seed)\n",
    "            np.random.seed(42 + seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "            model = MultiBranchDNN(dims_by_key=dims, out_dim=out_dim, p=0.2).to(device)\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            w = w_t.to(device)\n",
    "\n",
    "            ds_tr = MultiModalDataset(dnn_train, Y, dnn_keys, train_idx)\n",
    "            ds_va = MultiModalDataset(dnn_train, Y, dnn_keys, val_idx)\n",
    "            dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=0, collate_fn=_collate)\n",
    "            dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=_collate)\n",
    "\n",
    "            for ep in range(1, epochs + 1):\n",
    "                model.train()\n",
    "                for xs, yb in dl_tr:\n",
    "                    xs = [x.to(device, non_blocking=True).float() for x in xs]\n",
    "                    yb = yb.to(device, non_blocking=True).float()\n",
    "                    logits = model(xs)\n",
    "                    # IA-weighted BCE: class weights applied per term (broadcast over batch)\n",
    "                    loss_per = F.binary_cross_entropy_with_logits(logits, yb, reduction='none')\n",
    "                    loss = (loss_per * w).mean()\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "\n",
    "                # quick val metric (kept simple to avoid extra compute)\n",
    "                if 'ia_weighted_f1' in globals():\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        va_scores = []\n",
    "                        va_true = []\n",
    "                        for xs, yb in dl_va:\n",
    "                            xs = [x.to(device, non_blocking=True).float() for x in xs]\n",
    "                            logits = model(xs)\n",
    "                            va_scores.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                            va_true.append(yb.numpy())\n",
    "                        va_scores = np.vstack(va_scores)\n",
    "                        va_true = np.vstack(va_true)\n",
    "                        s = ia_weighted_f1(va_true, va_scores, thr=0.3)\n",
    "                    print(f'  seed={seed} ep={ep}/{epochs} IA-F1={s}')\n",
    "\n",
    "            return model\n",
    "\n",
    "        def predict_on_split(model: nn.Module, X_dict, idx, batch_size: int, device: torch.device):\n",
    "            ds = MultiModalDataset(X_dict, None, dnn_keys, idx)\n",
    "            dl = DataLoader(ds, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=0, collate_fn=_collate)\n",
    "            preds = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for xs in dl:\n",
    "                    xs = [x.to(device, non_blocking=True).float() for x in xs]\n",
    "                    logits = model(xs)\n",
    "                    preds.append(torch.sigmoid(logits).cpu().numpy())\n",
    "            return np.vstack(preds).astype(np.float32)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print('DNN device:', device)\n",
    "\n",
    "        n_splits = 5\n",
    "        n_seeds = 5\n",
    "        epochs = 3\n",
    "        batch_size = 256\n",
    "        lr = 1e-3\n",
    "\n",
    "        oof_pred_dnn = np.zeros((int(Y.shape[0]), out_dim), dtype=np.float32)\n",
    "        test_n = int(dnn_test[dnn_keys[0]].shape[0])\n",
    "        test_pred_dnn = np.zeros((test_n, out_dim), dtype=np.float32)\n",
    "        counts = np.zeros((int(Y.shape[0]), 1), dtype=np.float32)\n",
    "\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(np.arange(int(Y.shape[0]))), start=1):\n",
    "            print(f'Fold {fold}/{n_splits} (DNN multi-branch)')\n",
    "            fold_test = np.zeros_like(test_pred_dnn)\n",
    "            for seed in range(n_seeds):\n",
    "                print(f'  seed {seed+1}/{n_seeds}')\n",
    "                model = train_one_seed_fold(tr_idx, va_idx, seed=seed, epochs=epochs, batch_size=batch_size, lr=lr, device=device)\n",
    "\n",
    "                # OOF preds\n",
    "                preds_va = predict_on_split(model, dnn_train, va_idx, batch_size=1024, device=device)\n",
    "                oof_pred_dnn[va_idx] += preds_va\n",
    "                counts[va_idx] += 1.0\n",
    "\n",
    "                # TEST preds\n",
    "                te_idx = np.arange(test_n, dtype=np.int64)\n",
    "                preds_te = predict_on_split(model, dnn_test, te_idx, batch_size=1024, device=device)\n",
    "                fold_test += preds_te\n",
    "\n",
    "                # free VRAM aggressively (important on Colab/Kaggle)\n",
    "                del model\n",
    "                gc.collect()\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            test_pred_dnn += (fold_test / float(n_seeds))\n",
    "\n",
    "        oof_pred_dnn = (oof_pred_dnn / np.maximum(counts, 1.0)).astype(np.float32)\n",
    "        test_pred_dnn = (test_pred_dnn / float(n_splits)).astype(np.float32)\n",
    "\n",
    "        # Contract guardrail: MUST remain (n_train, 13500) and (n_test, 13500)\n",
    "        if int(oof_pred_dnn.shape[1]) != 13500 or int(test_pred_dnn.shape[1]) != 13500:\n",
    "            raise RuntimeError(f'DNN output contract violated: oof={oof_pred_dnn.shape} test={test_pred_dnn.shape}')\n",
    "\n",
    "        np.save(dnn_oof_path, oof_pred_dnn)\n",
    "        np.save(dnn_test_path, test_pred_dnn)\n",
    "        print('Saved:', dnn_oof_path)\n",
    "        print('Saved:', dnn_test_path)\n",
    "\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07c_level1_dnn',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str(dnn_oof_path.as_posix()),\n",
    "            str(dnn_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 DNN predictions (OOF + test).',\n",
    "    )\n",
    "\n",
    "    # Diagnostics: probability histograms + IA-F1 vs threshold (sampled)\n",
    "    try:\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "        def _sub(y_true: np.ndarray, y_score: np.ndarray):\n",
    "            n = int(y_true.shape[0])\n",
    "            m = min(n, int(DIAG_N))\n",
    "            if m <= 0:\n",
    "                return y_true[:0], y_score[:0]\n",
    "            idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "            return y_true[idx], y_score[idx]\n",
    "        y_t, y_s = _sub(Y, oof_pred_dnn)\n",
    "        row_max_oof = y_s.max(axis=1)\n",
    "        row_mean_oof = y_s.mean(axis=1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_max_oof, bins=60, alpha=0.7)\n",
    "        plt.title('DNN OOF: max probability per protein (sampled)')\n",
    "        plt.xlabel('max prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_mean_oof, bins=60, alpha=0.7)\n",
    "        plt.title('DNN OOF: mean probability per protein (sampled)')\n",
    "        plt.xlabel('mean prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        if test_pred_dnn is not None:\n",
    "            te_s = test_pred_dnn\n",
    "            te_m = min(int(te_s.shape[0]), int(DIAG_N))\n",
    "            te_idx = np.linspace(0, int(te_s.shape[0]) - 1, num=te_m, dtype=np.int64) if te_m > 0 else np.array([], dtype=np.int64)\n",
    "            row_max_te = te_s[te_idx].max(axis=1) if te_m > 0 else np.array([], dtype=np.float32)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(row_max_oof, bins=60, alpha=0.5, label='OOF')\n",
    "            plt.hist(row_max_te, bins=60, alpha=0.5, label='test')\n",
    "            plt.title('DNN: max probability per protein (OOF vs test; sampled)')\n",
    "            plt.xlabel('max prob')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(y_t, y_s, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('DNN OOF: IA-weighted F1 vs threshold (sampled)')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('DNN diagnostics skipped:', repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3efa7",
   "metadata": {
    "id": "7fc3efa7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 13E - KNN (cosine; ESM2-3B) + checkpoint push\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping KNN (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from sklearn.model_selection import KFold\n",
    "    import json\n",
    "    PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    knn_oof_path = PRED_DIR / 'oof_pred_knn.npy'\n",
    "    knn_test_path = PRED_DIR / 'test_pred_knn.npy'\n",
    "    # Backwards-compatible copies (some downstream code loads from WORK_ROOT/features)\n",
    "    knn_oof_compat = WORK_ROOT / 'features' / 'oof_pred_knn.npy'\n",
    "    knn_test_compat = WORK_ROOT / 'features' / 'test_pred_knn.npy'\n",
    "    if knn_oof_path.exists() and knn_test_path.exists():\n",
    "        oof_pred_knn = np.load(knn_oof_path)\n",
    "        test_pred_knn = np.load(knn_test_path)\n",
    "        oof_max_sim = None\n",
    "    else:\n",
    "        if 'features_train' not in globals() or 'features_test' not in globals():\n",
    "            raise RuntimeError('Missing `features_train`/`features_test`. Run the Phase 2 feature load cell first.')\n",
    "        if 'esm2_3b' not in features_train:\n",
    "            raise FileNotFoundError(\"Missing required modality 'esm2_3b' in features_train. Ensure features/train_embeds_esm2_3b.npy exists.\")\n",
    "        X_knn = features_train['esm2_3b'].astype(np.float32)\n",
    "        X_knn_test = features_test['esm2_3b'].astype(np.float32)\n",
    "        # Enforce TOP_K alignment using the persisted term list.\n",
    "        top_terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "        if top_terms_path.exists():\n",
    "            top_terms_knn = json.loads(top_terms_path.read_text())\n",
    "            if Y.shape[1] != len(top_terms_knn):\n",
    "                raise ValueError(\n",
    "                    f'KNN shape mismatch: Y has {Y.shape[1]} cols but top_terms_13500.json has {len(top_terms_knn)} terms.'\n",
    "                )\n",
    "        else:\n",
    "            # Fall back to in-memory top_terms if present (should be created in Phase 2 cell).\n",
    "            if 'top_terms' not in globals():\n",
    "                raise RuntimeError('Missing top_terms_13500.json and in-memory top_terms. Run the Phase 2 cell first.')\n",
    "            top_terms_knn = list(top_terms)\n",
    "            if Y.shape[1] != len(top_terms_knn):\n",
    "                raise ValueError(\n",
    "                    f'KNN shape mismatch: Y has {Y.shape[1]} cols but top_terms has {len(top_terms_knn)} terms.'\n",
    "                )\n",
    "        # KNN needs binary targets (presence/absence), not counts.\n",
    "        Y_knn = (Y > 0).astype(np.float32)\n",
    "        def _l2_norm(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "            n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "            return x / np.maximum(n, eps)\n",
    "        # Cosine distance is best-behaved on L2-normalised vectors\n",
    "        X_knn = _l2_norm(X_knn)\n",
    "        X_knn_test = _l2_norm(X_knn_test)\n",
    "        KNN_K = int(globals().get('KNN_K', 50))\n",
    "        KNN_BATCH = int(globals().get('KNN_BATCH', 256))\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        oof_pred_knn = np.zeros((X_knn.shape[0], Y_knn.shape[1]), dtype=np.float32)\n",
    "        test_pred_knn = np.zeros((X_knn_test.shape[0], Y_knn.shape[1]), dtype=np.float32)\n",
    "        oof_max_sim = np.zeros((X_knn.shape[0],), dtype=np.float32)\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_knn), start=1):\n",
    "            print(f'Fold {fold}/{n_splits} (KNN)')\n",
    "            knn = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=-1)\n",
    "            knn.fit(X_knn[tr_idx])\n",
    "            dists, neigh = knn.kneighbors(X_knn[va_idx], return_distance=True)\n",
    "            sims = np.clip((1.0 - dists).astype(np.float32), 0.0, 1.0)\n",
    "            oof_max_sim[va_idx] = sims.max(axis=1)\n",
    "            neigh_global = tr_idx[neigh]  # map to global row indices into Y_knn\n",
    "            for i in range(0, len(va_idx), KNN_BATCH):\n",
    "                j = min(i + KNN_BATCH, len(va_idx))\n",
    "                neigh_b = neigh_global[i:j]\n",
    "                sims_b = sims[i:j]\n",
    "                denom = np.maximum(sims_b.sum(axis=1, keepdims=True), 1e-8)\n",
    "                Y_nei = Y_knn[neigh_b]  # (B, K, L)\n",
    "                scores = (np.einsum('bk,bkl->bl', sims_b, Y_nei) / denom).astype(np.float32)\n",
    "                oof_pred_knn[va_idx[i:j]] = scores\n",
    "            if 'ia_weighted_f1' in globals():\n",
    "                print('  IA-F1:', ia_weighted_f1(Y_knn[va_idx], oof_pred_knn[va_idx], thr=0.3))\n",
    "        # Final model on full train -> test\n",
    "        knn_final = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=-1)\n",
    "        knn_final.fit(X_knn)\n",
    "        dists_te, neigh_te = knn_final.kneighbors(X_knn_test, return_distance=True)\n",
    "        sims_te = np.clip((1.0 - dists_te).astype(np.float32), 0.0, 1.0)\n",
    "        denom_te = np.maximum(sims_te.sum(axis=1, keepdims=True), 1e-8)\n",
    "        for i in range(0, X_knn_test.shape[0], KNN_BATCH):\n",
    "            j = min(i + KNN_BATCH, X_knn_test.shape[0])\n",
    "            neigh_b = neigh_te[i:j]\n",
    "            sims_b = sims_te[i:j]\n",
    "            Y_nei = Y_knn[neigh_b]\n",
    "            scores = (np.einsum('bk,bkl->bl', sims_b, Y_nei) / denom_te[i:j]).astype(np.float32)\n",
    "            test_pred_knn[i:j] = scores\n",
    "        np.save(knn_oof_path, oof_pred_knn)\n",
    "        np.save(knn_test_path, test_pred_knn)\n",
    "        np.save(knn_oof_compat, oof_pred_knn)\n",
    "        np.save(knn_test_compat, test_pred_knn)\n",
    "        print('Saved:', knn_oof_path)\n",
    "        print('Saved:', knn_test_path)\n",
    "        print('Saved:', knn_oof_compat)\n",
    "        print('Saved:', knn_test_compat)\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07d_level1_knn',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str(knn_oof_path.as_posix()),\n",
    "            str(knn_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 KNN (cosine) predictions using ESM2-3B embeddings (OOF + test).',\n",
    "    )\n",
    "    # Diagnostics: similarity distribution + IA-F1 vs threshold\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        if oof_max_sim is not None:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(oof_max_sim, bins=50)\n",
    "            plt.title('KNN OOF diagnostic: max cosine similarity to neighbours (per protein)')\n",
    "            plt.xlabel('max similarity')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(Y, oof_pred_knn, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('KNN OOF: IA-weighted F1 vs threshold')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('KNN diagnostics skipped:', repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15acc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13F - Calibrate per-aspect thresholds (BP/MF/CC) from OOF (read-only)\n",
    "# ========================================================================\n",
    "# Goal: derive thresholds per GO aspect and persist to features/aspect_thresholds.json.\n",
    "# These thresholds are used later in Phase 3/4 (stacking + submission filtering).\n",
    "#\n",
    "# We fit thresholds on a deterministic subsample of rows to keep runtime bounded.\n",
    "# This is *calibration*, not training.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "FEAT_DIR = WORK_ROOT / 'features'\n",
    "PRED_DIR = FEAT_DIR / 'level1_preds'\n",
    "FEAT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "thr_path = FEAT_DIR / 'aspect_thresholds.json'\n",
    "thr_meta_path = FEAT_DIR / 'aspect_thresholds_meta.json'\n",
    "\n",
    "if thr_path.exists():\n",
    "    thr_map = json.loads(thr_path.read_text(encoding='utf-8'))\n",
    "    print('Loaded existing aspect thresholds:', thr_map)\n",
    "else:\n",
    "    # ---- term contract ----\n",
    "    top_terms_path = FEAT_DIR / 'top_terms_13500.json'\n",
    "    if not top_terms_path.exists():\n",
    "        raise FileNotFoundError(f'Missing {top_terms_path}. Run Phase 2 setup first.')\n",
    "    top_terms = [str(t) for t in json.loads(top_terms_path.read_text(encoding='utf-8'))]\n",
    "    if len(top_terms) != 13500:\n",
    "        raise RuntimeError(f'Expected top_terms=13500, got {len(top_terms)}')\n",
    "\n",
    "    # ---- GO namespaces -> aspects ----\n",
    "    if 'go_namespaces' in globals() and isinstance(globals()['go_namespaces'], dict):\n",
    "        go_namespaces = globals()['go_namespaces']\n",
    "    elif 'term_to_ns' in globals() and isinstance(globals()['term_to_ns'], dict):\n",
    "        go_namespaces = globals()['term_to_ns']\n",
    "    else:\n",
    "        import obonet\n",
    "        obo_path = None\n",
    "        for p in [WORK_ROOT / 'Train' / 'go-basic.obo', WORK_ROOT / 'go-basic.obo', Path('Train/go-basic.obo'), Path('go-basic.obo')]:\n",
    "            if p.exists():\n",
    "                obo_path = p\n",
    "                break\n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError('go-basic.obo not found for namespace mapping')\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        go_namespaces = {node: data.get('namespace', 'unknown') for node, data in graph.nodes(data=True)}\n",
    "\n",
    "    ns_to_aspect = {\n",
    "        'biological_process': 'BP',\n",
    "        'molecular_function': 'MF',\n",
    "        'cellular_component': 'CC',\n",
    "    }\n",
    "    aspects = np.array([ns_to_aspect.get(go_namespaces.get(t, 'unknown'), 'UNK') for t in top_terms], dtype='<U3')\n",
    "\n",
    "    # ---- IA weights ----\n",
    "    ia_path = WORK_ROOT / 'IA.tsv'\n",
    "    if not ia_path.exists():\n",
    "        raise FileNotFoundError(f'Missing IA.tsv at {ia_path}')\n",
    "    ia_df = pd.read_csv(ia_path, sep='\\t')\n",
    "    cols = [c.lower() for c in ia_df.columns]\n",
    "    term_col = ia_df.columns[cols.index('term')] if 'term' in cols else ia_df.columns[0]\n",
    "    if 'ia' in cols:\n",
    "        ia_col = ia_df.columns[cols.index('ia')]\n",
    "    elif 'information_accretion' in cols:\n",
    "        ia_col = ia_df.columns[cols.index('information_accretion')]\n",
    "    else:\n",
    "        ia_col = ia_df.columns[1] if len(ia_df.columns) > 1 else ia_df.columns[0]\n",
    "    ia_map = dict(zip(ia_df[term_col].astype(str), ia_df[ia_col].astype(np.float32)))\n",
    "    weights = np.asarray([ia_map.get(t, np.float32(1.0)) for t in top_terms], dtype=np.float32)\n",
    "\n",
    "    # ---- load Y + OOF predictions ----\n",
    "    if 'Y' in globals():\n",
    "        Y_full = np.asarray(globals()['Y'], dtype=np.float32)\n",
    "    else:\n",
    "        train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "        train_ids_raw = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "        train_ids = train_ids_raw.str.extract(r\"\\|(.*?)\\|\", expand=False).fillna(train_ids_raw)\n",
    "        train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "        y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "        y_df = y_df.reindex(train_ids.tolist(), fill_value=0)\n",
    "        y_df = y_df.reindex(columns=top_terms, fill_value=0)\n",
    "        Y_full = y_df.values.astype(np.float32)\n",
    "        del train_terms, train_ids_raw, train_ids, train_terms_top, y_df\n",
    "\n",
    "    def _load_oof(name: str) -> np.ndarray | None:\n",
    "        for p in [PRED_DIR / name, FEAT_DIR / name]:\n",
    "            if p.exists():\n",
    "                return np.load(p)\n",
    "        return None\n",
    "\n",
    "    oof_candidates = {\n",
    "        'logreg': _load_oof('oof_pred_logreg.npy'),\n",
    "        'gbdt': _load_oof('oof_pred_gbdt.npy'),\n",
    "        'dnn': _load_oof('oof_pred_dnn.npy'),\n",
    "        'knn': _load_oof('oof_pred_knn.npy'),\n",
    "    }\n",
    "    oof_candidates = {k: v for k, v in oof_candidates.items() if v is not None}\n",
    "    if not oof_candidates:\n",
    "        raise FileNotFoundError('No OOF preds found. Expected features/level1_preds/oof_pred_*.npy')\n",
    "\n",
    "    for k, v in oof_candidates.items():\n",
    "        if v.shape != Y_full.shape:\n",
    "            raise RuntimeError(f'OOF shape mismatch for {k}: got {v.shape}, expected {Y_full.shape}')\n",
    "\n",
    "    oof_mean = np.mean(np.stack(list(oof_candidates.values()), axis=0), axis=0).astype(np.float32)\n",
    "    print('Threshold calibration base = mean OOF of:', sorted(oof_candidates.keys()))\n",
    "\n",
    "    # deterministic subsample\n",
    "    DIAG_N = int(globals().get('CAFA_DIAG_N', 20000)) if 'CAFA_DIAG_N' in globals() else 20000\n",
    "    n = int(Y_full.shape[0])\n",
    "    m = min(n, int(DIAG_N))\n",
    "    idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "    Y_sub = (Y_full[idx] > 0).astype(np.uint8)\n",
    "    S_sub = oof_mean[idx].astype(np.float32, copy=False)\n",
    "\n",
    "    THRS = np.linspace(0.05, 0.60, 23, dtype=np.float32)\n",
    "    COL_CHUNK = 512\n",
    "\n",
    "    def _ia_f1_for_cols(cols: np.ndarray, thr: float) -> float:\n",
    "        w_tp = 0.0\n",
    "        w_pred = 0.0\n",
    "        w_true = 0.0\n",
    "        for c0 in range(0, int(cols.shape[0]), COL_CHUNK):\n",
    "            c = cols[c0 : c0 + COL_CHUNK]\n",
    "            yt = Y_sub[:, c].astype(bool, copy=False)\n",
    "            yp = (S_sub[:, c] >= float(thr))\n",
    "            tp = (yt & yp).sum(axis=0).astype(np.float64)\n",
    "            pred = yp.sum(axis=0).astype(np.float64)\n",
    "            true = yt.sum(axis=0).astype(np.float64)\n",
    "            w = weights[c].astype(np.float64)\n",
    "            w_tp += float((w * tp).sum())\n",
    "            w_pred += float((w * pred).sum())\n",
    "            w_true += float((w * true).sum())\n",
    "        p = (w_tp / w_pred) if w_pred > 0 else 0.0\n",
    "        r = (w_tp / w_true) if w_true > 0 else 0.0\n",
    "        return (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "\n",
    "    def _best_thr(cols: np.ndarray) -> tuple[float, float]:\n",
    "        best_t = float(THRS[0])\n",
    "        best_s = -1.0\n",
    "        for t in THRS:\n",
    "            s = _ia_f1_for_cols(cols, float(t))\n",
    "            if s > best_s:\n",
    "                best_s = float(s)\n",
    "                best_t = float(t)\n",
    "        return best_t, best_s\n",
    "\n",
    "    cols_all = np.arange(len(top_terms), dtype=np.int64)\n",
    "    cols_bp = np.where(aspects == 'BP')[0].astype(np.int64)\n",
    "    cols_mf = np.where(aspects == 'MF')[0].astype(np.int64)\n",
    "    cols_cc = np.where(aspects == 'CC')[0].astype(np.int64)\n",
    "    cols_unk = np.where(aspects == 'UNK')[0].astype(np.int64)\n",
    "\n",
    "    thr_all, s_all = _best_thr(cols_all)\n",
    "    thr_bp, s_bp = _best_thr(cols_bp)\n",
    "    thr_mf, s_mf = _best_thr(cols_mf)\n",
    "    thr_cc, s_cc = _best_thr(cols_cc)\n",
    "    thr_unk, s_unk = _best_thr(cols_unk) if int(cols_unk.shape[0]) > 0 else (thr_all, s_all)\n",
    "\n",
    "    thr_map = {\n",
    "        'ALL': thr_all,\n",
    "        'BP': thr_bp,\n",
    "        'MF': thr_mf,\n",
    "        'CC': thr_cc,\n",
    "        'UNK': thr_unk,\n",
    "    }\n",
    "    thr_path.write_text(json.dumps(thr_map, indent=2), encoding='utf-8')\n",
    "    thr_meta_path.write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                'calibration_base': 'mean_ensemble_oof',\n",
    "                'models_used': sorted(oof_candidates.keys()),\n",
    "                'n_rows_total': int(Y_full.shape[0]),\n",
    "                'n_rows_used': int(m),\n",
    "                'thr_grid': THRS.tolist(),\n",
    "                'ia_f1_at_best': {'ALL': s_all, 'BP': s_bp, 'MF': s_mf, 'CC': s_cc, 'UNK': s_unk},\n",
    "                'aspect_counts': {'BP': int(cols_bp.shape[0]), 'MF': int(cols_mf.shape[0]), 'CC': int(cols_cc.shape[0]), 'UNK': int(cols_unk.shape[0])},\n",
    "            },\n",
    "            indent=2,\n",
    "        ),\n",
    "        encoding='utf-8',\n",
    "    )\n",
    "    print('Saved aspect thresholds:', thr_map)\n",
    "\n",
    "# Expose to later cells\n",
    "ASPECT_THRESHOLDS = thr_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447b494",
   "metadata": {
    "id": "f447b494",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 14 - Solution: 5. PHASE 3: HIERARCHY-AWARE STACKING (GCN)\n",
    "# 5. PHASE 3: HIERARCHY-AWARE STACKING (GCN)\n",
    "# ==========================================\n",
    "# NOTE: This cell is kept as a lightweight alternative stacker.\n",
    "# The main stacker used by Phase 4 is implemented in the next cell.\n",
    "TRAIN_STACKER = False\n",
    "\n",
    "if TRAIN_STACKER:\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import f1_score\n",
    "    import json\n",
    "\n",
    "    WORK_ROOT = Path(WORK_ROOT)\n",
    "\n",
    "    # Per-aspect thresholds (computed from OOF)\n",
    "    thr_path = WORK_ROOT / 'features' / 'aspect_thresholds.json'\n",
    "    if not thr_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {thr_path}. Run threshold calibration (Cell 13F) first.\")\n",
    "    ASPECT_THRESHOLDS = json.loads(thr_path.read_text(encoding='utf-8'))\n",
    "    print('Aspect thresholds:', ASPECT_THRESHOLDS)\n",
    "\n",
    "    def _load_level1_pred(fname: str) -> np.ndarray:\n",
    "        \"\"\"Load Level-1 prediction arrays from either features/level1_preds or features/.\"\"\"\n",
    "        cand = [\n",
    "            WORK_ROOT / 'features' / 'level1_preds' / fname,\n",
    "            WORK_ROOT / 'features' / fname,\n",
    "        ]\n",
    "        for p in cand:\n",
    "            if p.exists():\n",
    "                return np.load(p).astype(np.float32)\n",
    "        raise FileNotFoundError(f'Missing Level-1 prediction: {fname}. Run Phase 2 first.')\n",
    "\n",
    "    print(\"Loading OOF Predictions for stacking...\")\n",
    "    oof_logreg = _load_level1_pred('oof_pred_logreg.npy')\n",
    "\n",
    "    try:\n",
    "        oof_gbdt = _load_level1_pred('oof_pred_gbdt.npy')\n",
    "    except Exception:\n",
    "        oof_gbdt = np.zeros_like(oof_logreg)\n",
    "\n",
    "    try:\n",
    "        oof_dnn = _load_level1_pred('oof_pred_dnn.npy')\n",
    "    except Exception:\n",
    "        oof_dnn = np.zeros_like(oof_logreg)\n",
    "\n",
    "    try:\n",
    "        oof_knn = _load_level1_pred('oof_pred_knn.npy')\n",
    "    except Exception:\n",
    "        oof_knn = np.zeros_like(oof_logreg)\n",
    "\n",
    "    X_stack = np.hstack([oof_logreg, oof_gbdt, oof_dnn, oof_knn])\n",
    "    Y_stack = Y\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_stack_t = torch.tensor(X_stack, dtype=torch.float32).to(device)\n",
    "    Y_stack_t = torch.tensor(Y_stack, dtype=torch.float32).to(device)\n",
    "\n",
    "    class Stacker(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(2048, output_dim),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    stacker = Stacker(X_stack.shape[1], Y_stack.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(stacker.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Build aspect vector aligned to the 13,500 term contract\n",
    "    terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "    if not terms_path.exists():\n",
    "        raise FileNotFoundError(f'Missing {terms_path}. Run Phase 2 first.')\n",
    "    top_terms = json.loads(terms_path.read_text(encoding='utf-8'))\n",
    "    if 'go_namespaces' not in globals():\n",
    "        raise RuntimeError('Missing go_namespaces mapping; run Phase 2 setup first.')\n",
    "    ns_to_aspect = {'molecular_function': 'MF', 'biological_process': 'BP', 'cellular_component': 'CC'}\n",
    "    term_aspects = np.array([ns_to_aspect.get(go_namespaces.get(t, 'unknown'), 'UNK') for t in top_terms], dtype='<U3')\n",
    "    thr_vec = np.array([float(ASPECT_THRESHOLDS.get(a, ASPECT_THRESHOLDS.get('ALL', 0.3))) for a in term_aspects], dtype=np.float32)\n",
    "\n",
    "    stacker.train()\n",
    "    for epoch in range(20):\n",
    "        optimizer.zero_grad()\n",
    "        out = stacker(X_stack_t)\n",
    "        loss = criterion(out, Y_stack_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                out_np = out.detach().cpu().numpy()\n",
    "                preds = (out_np > thr_vec[None, :]).astype(np.int8)\n",
    "                f1 = f1_score(Y_stack[:1000], preds[:1000], average='micro')\n",
    "            print(f\"Epoch {epoch}: Loss {loss.item():.4f}, Approx micro-F1@per-aspect-thr {f1:.4f}\")\n",
    "\n",
    "    torch.save(stacker.state_dict(), WORK_ROOT / 'features' / 'final_stacker.pth')\n",
    "    print(\"Saved: final_stacker.pth\")\n",
    "\n",
    "    stacker.eval()\n",
    "    with torch.no_grad():\n",
    "        final_preds = stacker(X_stack_t).cpu().numpy()\n",
    "        final_bin = (final_preds > thr_vec[None, :]).astype(int)\n",
    "        final_f1 = f1_score(Y_stack, final_bin, average='micro')\n",
    "    print(f\"Final Stacker micro-F1@per-aspect-thr: {final_f1:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping Phase 3 (lightweight stacker).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132ae0e",
   "metadata": {
    "id": "3132ae0e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 15 - Solution: 5b. PHASE 3: HIERARCHY-AWARE STACKING (GRAPH SMOOTHING GCN)\n",
    "# 5b. PHASE 3: HIERARCHY-AWARE STACKING (GRAPH SMOOTHING GCN)\n",
    "# =========================================================\n",
    "# Critical correction: GCN input features must be the *raw* Level-1 OOF predictions per model,\n",
    "# not an average. We therefore treat per-term base-model probabilities as per-node feature channels.\n",
    "# External priors (IEA) remain injected conservatively at weight=0.25 (compliant).\n",
    "\n",
    "# Option B strictness: if PROCESS_EXTERNAL=True, we REQUIRE the propagated prior files to exist.\n",
    "PROCESS_EXTERNAL = globals().get('PROCESS_EXTERNAL', True)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Load per-aspect thresholds (computed from OOF)\n",
    "thr_path = WORK_ROOT / 'features' / 'aspect_thresholds.json'\n",
    "if not thr_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing {thr_path}. Run the threshold calibration cell after Level-1 models (Cell 13F) first.\"\n",
    "    )\n",
    "ASPECT_THRESHOLDS = json.loads(thr_path.read_text(encoding='utf-8'))\n",
    "print('Aspect thresholds:', ASPECT_THRESHOLDS)\n",
    "\n",
    "# Load the persisted term contract (must match Phase 2)\n",
    "terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "if not terms_path.exists():\n",
    "    raise FileNotFoundError(f'Missing {terms_path}. Run Phase 2 target construction first.')\n",
    "top_terms = [str(t) for t in json.loads(terms_path.read_text(encoding='utf-8'))]\n",
    "TOP_K = int(len(top_terms))\n",
    "print(f'TOP_K (term contract) = {TOP_K}')\n",
    "if TOP_K != 13500:\n",
    "    raise RuntimeError(f'Expected TOP_K=13500, got {TOP_K}')\n",
    "\n",
    "def _load_level1_pred(fname: str):\n",
    "    \"\"\"Load Level-1 prediction arrays from either features/level1_preds or features/.\"\"\"\n",
    "    cand = [\n",
    "        WORK_ROOT / 'features' / 'level1_preds' / fname,\n",
    "        WORK_ROOT / 'features' / fname,\n",
    "    ]\n",
    "    for p in cand:\n",
    "        if p.exists():\n",
    "            return np.load(p, mmap_mode='r')\n",
    "    return None\n",
    "\n",
    "# 1. Load Level-1 predictions (keep as memmaps; batch-slice later for RAM safety)\n",
    "MODEL_FILES = [\n",
    "    ('logreg', 'oof_pred_logreg.npy', 'test_pred_logreg.npy'),\n",
    "    ('gbdt', 'oof_pred_gbdt.npy', 'test_pred_gbdt.npy'),\n",
    "    ('dnn', 'oof_pred_dnn.npy', 'test_pred_dnn.npy'),\n",
    "    ('knn', 'oof_pred_knn.npy', 'test_pred_knn.npy'),\n",
    " ]\n",
    "\n",
    "oof_by_model = {}\n",
    "test_by_model = {}\n",
    "for name, oof_f, te_f in MODEL_FILES:\n",
    "    oof = _load_level1_pred(oof_f)\n",
    "    te = _load_level1_pred(te_f)\n",
    "    if oof is None or te is None:\n",
    "        continue\n",
    "    oof_by_model[name] = oof\n",
    "    test_by_model[name] = te\n",
    "\n",
    "if not oof_by_model:\n",
    "    raise FileNotFoundError('No Level-1 OOF predictions found. Run Phase 2 first.')\n",
    "models_used = sorted(oof_by_model.keys())\n",
    "print('GCN input channels (raw OOF features):', models_used)\n",
    "n_channels = int(len(models_used))\n",
    "\n",
    "# 2. Build Y label matrix for the persisted term contract (prefer existing Y if present)\n",
    "if 'Y' in globals():\n",
    "    Y_full = np.asarray(globals()['Y'], dtype=np.float32)\n",
    "    if Y_full.ndim != 2 or Y_full.shape[1] != TOP_K:\n",
    "        raise RuntimeError(f'Global Y has wrong shape: {Y_full.shape}, expected (_, {TOP_K})')\n",
    "    print('Using in-memory Y from Phase 2 setup:', Y_full.shape)\n",
    "    # also ensure train_ids for priors alignment\n",
    "    train_ids_raw = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "    train_ids = train_ids_raw.str.extract(r'\\|(.*?)\\|', expand=False).fillna(train_ids_raw)\n",
    "else:\n",
    "    train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "    train_ids_raw = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "    train_ids = train_ids_raw.str.extract(r'\\|(.*?)\\|', expand=False).fillna(train_ids_raw)\n",
    "    train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids, fill_value=0)\n",
    "    Y_df = Y_df.reindex(columns=top_terms, fill_value=0)\n",
    "    Y_full = Y_df.values.astype(np.float32)\n",
    "    print('Targets:', Y_full.shape)\n",
    "\n",
    "n_train = int(Y_full.shape[0])\n",
    "for m in models_used:\n",
    "    arr = oof_by_model[m]\n",
    "    if int(arr.shape[0]) != n_train or int(arr.shape[1]) != TOP_K:\n",
    "        raise RuntimeError(f'OOF shape mismatch for {m}: got {arr.shape}, expected ({n_train}, {TOP_K})')\n",
    "\n",
    "# 2b. External priors (Phase 1 Step 4 outputs) -> inject as *conservative* extra signal\n",
    "EXTERNAL_PRIOR_WEIGHT = 0.25\n",
    "ext_dir = WORK_ROOT / 'external'\n",
    "prior_train_path = ext_dir / 'prop_train_no_kaggle.tsv.gz'\n",
    "prior_test_path = ext_dir / 'prop_test_no_kaggle.tsv.gz'\n",
    "prior_train_np = None\n",
    "prior_test_np = None\n",
    "\n",
    "if PROCESS_EXTERNAL:\n",
    "    if not (prior_train_path.exists() and prior_test_path.exists()):\n",
    "        raise FileNotFoundError(\n",
    "            f'Option B requires external priors, but missing: {prior_train_path} or {prior_test_path}. '\n",
    "            'Run Phase 1 Step 4 propagation or ensure your checkpoint dataset contains these files (run setup: STORE.pull()).'\n",
    "        )\n",
    "    prior_train = pd.read_csv(prior_train_path, sep='\\t')\n",
    "    prior_train = prior_train[prior_train['term'].isin(top_terms)]\n",
    "    prior_mat = prior_train.pivot_table(index='EntryID', columns='term', values='score', aggfunc='max', fill_value=0.0)\n",
    "    prior_mat = prior_mat.reindex(train_ids.tolist(), fill_value=0.0)\n",
    "    prior_mat = prior_mat.reindex(columns=top_terms, fill_value=0.0)\n",
    "    prior_train_np = prior_mat.values.astype(np.float32)\n",
    "    print(f'Loaded external IEA train prior: {prior_train_np.shape} (weight={EXTERNAL_PRIOR_WEIGHT})')\n",
    "\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id'].astype(str)\n",
    "    prior_test = pd.read_csv(prior_test_path, sep='\\t')\n",
    "    prior_test = prior_test[prior_test['term'].isin(top_terms)]\n",
    "    prior_t = prior_test.pivot_table(index='EntryID', columns='term', values='score', aggfunc='max', fill_value=0.0)\n",
    "    prior_t = prior_t.reindex(test_ids.tolist(), fill_value=0.0)\n",
    "    prior_t = prior_t.reindex(columns=top_terms, fill_value=0.0)\n",
    "    prior_test_np = prior_t.values.astype(np.float32)\n",
    "    print(f'Loaded external IEA test prior: {prior_test_np.shape} (weight={EXTERNAL_PRIOR_WEIGHT})')\n",
    "\n",
    "# 3. Graph adjacency from go-basic.obo (reload if needed)\n",
    "if 'go_parents' not in locals() or 'go_namespaces' not in locals():\n",
    "    print('Reloading GO graph (parse_obo)...')\n",
    "    def parse_obo(path: Path):\n",
    "        parents = {}\n",
    "        namespaces = {}\n",
    "        cur_id, cur_ns = None, None\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == '[Term]':\n",
    "                    if cur_id and cur_ns:\n",
    "                        namespaces[cur_id] = cur_ns\n",
    "                    cur_id, cur_ns = None, None\n",
    "                elif line.startswith('id: GO:'):\n",
    "                    cur_id = line.split('id: ', 1)[1]\n",
    "                elif line.startswith('namespace:'):\n",
    "                    cur_ns = line.split('namespace: ', 1)[1]\n",
    "                elif line.startswith('is_a:') and cur_id:\n",
    "                    parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                    parents.setdefault(cur_id, set()).add(parent)\n",
    "            if cur_id and cur_ns:\n",
    "                namespaces[cur_id] = cur_ns\n",
    "        return parents, namespaces\n",
    "    go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "\n",
    "def build_adjacency(terms_list, parents_dict):\n",
    "    term_to_idx = {t: i for i, t in enumerate(terms_list)}\n",
    "    n_terms = len(terms_list)\n",
    "    src, dst = [], []\n",
    "    for child in terms_list:\n",
    "        parents = parents_dict.get(child, set())\n",
    "        if not parents:\n",
    "            continue\n",
    "        child_idx = term_to_idx[child]\n",
    "        for parent in parents:\n",
    "            if parent in term_to_idx:\n",
    "                parent_idx = term_to_idx[parent]\n",
    "                # undirected edges (sym)\n",
    "                src.append(child_idx)\n",
    "                dst.append(parent_idx)\n",
    "                src.append(parent_idx)\n",
    "                dst.append(child_idx)\n",
    "    # self loops\n",
    "    src.extend(range(n_terms))\n",
    "    dst.extend(range(n_terms))\n",
    "    indices = torch.tensor([src, dst], dtype=torch.long, device=device)\n",
    "    values = torch.ones(len(src), dtype=torch.float32, device=device)\n",
    "    return torch.sparse_coo_tensor(indices, values, (n_terms, n_terms)).coalesce()\n",
    "\n",
    "# 4. Ontology split (BP/MF/CC)\n",
    "ns_to_aspect = {'molecular_function': 'MF', 'biological_process': 'BP', 'cellular_component': 'CC'}\n",
    "aspects = [ns_to_aspect.get(go_namespaces.get(t, 'unknown'), 'UNK') for t in top_terms]\n",
    "bp_idx = [i for i, a in enumerate(aspects) if a == 'BP']\n",
    "mf_idx = [i for i, a in enumerate(aspects) if a == 'MF']\n",
    "cc_idx = [i for i, a in enumerate(aspects) if a == 'CC']\n",
    "print(f'Aspect split (in contract): BP={len(bp_idx)} MF={len(mf_idx)} CC={len(cc_idx)} UNK={aspects.count(\"UNK\")}')\n",
    "\n",
    "class ChannelGCN(nn.Module):\n",
    "    \"\"\"Graph smoothing over GO terms with per-node multi-channel features.\"\"\"\n",
    "    def __init__(self, n_channels: int, hidden_dim: int, adj_matrix: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.adj = adj_matrix\n",
    "        self.fc1 = nn.Linear(int(n_channels), int(hidden_dim))\n",
    "        self.fc2 = nn.Linear(int(hidden_dim), 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.30)\n",
    "    def forward(self, x):\n",
    "        # x: (B, N, C) -> logits: (B, N)\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h = self.dropout(h)\n",
    "        logits = self.fc2(h).squeeze(-1)\n",
    "        # smooth across GO graph on the node axis (N)\n",
    "        logits = torch.sparse.mm(self.adj, logits.t()).t()\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "def _batch_stack(arrs: list, rows: np.ndarray, cols: list[int], prior_np: np.ndarray | None) -> torch.Tensor:\n",
    "    # returns (B, N, C) on device\n",
    "    # Slice per model to (B, N) and stack to (B, N, C) without materialising full tensors for all terms.\n",
    "    feats = []\n",
    "    for a in arrs:\n",
    "        x = np.asarray(a[rows][:, cols], dtype=np.float32)\n",
    "        feats.append(x)\n",
    "    x = np.stack(feats, axis=2)  # (B, N, C)\n",
    "    if prior_np is not None:\n",
    "        p = np.asarray(prior_np[rows][:, cols], dtype=np.float32)\n",
    "        x = np.maximum(x, (EXTERNAL_PRIOR_WEIGHT * p)[:, :, None])\n",
    "    return torch.from_numpy(x).to(device=device, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "def _batch_y(Y_np: np.ndarray, rows: np.ndarray, cols: list[int]) -> torch.Tensor:\n",
    "    y = np.asarray(Y_np[rows][:, cols], dtype=np.float32)\n",
    "    return torch.from_numpy(y).to(device=device, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "def train_one(aspect_name: str, idx_cols: list[int]):\n",
    "    if not idx_cols:\n",
    "        print(f'[{aspect_name}] No terms; skipping.')\n",
    "        return None\n",
    "\n",
    "    terms_sub = [top_terms[i] for i in idx_cols]\n",
    "    adj = build_adjacency(terms_sub, go_parents)\n",
    "\n",
    "    # Model sees C channels per node (raw OOF preds from each base model)\n",
    "    model = ChannelGCN(n_channels=n_channels, hidden_dim=128, adj_matrix=adj).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    n_samples = int(n_train)\n",
    "    BS = 128\n",
    "    EPOCHS = 5\n",
    "    thr = float(ASPECT_THRESHOLDS.get(aspect_name, ASPECT_THRESHOLDS.get('ALL', 0.3)))\n",
    "\n",
    "    # Build deterministic eval subset indices (kept small)\n",
    "    eval_n = min(2000, n_samples)\n",
    "    eval_rows = np.arange(eval_n, dtype=np.int64)\n",
    "\n",
    "    model.train()\n",
    "    print(f'\\n=== Training GCN[{aspect_name}] nodes={len(idx_cols)} channels={n_channels} thr={thr:.3f} ===')\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        perm = np.random.RandomState(42 + epoch).permutation(n_samples)\n",
    "        for i in range(0, n_samples, BS):\n",
    "            rows = perm[i:i + BS]\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            xb = _batch_stack([oof_by_model[m] for m in models_used], rows, idx_cols, prior_train_np)\n",
    "            yb = _batch_y(Y_full, rows, idx_cols)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "        # quick eval micro-F1 (subset)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            xb = _batch_stack([oof_by_model[m] for m in models_used], eval_rows, idx_cols, prior_train_np)\n",
    "            yb = _batch_y(Y_full, eval_rows, idx_cols)\n",
    "            pred = (model(xb) > thr).float().cpu().numpy()\n",
    "            f1 = f1_score(yb.cpu().numpy(), pred, average='micro')\n",
    "        model.train()\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} Loss={total_loss:.4f} micro-F1@thr({thr:.2f})={f1:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# 5. Write test_pred_gcn.npy (required by Phase 4)\n",
    "out_path = WORK_ROOT / 'features' / 'test_pred_gcn.npy'\n",
    "if out_path.exists():\n",
    "    print(f'GCN stacker output already exists; skipping: {out_path}')\n",
    "else:\n",
    "    # Baseline for non-modelled cols = mean ensemble across available Level-1 test preds (+ prior floor if enabled)\n",
    "    print('\\nPreparing baseline test prediction (mean ensemble)...')\n",
    "    test_n = int(next(iter(test_by_model.values())).shape[0])\n",
    "    for m in models_used:\n",
    "        arr = test_by_model[m]\n",
    "        if int(arr.shape[0]) != test_n or int(arr.shape[1]) != TOP_K:\n",
    "            raise RuntimeError(f'Test pred shape mismatch for {m}: got {arr.shape}, expected ({test_n}, {TOP_K})')\n",
    "    base = np.zeros((test_n, TOP_K), dtype=np.float32)\n",
    "    for m in models_used:\n",
    "        base += np.asarray(test_by_model[m], dtype=np.float32)\n",
    "    base /= float(n_channels)\n",
    "    if prior_test_np is not None:\n",
    "        base = np.maximum(base, EXTERNAL_PRIOR_WEIGHT * prior_test_np)\n",
    "    final_test = base.astype(np.float32, copy=False)\n",
    "\n",
    "    def _predict_aspect(model: nn.Module, idx_cols: list[int], bs: int = 128) -> np.ndarray:\n",
    "        model.eval()\n",
    "        outs = []\n",
    "        for i in range(0, test_n, bs):\n",
    "            rows = np.arange(i, min(i + bs, test_n), dtype=np.int64)\n",
    "            xb = _batch_stack([test_by_model[m] for m in models_used], rows, idx_cols, prior_test_np)\n",
    "            with torch.no_grad():\n",
    "                out = model(xb).detach().float().cpu().numpy()\n",
    "            outs.append(out)\n",
    "        return np.vstack(outs).astype(np.float32)\n",
    "\n",
    "    for aspect_name, idx_cols in [('MF', mf_idx), ('BP', bp_idx), ('CC', cc_idx)]:\n",
    "        if not idx_cols:\n",
    "            continue\n",
    "        model = train_one(aspect_name, idx_cols)\n",
    "        if model is None:\n",
    "            continue\n",
    "        pred_te = _predict_aspect(model, idx_cols, bs=64)\n",
    "        if pred_te.shape != (test_n, len(idx_cols)):\n",
    "            raise RuntimeError(f'GCN[{aspect_name}] shape mismatch: got {pred_te.shape} expected ({test_n}, {len(idx_cols)})')\n",
    "        final_test[:, idx_cols] = pred_te\n",
    "\n",
    "    np.save(out_path, final_test.astype(np.float32, copy=False))\n",
    "    print('Saved:', out_path)\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        try:\n",
    "            STORE.maybe_push('stage_08_stacker_gcn', [out_path, terms_path], note='GCN graph-smoothing stacker (test preds)')\n",
    "        except Exception as e:\n",
    "            print('WARN: failed to push stage_08_stacker_gcn:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784143e",
   "metadata": {
    "id": "0784143e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 16 - Solution: 6. PHASE 4: POST-PROCESSING & SUBMISSION\n",
    "# 6. PHASE 4: POST-PROCESSING & SUBMISSION\n",
    "# ========================================\n",
    "# HARDWARE: CPU / GPU\n",
    "# ========================================\n",
    "# This phase applies the \"Strict Post-Processing\" rules (Max/Min Propagation)\n",
    "# and generates the final submission file.\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if submission already exists\n",
    "if (WORK_ROOT / 'submission.tsv').exists():\n",
    "    print(\"submission.tsv already exists. Skipping Phase 4.\")\n",
    "else:\n",
    "    print(\"Starting Phase 4: Post-processing & submission...\")\n",
    "\n",
    "    WORK_ROOT = Path(WORK_ROOT)\n",
    "\n",
    "    # Ensure go_parents is available (from Phase 1)\n",
    "    if 'go_parents' not in locals() or 'go_namespaces' not in locals():\n",
    "        print(\"Reloading GO graph (parse_obo)...\")\n",
    "        def parse_obo(path: Path):\n",
    "            parents = {}\n",
    "            namespaces = {}\n",
    "            cur_id, cur_ns = None, None\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line == '[Term]':\n",
    "                        if cur_id and cur_ns:\n",
    "                            namespaces[cur_id] = cur_ns\n",
    "                        cur_id, cur_ns = None, None\n",
    "                    elif line.startswith('id: GO:'):\n",
    "                        cur_id = line.split('id: ', 1)[1]\n",
    "                    elif line.startswith('namespace:'):\n",
    "                        cur_ns = line.split('namespace: ', 1)[1]\n",
    "                    elif line.startswith('is_a:') and cur_id:\n",
    "                        parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                        parents.setdefault(cur_id, set()).add(parent)\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "            return parents, namespaces\n",
    "        go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "\n",
    "    # Load per-aspect thresholds (computed from OOF)\n",
    "    thr_path = WORK_ROOT / 'features' / 'aspect_thresholds.json'\n",
    "    if not thr_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Missing {thr_path}. Run the threshold calibration cell after Level-1 models (Cell 13F) first.\"\n",
    "        )\n",
    "    ASPECT_THRESHOLDS = json.loads(thr_path.read_text(encoding='utf-8'))\n",
    "    print('Aspect thresholds:', ASPECT_THRESHOLDS)\n",
    "\n",
    "    # Load test IDs\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id']\n",
    "\n",
    "    # Load stacker predictions\n",
    "    pred_path = WORK_ROOT / 'features' / 'test_pred_gcn.npy'\n",
    "    if not pred_path.exists():\n",
    "        raise FileNotFoundError(\"Missing `test_pred_gcn.npy`. Run Phase 3 (GCN stacker) first.\")\n",
    "    preds = np.load(pred_path).astype(np.float32, copy=False)\n",
    "\n",
    "    # Load term list (must match Phase 2/3)\n",
    "    terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "    if not terms_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {terms_path}. Re-run Phase 2 target construction.\")\n",
    "    with open(terms_path, 'r', encoding='utf-8') as f:\n",
    "        top_terms = [str(t) for t in json.load(f)]\n",
    "    if preds.shape[1] != len(top_terms):\n",
    "        raise ValueError(f\"Shape mismatch: preds has {preds.shape[1]} terms, top_terms has {len(top_terms)}.\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Strict post-processing (Max/Min Propagation)\n",
    "    # ------------------------------------------\n",
    "    print(f\"Applying hierarchy rules on {len(top_terms)} terms...\")\n",
    "    df_pred = pd.DataFrame(preds, columns=top_terms)\n",
    "    term_set = set(top_terms)\n",
    "    term_to_parents = {}\n",
    "    term_to_children = {}\n",
    "    for term in top_terms:\n",
    "        parents = go_parents.get(term, set())\n",
    "        if not parents:\n",
    "            continue\n",
    "        parents = parents.intersection(term_set)\n",
    "        if not parents:\n",
    "            continue\n",
    "        term_to_parents[term] = list(parents)\n",
    "        for p in parents:\n",
    "            term_to_children.setdefault(p, []).append(term)\n",
    "\n",
    "    # Max Propagation (Child -> Parent)\n",
    "    # Previous code used range(2), which is not enough for GO depth. Use >=10 iterations.\n",
    "    N_PROP_ITERS = 12\n",
    "    for _ in range(N_PROP_ITERS):\n",
    "        for child, parents in term_to_parents.items():\n",
    "            child_scores = df_pred[child].values\n",
    "            for parent in parents:\n",
    "                df_pred[parent] = np.maximum(df_pred[parent].values, child_scores)\n",
    "\n",
    "    # Min Propagation (Parent -> Child)\n",
    "    for _ in range(N_PROP_ITERS):\n",
    "        for parent, children in term_to_children.items():\n",
    "            parent_scores = df_pred[parent].values\n",
    "            for child in children:\n",
    "                df_pred[child] = np.minimum(df_pred[child].values, parent_scores)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Per-aspect thresholding BEFORE melt/prune\n",
    "    # ------------------------------------------\n",
    "    ns_to_aspect = {\n",
    "        'molecular_function': 'MF',\n",
    "        'biological_process': 'BP',\n",
    "        'cellular_component': 'CC',\n",
    "    }\n",
    "    aspects = np.array([ns_to_aspect.get(go_namespaces.get(t, 'unknown'), 'UNK') for t in top_terms], dtype='<U3')\n",
    "    thr_vec = np.array([\n",
    "        float(ASPECT_THRESHOLDS.get(a, ASPECT_THRESHOLDS.get('ALL', 0.0))) for a in aspects\n",
    "    ], dtype=np.float32)\n",
    "    pred_np = df_pred.values.astype(np.float32, copy=False)\n",
    "    pred_np = np.where(pred_np >= thr_vec[None, :], pred_np, 0.0).astype(np.float32, copy=False)\n",
    "    df_pred = pd.DataFrame(pred_np, columns=top_terms)\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Submission formatting (CAFA rules)\n",
    "    # - tab-separated, no header\n",
    "    # - score in (0, 1.000]\n",
    "    # - up to 3 significant figures\n",
    "    # - <= 1500 terms per target (MF/BP/CC combined)\n",
    "    # ------------------------------------------\n",
    "    df_pred['EntryID'] = test_ids.values\n",
    "    submission = df_pred.melt(id_vars='EntryID', var_name='term', value_name='score')\n",
    "    # Enforce score range + remove zeros\n",
    "    submission['score'] = submission['score'].clip(lower=0.0, upper=1.0)\n",
    "    submission = submission[submission['score'] > 0.0]\n",
    "    # Keep top 1500 per protein (rule)\n",
    "    submission = submission.sort_values(['EntryID', 'score'], ascending=[True, False])\n",
    "    submission = submission.groupby('EntryID', sort=False).head(1500)\n",
    "    # Write with <= 3 significant figures\n",
    "    submission.to_csv(\n",
    "        WORK_ROOT / 'submission.tsv',\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        header=False,\n",
    "        float_format='%.3g',\n",
    "    )\n",
    "    print(f\"Done! Submission saved to {WORK_ROOT / 'submission.tsv'}\")\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.maybe_push('stage_09_submission', [WORK_ROOT / 'submission.tsv'], note='final submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644cdb6",
   "metadata": {
    "id": "a644cdb6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 17 - Solution: 7. PHASE 5: FREE TEXT PREDICTION (EXTRA)\n",
    "# 7. PHASE 5: FREE TEXT PREDICTION (EXTRA)\n",
    "# ==========================================\n",
    "# HARDWARE: CPU\n",
    "# ==========================================\n",
    "# Official CAFA constraints (summary):\n",
    "# - Combined file (GO + Text) allowed\n",
    "# - Text: up to 5 lines per protein; ASCII printable; no tabs; <=3000 chars per protein total\n",
    "# - Scores should be in (0, 1.000] and up to 3 significant figures\n",
    "if (WORK_ROOT / 'submission_with_text.tsv').exists():\n",
    "    print(\"submission_with_text.tsv already exists. Skipping Phase 5.\")\n",
    "elif not (WORK_ROOT / 'submission.tsv').exists():\n",
    "    print(\"submission.tsv not found. Please run Phase 4 first.\")\n",
    "else:\n",
    "    print(\"Starting Phase 5: Text Generation...\")\n",
    "    # 1. Load Submission & GO Graph\n",
    "    print(\"Loading submission and GO data...\")\n",
    "    submission = pd.read_csv(\n",
    "        WORK_ROOT / 'submission.tsv',\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=['EntryID', 'term', 'score'],\n",
    "    )\n",
    "    if 'graph' not in locals():\n",
    "        import obonet\n",
    "        graph = obonet.read_obo(PATH_GO_OBO)\n",
    "    # 2. Generate Text Descriptions\n",
    "    print(\"Generating descriptions...\")\n",
    "    # Pre-fetch term names to avoid graph lookups in loop\n",
    "    term_names = {node: data.get('name', 'unknown function') for node, data in graph.nodes(data=True)}\n",
    "    text_rows = []\n",
    "    unique_ids = submission['EntryID'].unique()\n",
    "    for protein_id in tqdm(unique_ids, desc=\"Generating Text\"):\n",
    "        prot_preds = submission[submission['EntryID'] == protein_id]\n",
    "        top_go = prot_preds.sort_values('score', ascending=False).head(3)\n",
    "        if top_go.empty:\n",
    "            # If we have no GO lines for this protein, skip text (keeps score>0 rule clean)\n",
    "            continue\n",
    "        term_descs = []\n",
    "        for _, row in top_go.iterrows():\n",
    "            term_id = row['term']\n",
    "            term_descs.append(term_names.get(term_id, term_id))\n",
    "        joined_terms = \", \".join(term_descs)\n",
    "        description = f\"{protein_id} is predicted to be involved in: {joined_terms}.\"\n",
    "        # Ensure no tabs in description\n",
    "        description = description.replace('\\t', ' ')\n",
    "        # Score: strictly > 0 and <= 1\n",
    "        score = float(top_go.iloc[0]['score'])\n",
    "        score = min(max(score, 0.001), 1.0)\n",
    "        # One line per protein (<=5 allowed)\n",
    "        text_rows.append({\n",
    "            'EntryID': protein_id,\n",
    "            'term': 'Text',\n",
    "            'score': score,\n",
    "            'description': description,\n",
    "        })\n",
    "    df_text = pd.DataFrame(text_rows)\n",
    "    print(\"Saving combined submission...\")\n",
    "    with open(WORK_ROOT / 'submission_with_text.tsv', 'w', encoding='utf-8') as f:\n",
    "        # 1) GO preds (3 cols) already CAFA-formatted in Phase 4\n",
    "        submission.to_csv(f, sep='\\t', index=False, header=False, float_format='%.3g')\n",
    "        # 2) Text preds (4 cols)\n",
    "        for _, row in df_text.iterrows():\n",
    "            # Up to 3 significant figures\n",
    "            score_str = format(float(row['score']), '.3g')\n",
    "            f.write(f\"{row['EntryID']}\\tText\\t{score_str}\\t{row['description']}\\n\")\n",
    "    print(f\"Done! Combined submission saved to {WORK_ROOT / 'submission_with_text.tsv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6e0f0",
   "metadata": {
    "id": "21e6e0f0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 18 - Diagnostics: what actually contributes (OOF ablations; read-only)\n",
    "# Goal: identify which Level-1 predictors help/hurt an OOF mean-ensemble under IA-F1.\n",
    "# This cell ONLY reads artefacts and plots; it does not affect training or saved outputs.\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "except Exception as e:\n",
    "    plt = None\n",
    "    sns = None\n",
    "    print('Plotting libs not available; will print tables only:', repr(e))\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "THRS = np.linspace(0.05, 0.60, 23)\n",
    "\n",
    "def _load_pred(stem: str) -> np.ndarray | None:\n",
    "    # Prefer level1_preds folder, fall back to legacy root folder\n",
    "    cands = [\n",
    "        WORK_ROOT / 'features' / 'level1_preds' / f'{stem}.npy',\n",
    "        WORK_ROOT / 'features' / f'{stem}.npy',\n",
    "    ]\n",
    "    for p in cands:\n",
    "        if p.exists():\n",
    "            return np.load(p)\n",
    "    return None\n",
    "\n",
    "def _load_targets_topk() -> tuple[np.ndarray, list[str]]:\n",
    "    train_terms_path = WORK_ROOT / 'parsed' / 'train_terms.parquet'\n",
    "    train_seq_path = WORK_ROOT / 'parsed' / 'train_seq.feather'\n",
    "    top_terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "    if not train_terms_path.exists() or not train_seq_path.exists():\n",
    "        raise FileNotFoundError('Missing parsed targets; expected parsed/train_terms.parquet and parsed/train_seq.feather')\n",
    "    train_terms = pd.read_parquet(train_terms_path)\n",
    "    train_ids_raw = pd.read_feather(train_seq_path)['id'].astype(str)\n",
    "    train_ids = train_ids_raw.str.extract(r'\\|(.*?)\\|', expand=False).fillna(train_ids_raw).tolist()\n",
    "    if top_terms_path.exists():\n",
    "        import json\n",
    "        top_terms = json.loads(top_terms_path.read_text())\n",
    "    else:\n",
    "        # Fallback: recompute top terms from training labels\n",
    "        top_terms = train_terms['term'].value_counts().head(13500).index.tolist()\n",
    "    train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "    y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "    y_df = y_df.reindex(train_ids, fill_value=0)\n",
    "    y_df = y_df.reindex(columns=top_terms, fill_value=0)\n",
    "    y = y_df.values.astype(np.float32)\n",
    "    return y, top_terms\n",
    "\n",
    "def _load_ia_weights(top_terms: list[str]) -> np.ndarray:\n",
    "    # IA.tsv is required under DATASET_ROOT; still guard for robustness\n",
    "    ia_path = WORK_ROOT / 'IA.tsv'\n",
    "    if not ia_path.exists():\n",
    "        return np.ones((len(top_terms),), dtype=np.float32)\n",
    "    ia_df = pd.read_csv(ia_path, sep='\\t', names=['term', 'ia'])\n",
    "    ia_map = dict(zip(ia_df['term'], ia_df['ia']))\n",
    "    w = np.array([float(ia_map.get(t, 0.0) or 0.0) for t in top_terms], dtype=np.float32)\n",
    "    # Avoid all-zeros (would collapse the metric)\n",
    "    if float(w.sum()) <= 0:\n",
    "        w = np.ones_like(w)\n",
    "    return w\n",
    "\n",
    "def _subsample_rows(y_true: np.ndarray, *ys: np.ndarray) -> tuple[np.ndarray, list[np.ndarray]]:\n",
    "    n = int(y_true.shape[0])\n",
    "    m = min(n, int(DIAG_N))\n",
    "    if m <= 0:\n",
    "        return y_true[:0], [a[:0] for a in ys]\n",
    "    idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "    out = [np.asarray(a[idx], dtype=np.float32) for a in ys]\n",
    "    return np.asarray(y_true[idx], dtype=np.float32), out\n",
    "\n",
    "def _ia_f1(y_true: np.ndarray, y_score: np.ndarray, weights: np.ndarray, thr: float) -> float:\n",
    "    # IA-weighted F1 over ALL terms (no aspect split here; goal is model contribution ranking)\n",
    "    y_true = (y_true > 0).astype(np.int8)\n",
    "    y_pred = (y_score >= float(thr)).astype(np.int8)\n",
    "    tp = (y_pred & y_true).sum(axis=0).astype(np.float64)\n",
    "    pred = y_pred.sum(axis=0).astype(np.float64)\n",
    "    true = y_true.sum(axis=0).astype(np.float64)\n",
    "    w = weights.astype(np.float64)\n",
    "    w_tp = float((w * tp).sum())\n",
    "    w_pred = float((w * pred).sum())\n",
    "    w_true = float((w * true).sum())\n",
    "    p = (w_tp / w_pred) if w_pred > 0 else 0.0\n",
    "    r = (w_tp / w_true) if w_true > 0 else 0.0\n",
    "    return (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "\n",
    "def _best_over_thrs(y_true: np.ndarray, y_score: np.ndarray, weights: np.ndarray) -> tuple[float, float, np.ndarray]:\n",
    "    scores = np.array([_ia_f1(y_true, y_score, weights, thr=float(t)) for t in THRS], dtype=np.float32)\n",
    "    best_i = int(np.argmax(scores))\n",
    "    return float(THRS[best_i]), float(scores[best_i]), scores\n",
    "# ----------------------------\n",
    "# Load Y + predictions\n",
    "# ----------------------------\n",
    "try:\n",
    "    Y_full, top_terms = _load_targets_topk()\n",
    "    w = _load_ia_weights(top_terms)\n",
    "except Exception as e:\n",
    "    print('Cannot run contributor analysis (missing targets/IA):', repr(e))\n",
    "    raise\n",
    "models = {\n",
    "    'logreg': _load_pred('oof_pred_logreg'),\n",
    "    'gbdt': _load_pred('oof_pred_gbdt'),\n",
    "    'dnn': _load_pred('oof_pred_dnn'),\n",
    "    'knn': _load_pred('oof_pred_knn'),\n",
    "}\n",
    "models = {k: v for k, v in models.items() if v is not None}\n",
    "if not models:\n",
    "    print('No OOF prediction matrices found. Expected files like features/level1_preds/oof_pred_logreg.npy')\n",
    "    raise SystemExit(0)\n",
    "# Sanity: shape alignment\n",
    "for k, v in models.items():\n",
    "    if v.shape != Y_full.shape:\n",
    "        raise ValueError(f'Model {k} shape mismatch: got {v.shape}, expected {Y_full.shape}')\n",
    "# Baseline = mean ensemble\n",
    "stack = np.mean(np.stack(list(models.values()), axis=0), axis=0).astype(np.float32)\n",
    "Y_sub, [stack_sub] = _subsample_rows(Y_full, stack)\n",
    "thr0, s0, curve0 = _best_over_thrs(Y_sub, stack_sub, w)\n",
    "print(f'Baseline mean-ensemble best IA-F1={s0:.4f} @ thr={thr0:.3f} (sampled N={len(Y_sub)})')\n",
    "# Ablations\n",
    "rows = []\n",
    "keys = list(models.keys())\n",
    "for drop in keys:\n",
    "    keep = [k for k in keys if k != drop]\n",
    "    st = np.mean(np.stack([models[k] for k in keep], axis=0), axis=0).astype(np.float32)\n",
    "    Y_sub, [st_sub] = _subsample_rows(Y_full, st)\n",
    "    thr, s, _ = _best_over_thrs(Y_sub, st_sub, w)\n",
    "    rows.append({'drop': drop, 'best_ia_f1': s, 'best_thr': thr, 'delta_vs_base': s - s0})\n",
    "ab = pd.DataFrame(rows).sort_values('delta_vs_base', ascending=True)\n",
    "print('\\nAblation deltas (negative = dropping hurts):')\n",
    "print(ab)\n",
    "if plt is not None:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.barh(ab['drop'], ab['delta_vs_base'])\n",
    "    plt.axvline(0.0, color='k', linewidth=1)\n",
    "    plt.title('OOF mean-ensemble: IA-F1 change when dropping one model (sampled)')\n",
    "    plt.xlabel(' IA-F1 vs baseline')\n",
    "    plt.ylabel('dropped model')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "011a2984555b43408e4e1c5e163b88d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "088aab8fb2474be8907b4b93e1acb3ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4471863dc9c64f8982c8f84fbd0929b6",
       "IPY_MODEL_a3afb8c80fe043ec90542de597a7b24c",
       "IPY_MODEL_bbc67a4be0254994aa963f470e366c45"
      ],
      "layout": "IPY_MODEL_a6938cdc756a4a449bbf1e4876a928dc"
     }
    },
    "08e42bc79a764754a9c29ae767a73056": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_200129b724424cefba7eab313c236046",
      "max": 849,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e036cfd62a1148989a25c8455b8af964",
      "value": 849
     }
    },
    "0b97f98e312a414693ae24c8e1adee8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0ee2c338d3c4dd499bfd94b3e0d6b72",
      "max": 7515077421,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_725e70ff793949459a453ecc5cc47af2",
      "value": 7515077421
     }
    },
    "0f0f149d523b42fc9c547995d844d0d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "15d93ab946bb49db9ac9e19e4e8dd772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfc5bd70a63845fb8f83fb4a0dbedb4a",
      "max": 7514916144,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9032af986942492686d77fd438b65011",
      "value": 7514916144
     }
    },
    "1b4eb7d199504354884cd3d1270267c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5eccbaab4174e6db90be072660966d8",
      "placeholder": "",
      "style": "IPY_MODEL_dbf709b4d0654086b44622309fcd4b4f",
      "value": "config.json:100%"
     }
    },
    "1fc5cc8957c84d91bd3b6b26f6ad3bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1fcdea1c4f24685ace973a7a7ad4b63",
      "placeholder": "",
      "style": "IPY_MODEL_c66898ee085b40c28df2cddac70c9f5c",
      "value": "EmbeddingANKH(SmartBatch):67%"
     }
    },
    "1ffca17f62e24e85a02c7a03d300f691": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "200129b724424cefba7eab313c236046": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22ddcb4f9fc24df7b51f964662b38767": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29a5e01835c645b8ada633c177d1a081": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78986fba9af248fb89683bb9ee1ca0f7",
      "placeholder": "",
      "style": "IPY_MODEL_9bb0e9eb949541ac8a4fbb64d0d19be4",
      "value": "model.safetensors:100%"
     }
    },
    "2c31b96c78624ca8acb17e3e8cdf007f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "310911b6bd4d47c3958e3aea1a3bf2d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "388a3ca0d6d340ebb05e13044802ba91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_310911b6bd4d47c3958e3aea1a3bf2d0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ffca17f62e24e85a02c7a03d300f691",
      "value": 1
     }
    },
    "3bcee3914b8f472885d1edc1e5f062c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_54aee4cf51a3433d8dde5e0ed1b6cb45",
       "IPY_MODEL_388a3ca0d6d340ebb05e13044802ba91",
       "IPY_MODEL_8aebce47ff0f40bd9ff677669c73cff2"
      ],
      "layout": "IPY_MODEL_cf6a6078b24c4682b53b982ca82fef9a"
     }
    },
    "42f83c186d1a4fdd89cc9cb37943c9ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4471863dc9c64f8982c8f84fbd0929b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c31b96c78624ca8acb17e3e8cdf007f",
      "placeholder": "",
      "style": "IPY_MODEL_469cb5f1147446e4a7bca4d1b00e7f74",
      "value": "special_tokens_map.json:"
     }
    },
    "469cb5f1147446e4a7bca4d1b00e7f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49a1e7eeb59549f7bde6b5dc77a5443c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_94258441ba164dd6a791e659d0a3ea13",
       "IPY_MODEL_8113820c64dd40118afafc8a56041910",
       "IPY_MODEL_7360f7ccef5349cb816c1774a30ae7e9"
      ],
      "layout": "IPY_MODEL_bd89145d91c54ed4be02ad48e7558271"
     }
    },
    "54aee4cf51a3433d8dde5e0ed1b6cb45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5c40b93b1b24c4d87f504ad48568979",
      "placeholder": "",
      "style": "IPY_MODEL_0f0f149d523b42fc9c547995d844d0d1",
      "value": "tokenizer.json:"
     }
    },
    "62fdfb366acf45ca95e556ab5ef6d483": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1b4eb7d199504354884cd3d1270267c4",
       "IPY_MODEL_08e42bc79a764754a9c29ae767a73056",
       "IPY_MODEL_c7e4de822c564d24a7c4bb58c107f980"
      ],
      "layout": "IPY_MODEL_011a2984555b43408e4e1c5e163b88d1"
     }
    },
    "6439fbe2447e4f059904d7af4806aa65": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_663d716c63f042699b2b49e3c9aaea60",
      "placeholder": "",
      "style": "IPY_MODEL_7a9b1a8477cb437ea2dcd8ac38a52311",
      "value": "pytorch_model.bin:100%"
     }
    },
    "6600ee0c54ee4903adb2a28a70d5950d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "663d716c63f042699b2b49e3c9aaea60": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69df82b9c6f14ddb99b8dee43fbd8d1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "725e70ff793949459a453ecc5cc47af2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7308a11db52f4cc1884a94f457cc5d0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7360f7ccef5349cb816c1774a30ae7e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9869069e7adc4fc29569093e9818faf8",
      "placeholder": "",
      "style": "IPY_MODEL_83d205cd553b4a419738077b6eea679c",
      "value": "2.85k/?[00:00&lt;00:00,335kB/s]"
     }
    },
    "7850656efc39421283d8f13684b311ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78986fba9af248fb89683bb9ee1ca0f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a9b1a8477cb437ea2dcd8ac38a52311": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8113820c64dd40118afafc8a56041910": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cde1407606464fd79d9aa61ef44428dd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83a2b49fb4564294aa88469de130fda8",
      "value": 1
     }
    },
    "83a2b49fb4564294aa88469de130fda8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83d205cd553b4a419738077b6eea679c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86e8814ed91749c999feb3bec80e3404": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6439fbe2447e4f059904d7af4806aa65",
       "IPY_MODEL_0b97f98e312a414693ae24c8e1adee8b",
       "IPY_MODEL_ab3bb536171f47a98a4f98ee8f46ab39"
      ],
      "layout": "IPY_MODEL_c744b89abd6e4c2183f432b840737a24"
     }
    },
    "8aa7f2081b26436dbb9b5f6a867a8c07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "8aebce47ff0f40bd9ff677669c73cff2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2061fb21a18495eab8241fa7bc8c6eb",
      "placeholder": "",
      "style": "IPY_MODEL_ee0a833bc86b459eba420ad0c96b939d",
      "value": "31.2k/?[00:00&lt;00:00,3.76MB/s]"
     }
    },
    "8fafe0b9758f45f182cd379c39ef8751": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9032af986942492686d77fd438b65011": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "94258441ba164dd6a791e659d0a3ea13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a854f2e91cd4f56b0937e4f222abb5c",
      "placeholder": "",
      "style": "IPY_MODEL_7850656efc39421283d8f13684b311ac",
      "value": "tokenizer_config.json:"
     }
    },
    "9869069e7adc4fc29569093e9818faf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a854f2e91cd4f56b0937e4f222abb5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bb0e9eb949541ac8a4fbb64d0d19be4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3afb8c80fe043ec90542de597a7b24c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8aa7f2081b26436dbb9b5f6a867a8c07",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6600ee0c54ee4903adb2a28a70d5950d",
      "value": 1
     }
    },
    "a4d42157de5744dcbba3a001dfbe9f86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6938cdc756a4a449bbf1e4876a928dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6f8a8cb5d5945dd9cb65d836d91463d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a709033de4a642b89430f2eb009d4a05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab2d76783f3044be907ea3019858b1e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fafe0b9758f45f182cd379c39ef8751",
      "max": 10301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb52621d45004c1c869beb22ba10abf1",
      "value": 6933
     }
    },
    "ab3bb536171f47a98a4f98ee8f46ab39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_42f83c186d1a4fdd89cc9cb37943c9ec",
      "placeholder": "",
      "style": "IPY_MODEL_a4d42157de5744dcbba3a001dfbe9f86",
      "value": "7.52G/7.52G[00:27&lt;00:00,320MB/s]"
     }
    },
    "b0ee2c338d3c4dd499bfd94b3e0d6b72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbc67a4be0254994aa963f470e366c45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f94a7c141ba1444f9070a9ffa788f447",
      "placeholder": "",
      "style": "IPY_MODEL_fce09728b0f0497992d9104932bfd2ea",
      "value": "2.58k/?[00:00&lt;00:00,312kB/s]"
     }
    },
    "bd89145d91c54ed4be02ad48e7558271": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1fcdea1c4f24685ace973a7a7ad4b63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c66898ee085b40c28df2cddac70c9f5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c744b89abd6e4c2183f432b840737a24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c784e4371d134289a029a1fefd7f8567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7e4de822c564d24a7c4bb58c107f980": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7308a11db52f4cc1884a94f457cc5d0a",
      "placeholder": "",
      "style": "IPY_MODEL_c784e4371d134289a029a1fefd7f8567",
      "value": "849/849[00:00&lt;00:00,123kB/s]"
     }
    },
    "cb52621d45004c1c869beb22ba10abf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cde1407606464fd79d9aa61ef44428dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "cf6a6078b24c4682b53b982ca82fef9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfc5bd70a63845fb8f83fb4a0dbedb4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3edc963c4f2461197f769252c3bfee6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5c40b93b1b24c4d87f504ad48568979": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbf709b4d0654086b44622309fcd4b4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e036cfd62a1148989a25c8455b8af964": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e2061fb21a18495eab8241fa7bc8c6eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5eccbaab4174e6db90be072660966d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8077f535fd048da929e5c39896fbbf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed9a029942704ac8ac66abf47546f0b2",
      "placeholder": "",
      "style": "IPY_MODEL_a6f8a8cb5d5945dd9cb65d836d91463d",
      "value": "6933/10301[11:37:31&lt;2:52:01,3.06s/it]"
     }
    },
    "ed9a029942704ac8ac66abf47546f0b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee0a833bc86b459eba420ad0c96b939d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3403693f736496f87a35cd05dbfb619": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1fc5cc8957c84d91bd3b6b26f6ad3bd8",
       "IPY_MODEL_ab2d76783f3044be907ea3019858b1e9",
       "IPY_MODEL_e8077f535fd048da929e5c39896fbbf8"
      ],
      "layout": "IPY_MODEL_22ddcb4f9fc24df7b51f964662b38767"
     }
    },
    "f87e7e87dc554744a85d6e75222b8410": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3edc963c4f2461197f769252c3bfee6",
      "placeholder": "",
      "style": "IPY_MODEL_69df82b9c6f14ddb99b8dee43fbd8d1a",
      "value": "7.51G/7.51G[00:26&lt;00:00,381MB/s]"
     }
    },
    "f94a7c141ba1444f9070a9ffa788f447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc3e7727c4ab4e20a44014e13eebd610": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_29a5e01835c645b8ada633c177d1a081",
       "IPY_MODEL_15d93ab946bb49db9ac9e19e4e8dd772",
       "IPY_MODEL_f87e7e87dc554744a85d6e75222b8410"
      ],
      "layout": "IPY_MODEL_a709033de4a642b89430f2eb009d4a05"
     }
    },
    "fce09728b0f0497992d9104932bfd2ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
