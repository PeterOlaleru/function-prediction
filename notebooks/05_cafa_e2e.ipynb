{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d42ecde8",
   "metadata": {
    "id": "bootstrap",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\n",
      "DATA_ROOT: C:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\cafa6_data\n"
     ]
    }
   ],
   "source": [
    "# CELL 01 - Setup (NO REPO)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Always run from a simple writable location; never cd into a repo.\n",
    "if os.path.exists('/content'):\n",
    "    os.chdir('/content')\n",
    "RUNTIME_ROOT = Path.cwd()\n",
    "DATA_ROOT = (RUNTIME_ROOT / 'cafa6_data')\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print(f'CWD: {Path.cwd()}')\n",
    "print(f'DATA_ROOT: {DATA_ROOT.resolve()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94540c0a",
   "metadata": {
    "id": "install",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local Detected\n",
      "+ c:\\Users\\Olale\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install pandas numpy scipy pyarrow scikit-learn torch transformers py-boost biopython obonet networkx matplotlib seaborn jupyter ipykernel nbformat nbclient tqdm requests urllib3 joblib psutil fastparquet pyyaml kaggle\n"
     ]
    }
   ],
   "source": [
    "# CELL 02 - Install dependencies (mandatory, early)\n",
    "import importlib.util\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def _detect_kaggle() -> bool:\n",
    "    return bool(os.environ.get('KAGGLE_KERNEL_RUN_TYPE') or os.environ.get('KAGGLE_URL_BASE') or os.environ.get('KAGGLE_DATA_PROXY_URL'))\n",
    "def _detect_colab() -> bool:\n",
    "    return bool(os.environ.get('COLAB_RELEASE_TAG') or os.environ.get('COLAB_GPU') or os.environ.get('COLAB_TPU_ADDR'))\n",
    "IS_KAGGLE = _detect_kaggle()\n",
    "IS_COLAB = (not IS_KAGGLE) and _detect_colab()\n",
    "if IS_KAGGLE:\n",
    "    print('Environment: Kaggle Detected')\n",
    "elif IS_COLAB:\n",
    "    print('Environment: Colab Detected')\n",
    "else:\n",
    "    print('Environment: Local Detected')\n",
    "\n",
    "def _pip_install(pkgs: list[str]) -> None:\n",
    "    if not pkgs:\n",
    "        return\n",
    "    print('+', sys.executable, '-m', 'pip', 'install', *pkgs)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install', *pkgs])\n",
    "# Kaggle has a heavily preinstalled environment; avoid upgrading core packages by default.\n",
    "# We still guarantee requirements are present by installing missing ones.\n",
    "REQUIRED = {\n",
    "    # Core\n",
    "    'pandas': 'pandas',\n",
    "    'numpy': 'numpy',\n",
    "    'scipy': 'scipy',\n",
    "    'pyarrow': 'pyarrow',\n",
    "    # ML\n",
    "    'scikit-learn': 'sklearn',\n",
    "    'torch': 'torch',\n",
    "    'transformers': 'transformers',\n",
    "    'py-boost': 'py_boost',\n",
    "    # Bio / graph\n",
    "    'biopython': 'Bio',\n",
    "    'obonet': 'obonet',\n",
    "    'networkx': 'networkx',\n",
    "    # Visualisation\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn',\n",
    "    # Notebook tooling\n",
    "    'jupyter': 'jupyter',\n",
    "    'ipykernel': 'ipykernel',\n",
    "    'nbformat': 'nbformat',\n",
    "    'nbclient': 'nbclient',\n",
    "    # Utils\n",
    "    'tqdm': 'tqdm',\n",
    "    'requests': 'requests',\n",
    "    'urllib3': 'urllib3',\n",
    "    'joblib': 'joblib',\n",
    "    'psutil': 'psutil',\n",
    "    'fastparquet': 'fastparquet',\n",
    "    'pyyaml': 'yaml',\n",
    "    'kaggle': 'kaggle',\n",
    "}\n",
    "if IS_KAGGLE:\n",
    "    missing = [pkg for pkg, mod in REQUIRED.items() if importlib.util.find_spec(mod) is None]\n",
    "    if missing:\n",
    "        _pip_install(missing)\n",
    "    else:\n",
    "        print('Kaggle: skipping pip install (already satisfied).')\n",
    "else:\n",
    "    # Colab/Local: enforce dependencies up-front.\n",
    "    _pip_install(list(REQUIRED.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908e2b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle auth: username length=8, key length=32\n"
     ]
    }
   ],
   "source": [
    "# CELL 02b - Kaggle auth bootstrap (username/key only)\n",
    "# This avoids relying on a pre-existing ~/.kaggle/kaggle.json.\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _detect_kaggle_env() -> bool:\n",
    "    return bool(\n",
    "        os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\n",
    "        or os.environ.get('KAGGLE_URL_BASE')\n",
    "        or os.environ.get('KAGGLE_DATA_PROXY_URL')\n",
    "    )\n",
    "\n",
    "\n",
    "def _detect_colab_env() -> bool:\n",
    "    return bool(os.environ.get('COLAB_RELEASE_TAG') or os.environ.get('COLAB_GPU') or os.environ.get('COLAB_TPU_ADDR'))\n",
    "\n",
    "\n",
    "IS_KAGGLE_ENV = _detect_kaggle_env()\n",
    "IS_COLAB_ENV = (not IS_KAGGLE_ENV) and _detect_colab_env()\n",
    "\n",
    "\n",
    "def _load_dotenv_if_present(dotenv_path: Path) -> None:\n",
    "    # Minimal .env loader (no extra deps). Lines like KEY=VALUE, with optional quotes.\n",
    "    try:\n",
    "        dotenv_path = Path(dotenv_path)\n",
    "        if not dotenv_path.exists():\n",
    "            return\n",
    "        for raw in dotenv_path.read_text(encoding='utf-8').splitlines():\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith('#') or '=' not in line:\n",
    "                continue\n",
    "            k, v = line.split('=', 1)\n",
    "            k = k.strip()\n",
    "            v = v.strip().strip('\"').strip(\"'\")\n",
    "            if k and k not in os.environ:\n",
    "                os.environ[k] = v\n",
    "    except Exception:\n",
    "        return\n",
    "\n",
    "\n",
    "# Local convenience: allow a repo-level .env (gitignored) to populate env vars.\n",
    "_cwd = Path.cwd()\n",
    "_load_dotenv_if_present(_cwd / '.env')\n",
    "_load_dotenv_if_present(_cwd.parent / '.env')\n",
    "\n",
    "\n",
    "def _get_secret_env_first(name: str) -> str:\n",
    "    # Policy:\n",
    "    # - Colab: use userdata.get(name)\n",
    "    # - Kaggle: use UserSecretsClient().get_secret(name)\n",
    "    # - Else: env vars (possibly loaded from .env)\n",
    "    v = (os.environ.get(name, '') or '').strip()\n",
    "    if v:\n",
    "        return v\n",
    "    if IS_COLAB_ENV:\n",
    "        try:\n",
    "            from google.colab import userdata  # type: ignore\n",
    "            return (userdata.get(name) or '').strip()\n",
    "        except Exception:\n",
    "            return ''\n",
    "    if IS_KAGGLE_ENV:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient  # type: ignore\n",
    "            return (UserSecretsClient().get_secret(name) or '').strip()\n",
    "        except Exception:\n",
    "            return ''\n",
    "    return ''\n",
    "\n",
    "\n",
    "_k_u = _get_secret_env_first('KAGGLE_USERNAME')\n",
    "_k_k = _get_secret_env_first('KAGGLE_KEY')\n",
    "\n",
    "if _k_u and _k_k:\n",
    "    os.environ['KAGGLE_USERNAME'] = _k_u\n",
    "    os.environ['KAGGLE_KEY'] = _k_k\n",
    "    print(f\"Kaggle auth: username length={len(_k_u)}, key length={len(_k_k)}\")\n",
    "else:\n",
    "    print('Kaggle auth: missing KAGGLE_USERNAME/KAGGLE_KEY (Kaggle downloads will fail).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d444c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset: peterola/cafa6-checkpoints\n",
      "KAGGLE_USERNAME set: True\n",
      "KAGGLE_KEY set: True\n",
      "datasets files rc: 0\n",
      "stdout:\n",
      " Next Page Token = CfDJ8KYR5TFjzfZErZMgR3fFdNjjHcaUEJdnn_OjUFakqoPsBMJjTQNQxaee9G7Z9MJbpZ0tqnDa1pstcif9ykFkVgvq2EB-3USS7OgFnXMdGpRFHDQFJI4zPaMzFqlRWlu18_LPfzFCBsT4CEPTaHLVXQkd\n",
      "name                                     size  creationDate                \n",
      "---------------------------------  ----------  --------------------------  \n",
      "README.md                                 115  2025-12-18 22:01:05.323000  \n",
      "cleanup_removed_ankh_npys.txt             175  2025-12-18 22:01:05         \n",
      "external/entryid_text.tsv           713222242  2025-12-18 22:01:26         \n",
      "external/prop_test_no_kaggle.tsv     51212345  2025-12-18 22:01:12.031000  \n",
      "external/prop_train_no_kaggle.tsv          19  2025-12-18 22:01:06.931000  \n",
      "features/test_embeds_esm2.npy      1148462208  2025-12-18 22:03:24         \n",
      "features/test_embeds_esm2_3b.npy   2296924288  2025-12-18 22:04:33.836000  \n",
      "features/test_embeds_t5.npy         918769792  2025-12-18 22:03:34.233000  \n",
      "features/test_embeds_text.npy      4611344550  2025-12-18 22:04:01.021000  \n",
      "features/text_vectorizer.joblib      83949164  2025-12-18 22:03:08         \n",
      "features/train_embeds_esm2.npy      421908608  2025-12-18 22:03:31         \n",
      "features/train_embeds_esm2_3b.npy   843817088  2025-12-18 22:03:03.468000  \n",
      "features/train_embeds_t5.npy        337526912  2025-12-18 22:02:17.455000  \n",
      "features/train_embeds_text.npy     1694061560  2025-12-18 22:02:34         \n",
      "manifest.json                            6479  2025-12-18 22:01:05.267000  \n",
      "parsed/term_counts.parquet             198649  2025-12-18 22:02:05.683000  \n",
      "parsed/term_priors.parquet             307506  2025-12-18 22:02:05.574000  \n",
      "parsed/test_seq.feather              96138586  2025-12-18 22:02:06.686000  \n",
      "parsed/test_taxa.feather              2521378  2025-12-18 22:02:05.683000  \n",
      "parsed/train_seq.feather             44512450  2025-12-18 22:02:06.532000  \n",
      "\n",
      "stderr:\n",
      " \n",
      "download f= parsed__train_seq.feather rc: 1\n",
      "stdout:\n",
      " Dataset URL: https://www.kaggle.com/datasets/peterola/cafa6-checkpoints\n",
      "License(s): CC0-1.0\n",
      "404 Client Error: Not Found for url: https://www.kaggle.com/api/v1/datasets/download/peterola/cafa6-checkpoints/parsed__train_seq.feather?filename=parsed__train_seq.feather&raw=false\n",
      "\n",
      "stderr:\n",
      " \n",
      "download f= parsed/train_seq.feather rc: 1\n",
      "stdout:\n",
      " Dataset URL: https://www.kaggle.com/datasets/peterola/cafa6-checkpoints\n",
      "License(s): CC0-1.0\n",
      "404 Client Error: Not Found for url: https://www.kaggle.com/api/v1/datasets/download/peterola/cafa6-checkpoints/parsed/train_seq.feather?filename=parsed%2Ftrain_seq.feather&raw=false\n",
      "\n",
      "stderr:\n",
      " \n",
      "download f= train_seq.feather rc: 1\n",
      "stdout:\n",
      " Dataset URL: https://www.kaggle.com/datasets/peterola/cafa6-checkpoints\n",
      "License(s): CC0-1.0\n",
      "404 Client Error: Not Found for url: https://www.kaggle.com/api/v1/datasets/download/peterola/cafa6-checkpoints/train_seq.feather?filename=train_seq.feather&raw=false\n",
      "\n",
      "stderr:\n",
      " \n",
      "download f= manifest.json rc: 0\n",
      "stdout:\n",
      " Dataset URL: https://www.kaggle.com/datasets/peterola/cafa6-checkpoints\n",
      "License(s): CC0-1.0\n",
      "Downloading manifest.json to cafa6_data\\_tmp_kaggle_debug\n",
      "\n",
      "\n",
      "stderr:\n",
      " \n",
      "  0%|          | 0.00/6.33k [00:00<?, ?B/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.33k/6.33k [00:00<00:00, 6.48MB/s]\n",
      "\n",
      "tmp files: ['manifest.json']\n"
     ]
    }
   ],
   "source": [
    "# CELL 02c - Kaggle CLI sanity check (debug)\n",
    "# Verifies the current auth can see the checkpoint dataset and download a single file.\n",
    "\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "_dataset = os.environ.get('CAFA_CHECKPOINT_DATASET_ID', 'peterola/cafa6-checkpoints')\n",
    "_tmp = Path('cafa6_data') / '_tmp_kaggle_debug'\n",
    "_tmp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Using dataset:', _dataset)\n",
    "print('KAGGLE_USERNAME set:', bool(os.environ.get('KAGGLE_USERNAME')))\n",
    "print('KAGGLE_KEY set:', bool(os.environ.get('KAGGLE_KEY')))\n",
    "\n",
    "p = subprocess.run(['kaggle', 'datasets', 'files', '-d', _dataset], capture_output=True, text=True)\n",
    "print('datasets files rc:', p.returncode)\n",
    "print('stdout:\\n', p.stdout[:2000])\n",
    "print('stderr:\\n', p.stderr[:2000])\n",
    "\n",
    "candidates = [\n",
    "    # flat name (our mapping)\n",
    "    'parsed__train_seq.feather',\n",
    "    # path as listed\n",
    "    'parsed/train_seq.feather',\n",
    "    # basename only (some Kaggle endpoints behave like this)\n",
    "    'train_seq.feather',\n",
    "    # top-level file (control)\n",
    "    'manifest.json',\n",
    "]\n",
    "\n",
    "for f in candidates:\n",
    "    p = subprocess.run(\n",
    "        ['kaggle', 'datasets', 'download', '-d', _dataset, '-f', f, '-p', str(_tmp)],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "    print('download f=', f, 'rc:', p.returncode)\n",
    "    print('stdout:\\n', p.stdout[:2000])\n",
    "    print('stderr:\\n', p.stderr[:2000])\n",
    "\n",
    "print('tmp files:', [x.name for x in _tmp.glob('*')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858dad8",
   "metadata": {
    "id": "1858dad8",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Local Detected\n",
      "DATASET_ROOT: C:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\cafa6_data\n",
      "Checkpoint status (after pull):\n",
      "  WORK_ROOT: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\cafa6_data\n",
      "  parsed/: OK (0.0 MB)\n",
      "  external/: OK (0.0 MB)\n",
      "  features/: OK (0.0 MB)\n",
      "  external/entryid_text.tsv: MISSING\n",
      "  parsed/train_seq.feather: MISSING\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 03 - Solution: 1. SETUP, CONFIG & DIAGNOSTICS\n",
    "# 1. SETUP, CONFIG & DIAGNOSTICS\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------\n",
    "# Environment Detection & Paths\n",
    "# ------------------------------------------\n",
    "# Kaggle images can have `google-colab` installed; never use `import google.colab` as a signal.\n",
    "\n",
    "def _detect_kaggle() -> bool:\n",
    "    # Kaggle kernels reliably set at least one of these env vars.\n",
    "    return bool(\n",
    "        os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\n",
    "        or os.environ.get('KAGGLE_URL_BASE')\n",
    "        or os.environ.get('KAGGLE_DATA_PROXY_URL')\n",
    "    )\n",
    "\n",
    "def _detect_colab() -> bool:\n",
    "    # Colab sets these env vars; this avoids false positives on Kaggle.\n",
    "    return bool(os.environ.get('COLAB_RELEASE_TAG') or os.environ.get('COLAB_GPU') or os.environ.get('COLAB_TPU_ADDR'))\n",
    "\n",
    "IS_KAGGLE = _detect_kaggle()\n",
    "IS_COLAB = (not IS_KAGGLE) and _detect_colab()\n",
    "\n",
    "KAGGLE_INPUT_ROOT = Path('/kaggle/input')\n",
    "if IS_KAGGLE:\n",
    "    print('Environment: Kaggle Detected')\n",
    "    WORKING_ROOT = Path('/kaggle/working')\n",
    "    if KAGGLE_INPUT_ROOT.exists():\n",
    "        for dirname, _, filenames in os.walk(str(KAGGLE_INPUT_ROOT)):\n",
    "            for filename in filenames:\n",
    "                print(os.path.join(dirname, filename))\n",
    "elif IS_COLAB:\n",
    "    print('Environment: Colab Detected')\n",
    "    WORKING_ROOT = Path(os.environ.get('CAFA_WORKING_ROOT', str(Path('/content'))))\n",
    "else:\n",
    "    print('Environment: Local Detected')\n",
    "    # No repo assumptions: treat current working dir as runtime root.\n",
    "    WORKING_ROOT = Path.cwd()\n",
    "# ------------------------------------------\n",
    "# Local cache roots (ephemeral) + artefacts root\n",
    "# ------------------------------------------\n",
    "# Single source of truth for this notebook: everything lives under cafa6_data/.\n",
    "# If Cell 1 ran, reuse its DATA_ROOT so we don't fork paths.\n",
    "if 'DATA_ROOT' in globals():\n",
    "    WORK_ROOT = Path(DATA_ROOT)\n",
    "    WORKING_ROOT = WORK_ROOT.parent\n",
    "else:\n",
    "    WORK_ROOT = WORKING_ROOT / 'cafa6_data'\n",
    "WORK_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "for _d in ['parsed', 'features', 'external', 'Train', 'Test']:\n",
    "    (WORK_ROOT / _d).mkdir(parents=True, exist_ok=True)\n",
    "# Keep caches OUT of WORK_ROOT so we never accidentally publish them.\n",
    "CACHE_ROOT = WORKING_ROOT / 'cache'\n",
    "CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "os.environ.setdefault('HF_HOME', str(CACHE_ROOT / 'hf_home'))\n",
    "os.environ.setdefault('TRANSFORMERS_CACHE', str(CACHE_ROOT / 'hf_home'))\n",
    "os.environ.setdefault('HF_HUB_CACHE', str(CACHE_ROOT / 'hf_hub'))\n",
    "os.environ.setdefault('TORCH_HOME', str(CACHE_ROOT / 'torch_home'))\n",
    "# Runtime provenance guards: never publish downloaded artefacts.\n",
    "RUN_START_TS = time.time()\n",
    "DOWNLOADED_PATHS: set[Path] = set()\n",
    "\n",
    "def _mark_downloaded(p: Path) -> None:\n",
    "    DOWNLOADED_PATHS.add(Path(p).resolve())\n",
    "# ------------------------------------------\n",
    "# Dataset Discovery (competition data)\n",
    "DATASET_SLUG = 'cafa-6-protein-function-prediction'\n",
    "# Required competition files (MANDATORY):\n",
    "REQUIRED_COMP_FILES = [\n",
    "    'IA.tsv',\n",
    "    'sample_submission.tsv',\n",
    "    'Train/go-basic.obo',\n",
    "    'Train/train_sequences.fasta',\n",
    "    'Train/train_terms.tsv',\n",
    "    'Train/train_taxonomy.tsv',\n",
    "    'Test/testsuperset.fasta',\n",
    "    'Test/testsuperset-taxon-list.tsv',\n",
    "]\n",
    "\n",
    "def _ensure_kaggle_cli() -> None:\n",
    "    try:\n",
    "        subprocess.run(['kaggle', '--version'], check=True, capture_output=True, text=True)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install', 'kaggle'])\n",
    "        subprocess.run(['kaggle', '--version'], check=True)\n",
    "\n",
    "def _kaggle_env(require: bool = True) -> dict[str, str]:\n",
    "    env = os.environ.copy()\n",
    "    if require and (not env.get('KAGGLE_USERNAME') or not env.get('KAGGLE_KEY')):\n",
    "        raise RuntimeError('Missing Kaggle API auth: set KAGGLE_USERNAME and KAGGLE_KEY as secrets/env vars.')\n",
    "    return env\n",
    "\n",
    "def _download_comp_file(rel_path: str, target_root: Path) -> None:\n",
    "    \"\"\"Download one competition file into target_root, preserving folders.\"\"\"\n",
    "    rel_path = rel_path.replace('\\\\', '/')\n",
    "    out_path = target_root / rel_path\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        return\n",
    "    _ensure_kaggle_cli()\n",
    "    env = _kaggle_env(require=True)\n",
    "    tmp = target_root / '_tmp_download'\n",
    "    tmp.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = ['kaggle', 'competitions', 'download', '-c', DATASET_SLUG, '-f', rel_path, '-p', str(tmp)]\n",
    "    print('+', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True, env=env)\n",
    "    name = Path(rel_path).name\n",
    "    zip_path = tmp / f'{name}.zip'\n",
    "    if zip_path.exists():\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(tmp)\n",
    "        zip_path.unlink()\n",
    "    cand1 = tmp / rel_path\n",
    "    cand2 = tmp / name\n",
    "    src = cand1 if cand1.exists() else (cand2 if cand2.exists() else None)\n",
    "    if src is None:\n",
    "        raise FileNotFoundError(f'Downloaded file not found after unzip: {rel_path}')\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.move(str(src), str(out_path))\n",
    "    _mark_downloaded(out_path)\n",
    "\n",
    "def ensure_competition_data(data_root: Path) -> Path:\n",
    "    \"\"\"Ensures competition data exists under cafa6_data/. Returns dataset root.\"\"\"\n",
    "    data_root = Path(data_root)\n",
    "    # Kaggle: copy from mounted input if available.\n",
    "    if IS_KAGGLE and Path('/kaggle/input').exists():\n",
    "        mounted = Path('/kaggle/input') / DATASET_SLUG\n",
    "        if mounted.exists():\n",
    "            for rel in REQUIRED_COMP_FILES:\n",
    "                dst = data_root / rel\n",
    "                if dst.exists():\n",
    "                    continue\n",
    "                src = mounted / rel\n",
    "                dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                if not src.exists():\n",
    "                    raise FileNotFoundError(f'Missing in mounted Kaggle input: {src}')\n",
    "                shutil.copy2(src, dst)\n",
    "                _mark_downloaded(dst)\n",
    "            return data_root\n",
    "    # Colab/Local: download file-by-file via Kaggle API.\n",
    "    for rel in REQUIRED_COMP_FILES:\n",
    "        _download_comp_file(rel, data_root)\n",
    "    return data_root\n",
    "# Canonical dataset root for the entire notebook\n",
    "DATASET_ROOT = ensure_competition_data(WORK_ROOT)\n",
    "# Canonical competition paths (always under WORK_ROOT/cafa6_data)\n",
    "PATH_IA = WORK_ROOT / 'IA.tsv'\n",
    "PATH_SAMPLE_SUB = WORK_ROOT / 'sample_submission.tsv'\n",
    "PATH_GO_OBO = WORK_ROOT / 'Train' / 'go-basic.obo'\n",
    "PATH_TRAIN_FASTA = WORK_ROOT / 'Train' / 'train_sequences.fasta'\n",
    "PATH_TRAIN_TERMS = WORK_ROOT / 'Train' / 'train_terms.tsv'\n",
    "PATH_TRAIN_TAXON = WORK_ROOT / 'Train' / 'train_taxonomy.tsv'\n",
    "PATH_TEST_FASTA = WORK_ROOT / 'Test' / 'testsuperset.fasta'\n",
    "PATH_TEST_TAXON = WORK_ROOT / 'Test' / 'testsuperset-taxon-list.tsv'\n",
    "print('DATASET_ROOT:', DATASET_ROOT.resolve())\n",
    "# Checkpoint store (Kaggle Dataset = single source of truth)\n",
    "# ------------------------------------------\n",
    "\n",
    "def _get_secret(name: str) -> str:\n",
    "    # Secrets policy:\n",
    "    # - In Colab: ONLY fetch secrets via google.colab.userdata.get(name)\n",
    "    # - Elsewhere: env var -> Kaggle Secrets\n",
    "    if IS_COLAB:\n",
    "        try:\n",
    "            from google.colab import userdata  # type: ignore\n",
    "            return (userdata.get(name) or '').strip()\n",
    "        except Exception:\n",
    "            return ''\n",
    "    v = (os.environ.get(name, '') or '').strip()\n",
    "    if v:\n",
    "        return v\n",
    "    if IS_KAGGLE:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient  # type: ignore\n",
    "            v = (UserSecretsClient().get_secret(name) or '').strip()\n",
    "            if v:\n",
    "                return v\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ''\n",
    "# Kaggle Secrets are NOT automatically environment variables.\n",
    "# No optional rebuild toggles in this notebook.\n",
    "CHECKPOINT_DATASET_ID = (\n",
    "    _get_secret('CAFA_CHECKPOINT_DATASET_ID')\n",
    "    or _get_secret('CAFA_KAGGLE_DATASET_ID')\n",
    ")\n",
    "CHECKPOINT_DATASET_TITLE = os.environ.get('CAFA_CHECKPOINT_DATASET_TITLE', 'CAFA6 Checkpoints').strip()\n",
    "CHECKPOINT_PULL = str(os.environ.get('CAFA_CHECKPOINT_PULL', '1' if IS_KAGGLE else '0')).strip().lower() in {'1', 'true', 'yes'}\n",
    "CHECKPOINT_PUSH = str(os.environ.get('CAFA_CHECKPOINT_PUSH', '0')).strip().lower() in {'1', 'true', 'yes'}\n",
    "PUSH_EXISTING_CHECKPOINTS = str(os.environ.get('CAFA_CHECKPOINT_PUSH_EXISTING', '0')).strip().lower() in {'1', 'true', 'yes'}\n",
    "MANIFEST_PATH = WORK_ROOT / 'manifest.json'\n",
    "\n",
    "def _get_kaggle_token() -> str:\n",
    "    return _get_secret('KAGGLE_API_TOKEN')\n",
    "\n",
    "def _get_kaggle_user_key() -> tuple[str, str]:\n",
    "    # Kaggle CLI expects Kaggle API credentials: username + key.\n",
    "    username = _get_secret('KAGGLE_USERNAME')\n",
    "    key = _get_secret('KAGGLE_KEY')\n",
    "    if username and key:\n",
    "        return username, key\n",
    "    # Back-compat: allow KAGGLE_API_TOKEN to carry either JSON ({username,key}) or 'username:key'.\n",
    "    tok = _get_kaggle_token()\n",
    "    if tok:\n",
    "        try:\n",
    "            obj = json.loads(tok)\n",
    "            username = (obj.get('username') or '').strip()\n",
    "            key = (obj.get('key') or '').strip()\n",
    "            if username and key:\n",
    "                return username, key\n",
    "        except Exception:\n",
    "            pass\n",
    "        if ':' in tok:\n",
    "            u, k = tok.split(':', 1)\n",
    "            username = u.strip()\n",
    "            key = k.strip()\n",
    "            if username and key:\n",
    "                return username, key\n",
    "    return '', ''\n",
    "\n",
    "def _kaggle_env(require: bool = False) -> dict[str, str]:\n",
    "    env = os.environ.copy()\n",
    "    username, key = _get_kaggle_user_key()\n",
    "    if username and key:\n",
    "        # Export into both subprocess env (this call) and process env (subsequent cells).\n",
    "        env['KAGGLE_USERNAME'] = username\n",
    "        env['KAGGLE_KEY'] = key\n",
    "        os.environ.setdefault('KAGGLE_USERNAME', username)\n",
    "        os.environ.setdefault('KAGGLE_KEY', key)\n",
    "    if require and (not env.get('KAGGLE_USERNAME') or not env.get('KAGGLE_KEY')):\n",
    "        raise RuntimeError(\n",
    "            'Kaggle API auth missing. The `kaggle` CLI requires `KAGGLE_USERNAME` + `KAGGLE_KEY` '\n",
    "            '(set them as env vars or Kaggle/Colab secrets). Or attach the '\n",
    "            'checkpoint dataset as an Input so `STORE.pull()` can use the mounted copy.'\n",
    "        )\n",
    "    return env\n",
    "\n",
    "def _ensure_kaggle_cli() -> None:\n",
    "    try:\n",
    "        subprocess.run(['kaggle', '--version'], check=True, capture_output=True, text=True)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install', 'kaggle'])\n",
    "        subprocess.run(['kaggle', '--version'], check=True)\n",
    "\n",
    "def _load_manifest() -> dict:\n",
    "    if MANIFEST_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(MANIFEST_PATH.read_text(encoding='utf-8'))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def _update_manifest(stage: str, required_paths: list[Path], note: str = '') -> None:\n",
    "    m = _load_manifest()\n",
    "    stages = m.get('stages', {})\n",
    "    files = []\n",
    "    for p in required_paths:\n",
    "        p = Path(p)\n",
    "        rel = str(p.relative_to(WORK_ROOT)) if str(p).startswith(str(WORK_ROOT)) else str(p)\n",
    "        files.append({'path': rel, 'bytes': int(p.stat().st_size) if p.exists() else None})\n",
    "    stages[stage] = {\n",
    "        'ts_utc': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "        'note': note,\n",
    "        'files': files,\n",
    "    }\n",
    "    m['stages'] = stages\n",
    "    MANIFEST_PATH.write_text(json.dumps(m, indent=2), encoding='utf-8')\n",
    "\n",
    "def _stage_files_signature(required_paths: list[Path]) -> list[dict]:\n",
    "    sig = []\n",
    "    for p in required_paths:\n",
    "        p = Path(p)\n",
    "        rel = str(p.relative_to(WORK_ROOT)) if str(p).startswith(str(WORK_ROOT)) else str(p)\n",
    "        sig.append({'path': rel, 'bytes': int(p.stat().st_size) if p.exists() else None})\n",
    "    return sorted(sig, key=lambda x: x['path'])\n",
    "\n",
    "def _ckpt_fname(rel: str) -> str:\n",
    "    # File-by-file checkpointing: keep the Kaggle dataset flat (no directories),\n",
    "    # but reconstruct the original relative paths under WORK_ROOT when pulling.\n",
    "    rel = str(rel).replace('\\\\', '/').lstrip('/')\n",
    "    return rel.replace('/', '__')\n",
    "\n",
    "@dataclass\n",
    "\n",
    "class KaggleCheckpointStore:\n",
    "    work_root: Path\n",
    "    dataset_id: str\n",
    "    dataset_title: str\n",
    "    pull_enabled: bool\n",
    "    push_enabled: bool\n",
    "    input_root: Path\n",
    "    is_kaggle: bool\n",
    "    manifest_name: str = 'manifest.json'\n",
    "    @property\n",
    "    def mount_dir(self) -> Path | None:\n",
    "        if not self.is_kaggle or not self.dataset_id:\n",
    "            return None\n",
    "        slug = self.dataset_id.split('/')[-1]\n",
    "        p = self.input_root / slug\n",
    "        return p if p.exists() else None\n",
    "    def pull(self, required_files: list[str] | None = None) -> None:\n",
    "        \"\"\"Pulls checkpoint artefacts into WORK_ROOT (cafa6_data).\n",
    "        Contract: required_files is mandatory for this notebook and is fetched file-by-file\n",
    "        (either from mounted Kaggle input, or via Kaggle API).\n",
    "        \"\"\"\n",
    "        if not self.pull_enabled:\n",
    "            return\n",
    "        checkpoint_required = True\n",
    "        required_files = list(required_files or [])\n",
    "        if not required_files:\n",
    "            raise ValueError('STORE.pull() requires required_files for this notebook.')\n",
    "        if not self.dataset_id:\n",
    "            raise ValueError('Missing CAFA_CHECKPOINT_DATASET_ID=<user>/<slug>; cannot resume.')\n",
    "        # Fast path: Kaggle-mounted dataset input\n",
    "        if self.mount_dir is not None:\n",
    "            print(f'Pulling checkpoints (file-by-file) from Kaggle mounted dataset: {self.mount_dir}')\n",
    "            for rel in required_files:\n",
    "                rel = str(rel).replace('\\\\', '/')\n",
    "                fname = _ckpt_fname(rel)\n",
    "                src1 = self.mount_dir / fname\n",
    "                src2 = self.mount_dir / rel\n",
    "                src = src1 if src1.exists() else src2\n",
    "                dst = self.work_root / rel\n",
    "                dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                if not src.exists():\n",
    "                    raise FileNotFoundError(f'Missing required checkpoint file in mounted dataset: {rel} (tried {src1} and {src2})')\n",
    "                shutil.copy2(src, dst)\n",
    "                _mark_downloaded(dst)\n",
    "            return\n",
    "        print(f'Downloading checkpoints (file-by-file) from Kaggle API: {self.dataset_id}')\n",
    "        _ensure_kaggle_cli()\n",
    "        env = _kaggle_env(require=checkpoint_required)\n",
    "        tmp = self.work_root / '_tmp_kaggle_download'\n",
    "        if tmp.exists():\n",
    "            shutil.rmtree(tmp)\n",
    "        tmp.mkdir(parents=True, exist_ok=True)\n",
    "        for rel in required_files:\n",
    "            rel = str(rel).replace('\\\\', '/')\n",
    "            fname = _ckpt_fname(rel)\n",
    "            tried = []\n",
    "            ok = False\n",
    "            for f in [fname, rel]:\n",
    "                cmd = ['kaggle', 'datasets', 'download', '-d', self.dataset_id, '-f', f, '-p', str(tmp)]\n",
    "                print('+', ' '.join(cmd))\n",
    "                p = subprocess.run(cmd, text=True, capture_output=True, env=env)\n",
    "                tried.append((f, p.returncode, (p.stdout or '')[:500], (p.stderr or '')[:500]))\n",
    "                if p.returncode == 0:\n",
    "                    ok = True\n",
    "                    break\n",
    "            if not ok:\n",
    "                raise RuntimeError(f'Checkpoint download failed for {rel}. Tried: {tried}')\n",
    "            # Extract any outer zips produced by Kaggle downloads.\n",
    "            for z in sorted(tmp.glob('*.zip')):\n",
    "                with zipfile.ZipFile(z, 'r') as zf:\n",
    "                    zf.extractall(tmp)\n",
    "                try:\n",
    "                    z.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            cand1 = tmp / fname\n",
    "            cand2 = tmp / Path(fname).name\n",
    "            cand3 = tmp / rel\n",
    "            src = cand1 if cand1.exists() else (cand3 if cand3.exists() else (cand2 if cand2.exists() else None))\n",
    "            if src is None:\n",
    "                raise FileNotFoundError(f'Downloaded checkpoint file not found after unzip: {rel} (expected {fname})')\n",
    "            dst = self.work_root / rel\n",
    "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(str(src), str(dst))\n",
    "            _mark_downloaded(dst)\n",
    "        shutil.rmtree(tmp)\n",
    "    def _can_publish(self, required_paths: list[Path]) -> tuple[bool, str]:\n",
    "        # 1) Must exist\n",
    "        missing = [Path(p) for p in required_paths if not Path(p).exists()]\n",
    "        if missing:\n",
    "            return False, 'missing required artefacts: ' + ', '.join([str(m) for m in missing])\n",
    "        # 2) Must not be downloaded/pulled\n",
    "        downloaded = [Path(p) for p in required_paths if Path(p).resolve() in DOWNLOADED_PATHS]\n",
    "        if downloaded:\n",
    "            return False, 'refusing to publish downloaded artefacts: ' + ', '.join([str(d) for d in downloaded])\n",
    "        # 3) Must be freshly built in this runtime\n",
    "        threshold = float(RUN_START_TS) - 1.0\n",
    "        not_fresh = [Path(p) for p in required_paths if Path(p).stat().st_mtime < threshold]\n",
    "        if not_fresh:\n",
    "            return False, 'refusing to publish non-fresh artefacts (not built this run): ' + ', '.join([str(n) for n in not_fresh])\n",
    "        return True, ''\n",
    "    def maybe_push(self, stage: str, required_paths: list[Path], note: str = '') -> None:\n",
    "        if not self.push_enabled:\n",
    "            return\n",
    "        ok, reason = self._can_publish(required_paths)\n",
    "        if not ok:\n",
    "            print(f'Skipping checkpoint publish for {stage}: {reason}')\n",
    "            return\n",
    "        self.push(stage, required_paths, note=note)\n",
    "    def push(self, stage: str, required_paths: list[Path], note: str = '') -> None:\n",
    "        if not self.push_enabled:\n",
    "            return\n",
    "        if not self.dataset_id:\n",
    "            raise ValueError('Missing CAFA_CHECKPOINT_DATASET_ID=<user>/<slug>; cannot checkpoint.')\n",
    "        ok, reason = self._can_publish(required_paths)\n",
    "        if not ok:\n",
    "            raise RuntimeError(f'Checkpoint publish blocked for {stage}: {reason}')\n",
    "        # Optional skip: identical stage signature\n",
    "        m = _load_manifest()\n",
    "        existing = (m.get('stages', {}) or {}).get(stage) if isinstance(m, dict) else None\n",
    "        if isinstance(existing, dict):\n",
    "            prev_files = existing.get('files', [])\n",
    "            if isinstance(prev_files, list):\n",
    "                prev_sig = sorted([{'path': f.get('path'), 'bytes': f.get('bytes')} for f in prev_files if isinstance(f, dict)], key=lambda x: str(x.get('path')))\n",
    "                cur_sig = _stage_files_signature(required_paths)\n",
    "                if prev_sig == cur_sig:\n",
    "                    print(f'Checkpoint stage {stage} unchanged; skipping publish')\n",
    "                    return\n",
    "        _update_manifest(stage, required_paths, note=note)\n",
    "        # Publish ONLY what we just built: stage to a clean directory\n",
    "        publish_root = self.work_root / '_publish_tmp' / stage\n",
    "        if publish_root.exists():\n",
    "            shutil.rmtree(publish_root)\n",
    "        publish_root.mkdir(parents=True, exist_ok=True)\n",
    "        work_root_resolved = self.work_root.resolve()\n",
    "        for p in required_paths:\n",
    "            p = Path(p).resolve()\n",
    "            try:\n",
    "                rel = p.relative_to(work_root_resolved)\n",
    "            except Exception:\n",
    "                raise ValueError(f'All checkpoint artefacts must live under WORK_ROOT. Got: {p}')\n",
    "            # Publish file-by-file (flat dataset; no directory zips).\n",
    "            dst = publish_root / _ckpt_fname(rel.as_posix())\n",
    "            shutil.copy2(p, dst)\n",
    "        # Include the manifest (freshly written) in the publication\n",
    "        manifest_path = MANIFEST_PATH\n",
    "        if manifest_path.exists():\n",
    "            shutil.copy2(manifest_path, publish_root / self.manifest_name)\n",
    "        (publish_root / 'dataset-metadata.json').write_text(\n",
    "            json.dumps({'title': self.dataset_title, 'id': self.dataset_id, 'licenses': [{'name': 'CC0-1.0'}]}, indent=2),\n",
    "            encoding='utf-8',\n",
    "        )\n",
    "        (publish_root / 'README.md').write_text(\n",
    "            f\"# {self.dataset_title}\\n\"\n",
    "            \"Auto-published checkpoint dataset for CAFA6.\\n\"\n",
    "            f\"Latest stage: {stage}\\n\",\n",
    "            encoding='utf-8',\n",
    "        )\n",
    "        _ensure_kaggle_cli()\n",
    "        env = _kaggle_env(require=True)\n",
    "        msg = f'{stage}: {note}'.strip() if note else stage\n",
    "        # publish_root contains only files at top-level; no --dir-mode required (avoids zipped folders).\n",
    "        p = subprocess.run(['kaggle', 'datasets', 'version', '-p', str(publish_root), '-m', msg], text=True, capture_output=True, env=env)\n",
    "        if p.returncode != 0:\n",
    "            p2 = subprocess.run(['kaggle', 'datasets', 'create', '-p', str(publish_root)], text=True, capture_output=True, env=env)\n",
    "            if p2.returncode != 0:\n",
    "                print(p.stdout); print(p.stderr); print(p2.stdout); print(p2.stderr)\n",
    "                raise RuntimeError('Kaggle dataset publish failed. See logs above.')\n",
    "            print(p2.stdout); print(p2.stderr)\n",
    "        else:\n",
    "            print(p.stdout); print(p.stderr)\n",
    "            print('Published new checkpoint dataset version:', self.dataset_id)\n",
    "# Checkpoint artefacts required for this notebook (MANDATORY, pulled file-by-file)\n",
    "# We use stage-based required sets so you can resume from a specific milestone.\n",
    "# Select via CAFA_CHECKPOINT_PULL_STAGE (only used when CAFA_CHECKPOINT_PULL=1).\n",
    "#\n",
    "# Stages are cumulative by design (i.e. later stages include their prerequisites).\n",
    "# This avoids half-hydrated states when you resume from later cells.\n",
    "has_test = bool(PATH_TEST_FASTA.exists())\n",
    "STAGE_REQUIRED_FILES: dict[str, list[str]] = {}\n",
    "# Stage 01: parsed core\n",
    "STAGE_REQUIRED_FILES['stage_01_parsed'] = [\n",
    "    'parsed/train_seq.feather',\n",
    "    'parsed/train_terms.parquet',\n",
    "    'parsed/train_taxa.feather',\n",
    "    'parsed/term_counts.parquet',\n",
    "    'parsed/term_priors.parquet',\n",
    "] + ([\n",
    "    'parsed/test_seq.feather',\n",
    "    'parsed/test_taxa.feather',\n",
    "] if has_test else [])\n",
    "# Stage 02: external text corpus\n",
    "STAGE_REQUIRED_FILES['stage_02_entryid_text'] = STAGE_REQUIRED_FILES['stage_01_parsed'] + [\n",
    "    'external/entryid_text.tsv',\n",
    "]\n",
    "# Stage 03: TF-IDF text embeddings\n",
    "STAGE_REQUIRED_FILES['stage_03_tfidf_text'] = STAGE_REQUIRED_FILES['stage_02_entryid_text'] + [\n",
    "    'features/text_vectorizer.joblib',\n",
    "    'features/train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy',\n",
    "]\n",
    "# Stage 04: external propagated GOA priors (IEA)\n",
    "STAGE_REQUIRED_FILES['stage_04_external_goa_priors'] = STAGE_REQUIRED_FILES['stage_03_tfidf_text'] + [\n",
    "    'external/prop_train_no_kaggle.tsv.gz',\n",
    "    'external/prop_test_no_kaggle.tsv.gz',\n",
    "]\n",
    "# Stage 06: sequence embeddings (core modalities)\n",
    "STAGE_REQUIRED_FILES['stage_06_embeddings_core'] = STAGE_REQUIRED_FILES['stage_04_external_goa_priors'] + [\n",
    "    'features/train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy',\n",
    "    'features/train_embeds_ankh.npy',\n",
    "    'features/test_embeds_ankh.npy',\n",
    "]\n",
    "# Stage 07: Level-1 OOF + test predictions (enables going straight to stacker)\n",
    "STAGE_REQUIRED_FILES['stage_07_level1_preds'] = STAGE_REQUIRED_FILES['stage_04_external_goa_priors'] + [\n",
    "    'features/top_terms_13500.json',\n",
    "    'features/level1_preds/oof_pred_logreg.npy',\n",
    "    'features/level1_preds/test_pred_logreg.npy',\n",
    "    'features/level1_preds/oof_pred_gbdt.npy',\n",
    "    'features/level1_preds/test_pred_gbdt.npy',\n",
    "    'features/level1_preds/oof_pred_dnn.npy',\n",
    "    'features/level1_preds/test_pred_dnn.npy',\n",
    "    'features/level1_preds/oof_pred_knn.npy',\n",
    "    'features/level1_preds/test_pred_knn.npy',\n",
    "]\n",
    "# Stage 08: hierarchy-aware stacker output (enables going straight to submission)\n",
    "STAGE_REQUIRED_FILES['stage_08_stacker_gcn'] = STAGE_REQUIRED_FILES['stage_07_level1_preds'] + [\n",
    "    'features/test_pred_gcn.npy',\n",
    "]\n",
    "# Stage 09: final submission (rarely needed for pull, but included for completeness)\n",
    "STAGE_REQUIRED_FILES['stage_09_submission'] = STAGE_REQUIRED_FILES['stage_08_stacker_gcn'] + [\n",
    "    'features/test_pred_gcn.npy',\n",
    "    'submission.tsv',\n",
    "]\n",
    "def _select_required_checkpoint_files() -> list[str]:\n",
    "    if not CHECKPOINT_PULL:\n",
    "        return []\n",
    "    stage = os.getenv('CAFA_CHECKPOINT_PULL_STAGE', '').strip()\n",
    "    if not stage:\n",
    "        # Keep defaults conservative: Kaggle usually wants late-stage resume; local wants minimal or none.\n",
    "        stage = 'stage_07_level1_preds' if IS_KAGGLE else 'stage_01_parsed'\n",
    "    if stage.lower() in {'off', 'none', 'no', '0'}:\n",
    "        return []\n",
    "    if stage not in STAGE_REQUIRED_FILES:\n",
    "        raise ValueError(f'Unknown CAFA_CHECKPOINT_PULL_STAGE={stage!r}. Known: {sorted(STAGE_REQUIRED_FILES)}')\n",
    "    return list(STAGE_REQUIRED_FILES[stage])\n",
    "REQUIRED_CHECKPOINT_FILES = _select_required_checkpoint_files()\n",
    "STORE = KaggleCheckpointStore(\n",
    "    work_root=WORK_ROOT,\n",
    "    dataset_id=CHECKPOINT_DATASET_ID,\n",
    "    dataset_title=CHECKPOINT_DATASET_TITLE,\n",
    "    pull_enabled=CHECKPOINT_PULL,\n",
    "    push_enabled=CHECKPOINT_PUSH,\n",
    "    input_root=KAGGLE_INPUT_ROOT,\n",
    "    is_kaggle=IS_KAGGLE,\n",
    ")\n",
    "# Pull once at startup (fresh runtimes resume here)\n",
    "STORE.pull(required_files=REQUIRED_CHECKPOINT_FILES)\n",
    "# Post-pull diagnostics: make it obvious whether artefacts are present.\n",
    "\n",
    "def _p(path: Path) -> str:\n",
    "    return str(path)\n",
    "\n",
    "def _exists_bytes(path: Path) -> str:\n",
    "    if not path.exists():\n",
    "        return 'MISSING'\n",
    "    try:\n",
    "        return f'OK ({path.stat().st_size / (1024**2):.1f} MB)'\n",
    "    except Exception:\n",
    "        return 'OK'\n",
    "print('Checkpoint status (after pull):')\n",
    "print('  WORK_ROOT:', _p(WORK_ROOT))\n",
    "print('  parsed/:', _exists_bytes(WORK_ROOT / 'parsed'))\n",
    "print('  external/:', _exists_bytes(WORK_ROOT / 'external'))\n",
    "print('  features/:', _exists_bytes(WORK_ROOT / 'features'))\n",
    "print('  external/entryid_text.tsv:', _exists_bytes(WORK_ROOT / 'external' / 'entryid_text.tsv'))\n",
    "print('  parsed/train_seq.feather:', _exists_bytes(WORK_ROOT / 'parsed' / 'train_seq.feather'))\n",
    "\n",
    "def stage_present(required_paths: list[Path]) -> bool:\n",
    "    return all(Path(p).exists() for p in required_paths)\n",
    "# ------------------------------------------\n",
    "# Initial Diagnostics (Sequence Lengths)\n",
    "# ------------------------------------------\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "def read_fasta_lengths(path: Path, max_records=20000):\n",
    "    lengths = []\n",
    "    current = 0\n",
    "    n = 0\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if n > 0:\n",
    "                    lengths.append(current)\n",
    "                n += 1\n",
    "                current = 0\n",
    "                if max_records and n > max_records:\n",
    "                    break\n",
    "            else:\n",
    "                current += len(line)\n",
    "        if n > 0:\n",
    "            lengths.append(current)\n",
    "    return np.array(lengths)\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(read_fasta_lengths(PATH_TRAIN_FASTA), bins=50, alpha=0.5, label='Train')\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    plt.hist(read_fasta_lengths(PATH_TEST_FASTA), bins=50, alpha=0.5, label='Test')\n",
    "plt.title('Sequence Length Distribution (First 20k)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b013607",
   "metadata": {
    "id": "3b013607",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing OBO...\n",
      "GO Graph: 40119 nodes with parents, 48101 terms with namespace.\n",
      "Parsing FASTA...\n",
      "FASTA parsed and saved to artefacts.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms processed and priors saved.\n",
      "Processing Taxonomy...\n",
      "Taxonomy processed. Train: 82404, Test: 224309\n",
      "Saving Targets & Term List...\n",
      "Targets saved.\n"
     ]
    }
   ],
   "source": [
    "# CELL 04 - Solution: 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "# ------------------------------------------\n",
    "# B. Parse OBO & Terms (needed in-memory downstream)\n",
    "# ------------------------------------------\n",
    "\n",
    "def parse_obo(path: Path):\n",
    "    parents = {}\n",
    "    namespaces = {}\n",
    "    cur_id, cur_ns = None, None\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '[Term]':\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "                cur_id, cur_ns = None, None\n",
    "            elif line.startswith('id: GO:'):\n",
    "                cur_id = line.split('id: ', 1)[1]\n",
    "            elif line.startswith('namespace:'):\n",
    "                cur_ns = line.split('namespace: ', 1)[1]\n",
    "            elif line.startswith('is_a:') and cur_id:\n",
    "                parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                parents.setdefault(cur_id, set()).add(parent)\n",
    "        if cur_id and cur_ns:\n",
    "            namespaces[cur_id] = cur_ns\n",
    "    return parents, namespaces\n",
    "print(\"Parsing OBO...\")\n",
    "go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "print(f\"GO Graph: {len(go_parents)} nodes with parents, {len(go_namespaces)} terms with namespace.\")\n",
    "# ------------------------------------------\n",
    "# Milestone checkpoint: stage_01_parsed\n",
    "# ------------------------------------------\n",
    "parsed_dir = WORK_ROOT / 'parsed'\n",
    "parsed_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_train_seq = parsed_dir / 'train_seq.feather'\n",
    "out_test_seq = parsed_dir / 'test_seq.feather'\n",
    "out_train_terms = parsed_dir / 'train_terms.parquet'\n",
    "out_term_counts = parsed_dir / 'term_counts.parquet'\n",
    "out_term_priors = parsed_dir / 'term_priors.parquet'\n",
    "out_train_taxa = parsed_dir / 'train_taxa.feather'\n",
    "out_test_taxa = parsed_dir / 'test_taxa.feather'\n",
    "expected = [out_train_seq, out_train_terms, out_term_counts, out_term_priors, out_train_taxa]\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    expected += [out_test_seq, out_test_taxa]\n",
    "missing = [p for p in expected if not p.exists()]\n",
    "if not missing:\n",
    "    print(\"Parsed artefacts already exist; skipping Phase 1 writes.\")\n",
    "else:\n",
    "    # ------------------------------------------\n",
    "    # A. Parse FASTA to Feather\n",
    "    # ------------------------------------------\n",
    "    def parse_fasta(path: Path) -> pd.DataFrame:\n",
    "        ids, seqs = [], []\n",
    "        cur_id, cur_seq = None, []\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    if cur_id:\n",
    "                        ids.append(cur_id)\n",
    "                        seqs.append(''.join(cur_seq))\n",
    "                    cur_id = line[1:].split()[0]\n",
    "                    cur_seq = []\n",
    "                else:\n",
    "                    cur_seq.append(line)\n",
    "            if cur_id:\n",
    "                ids.append(cur_id)\n",
    "                seqs.append(''.join(cur_seq))\n",
    "        return pd.DataFrame({'id': ids, 'sequence': seqs})\n",
    "    print(\"Parsing FASTA...\")\n",
    "    parse_fasta(PATH_TRAIN_FASTA).to_feather(out_train_seq)\n",
    "    if PATH_TEST_FASTA.exists():\n",
    "        parse_fasta(PATH_TEST_FASTA).to_feather(out_test_seq)\n",
    "    print(\"FASTA parsed and saved to artefacts.\")\n",
    "    # ------------------------------------------\n",
    "    # C. Process Terms & Priors\n",
    "    # ------------------------------------------\n",
    "    terms = pd.read_csv(PATH_TRAIN_TERMS, sep='\\t')\n",
    "    col_term = terms.columns[1]\n",
    "    terms['aspect'] = terms[col_term].map(lambda x: go_namespaces.get(x, 'UNK'))\n",
    "    # Plot Aspects\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    terms['aspect'].value_counts().plot(kind='bar', title='Annotations by Namespace')\n",
    "    plt.show()\n",
    "    # Save Priors\n",
    "    priors = (terms[col_term].value_counts() / terms.iloc[:, 0].nunique()).reset_index()\n",
    "    priors.columns = ['term', 'prior']\n",
    "    if PATH_IA.exists():\n",
    "        ia = pd.read_csv(PATH_IA, sep='\\t', names=['term', 'ia'])\n",
    "        priors = priors.merge(ia, on='term', how='left').fillna(0)\n",
    "    priors.to_parquet(out_term_priors)\n",
    "    print(\"Terms processed and priors saved.\")\n",
    "    # ------------------------------------------\n",
    "    # D. Process Taxonomy\n",
    "    # ------------------------------------------\n",
    "    print(\"Processing Taxonomy...\")\n",
    "    # Train Taxonomy\n",
    "    tax_train = pd.read_csv(PATH_TRAIN_TAXON, sep='\\t', header=None, names=['id', 'taxon_id'])\n",
    "    tax_train['taxon_id'] = tax_train['taxon_id'].astype(int)\n",
    "    tax_train.to_feather(out_train_taxa)\n",
    "    # Test Taxonomy (Extract from FASTA headers)\n",
    "    if PATH_TEST_FASTA.exists():\n",
    "        ids, taxons = [], []\n",
    "        with PATH_TEST_FASTA.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    parts = line[1:].split()\n",
    "                    ids.append(parts[0])\n",
    "                    # Assume second part is taxon if present\n",
    "                    if len(parts) > 1:\n",
    "                        try:\n",
    "                            taxons.append(int(parts[1]))\n",
    "                        except ValueError:\n",
    "                            taxons.append(0)\n",
    "                    else:\n",
    "                        taxons.append(0)\n",
    "        tax_test = pd.DataFrame({'id': ids, 'taxon_id': taxons})\n",
    "        tax_test.to_feather(out_test_taxa)\n",
    "        print(f\"Taxonomy processed. Train: {len(tax_train)}, Test: {len(tax_test)}\")\n",
    "    else:\n",
    "        print(f\"Taxonomy processed. Train: {len(tax_train)}\")\n",
    "    # ------------------------------------------\n",
    "    # E. Save Targets & Term List\n",
    "    # ------------------------------------------\n",
    "    print(\"Saving Targets & Term List...\")\n",
    "    # Save full terms list (long format)\n",
    "    terms.to_parquet(out_train_terms)\n",
    "    # Save unique term list with counts\n",
    "    term_counts = terms['term'].value_counts().reset_index()\n",
    "    term_counts.columns = ['term', 'count']\n",
    "    term_counts.to_parquet(out_term_counts)\n",
    "    print(\"Targets saved.\")\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.maybe_push('stage_01_parsed', [p for p in expected if p.exists()], note='parsed FASTA/taxa/terms/priors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9437b",
   "metadata": {
    "id": "inline_corpus_script",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 05 - Inline: EntryID->text corpus builder\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _norm_uniprot_accession(raw_id: str) -> str:\n",
    "    s = str(raw_id).strip()\n",
    "    parts = s.split(\"|\")\n",
    "    # Common UniProt FASTA header: sp|P12345|NAME_HUMAN\n",
    "    if len(parts) >= 2 and parts[0] in {\"sp\", \"tr\"}:\n",
    "        return parts[1]\n",
    "    if len(parts) >= 3:\n",
    "        return parts[1]\n",
    "    return s\n",
    "def _detect_id_col(df: pd.DataFrame) -> str:\n",
    "    for cand in [\"id\", \"EntryID\", \"entry_id\", \"accession\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    return df.columns[0]\n",
    "def _read_ids(feather_path: Path) -> list[str]:\n",
    "    df = pd.read_feather(feather_path)\n",
    "    col = _detect_id_col(df)\n",
    "    return df[col].astype(str).tolist()\n",
    "def _strip_go_leakage(text: str) -> str:\n",
    "    # Remove explicit GO identifiers and obvious \"GO:\" tokens.\n",
    "    text = re.sub(r\"\\bGO:\\d{7}\\b\", \" \", text)\n",
    "    text = text.replace(\"GO:\", \" \")\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "def _make_session(total_retries: int = 5) -> requests.Session:\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    from urllib3.util.retry import Retry\n",
    "    retry = Retry(\n",
    "        total=total_retries,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    s = requests.Session()\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    return s\n",
    "@dataclass\n",
    "\n",
    "class UniProtRow:\n",
    "    accession: str\n",
    "    protein_name: str\n",
    "    organism_name: str\n",
    "    keywords: str\n",
    "    protein_families: str\n",
    "    cc_function: str\n",
    "    cc_subcellular_location: str\n",
    "    pubmed_ids: list[str]\n",
    "def fetch_uniprot_tsv(\n",
    "    session: requests.Session,\n",
    "    accessions: list[str],\n",
    "    sleep_s: float,\n",
    "    fields: str,\n",
    ") -> dict[str, UniProtRow]:\n",
    "    \"\"\"Fetch UniProt rows for a small batch of accessions using the search endpoint (TSV).\"\"\"\n",
    "    # Query is a disjunction of accessions.\n",
    "    # Keep the batch modest to avoid URL length limits.\n",
    "    query = \" OR \".join([f\"accession:{a}\" for a in accessions])\n",
    "    url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "    params = {\n",
    "        \"query\": f\"({query})\",\n",
    "        \"format\": \"tsv\",\n",
    "        \"fields\": fields,\n",
    "        \"size\": 500,\n",
    "    }\n",
    "    resp = session.get(url, params=params, timeout=60)\n",
    "    # UniProt will return HTTP 400 for overly long/complex queries (often URL-length related).\n",
    "    # Fall back to a recursive split so large runs complete reliably.\n",
    "    if resp.status_code in {400, 414} and len(accessions) > 1:\n",
    "        mid = len(accessions) // 2\n",
    "        left = fetch_uniprot_tsv(session, accessions[:mid], sleep_s=sleep_s, fields=fields)\n",
    "        right = fetch_uniprot_tsv(session, accessions[mid:], sleep_s=sleep_s, fields=fields)\n",
    "        left.update(right)\n",
    "        return left\n",
    "    try:\n",
    "        resp.raise_for_status()\n",
    "    except requests.HTTPError as e:\n",
    "        detail = (resp.text or '')[:1000] if resp is not None else ''\n",
    "        raise requests.HTTPError(f'{e} :: UniProt response: {detail}') from e\n",
    "    if sleep_s:\n",
    "        time.sleep(sleep_s)\n",
    "    lines = resp.text.splitlines()\n",
    "    if not lines:\n",
    "        return {}\n",
    "    reader = csv.DictReader(lines, delimiter=\"\\t\")\n",
    "    out: dict[str, UniProtRow] = {}\n",
    "    for row in reader:\n",
    "        acc = (row.get(\"Entry\") or row.get(\"accession\") or \"\").strip()\n",
    "        if not acc:\n",
    "            continue\n",
    "        def _get(k: str) -> str:\n",
    "            v = row.get(k)\n",
    "            return str(v).strip() if v is not None else \"\"\n",
    "        pmids_raw = (_get(\"PubMed ID\") or _get(\"lit_pubmed_id\")).strip()\n",
    "        pmids: list[str] = []\n",
    "        if pmids_raw:\n",
    "            pmids = [p.strip() for p in re.split(r\"[;\\s]+\", pmids_raw) if p.strip().isdigit()]\n",
    "        out[acc] = UniProtRow(\n",
    "            accession=acc,\n",
    "            protein_name=_get(\"Protein names\"),\n",
    "            organism_name=_get(\"Organism\"),\n",
    "            keywords=_get(\"Keywords\"),\n",
    "            protein_families=_get(\"Protein families\"),\n",
    "            cc_function=_get(\"Function [CC]\"),\n",
    "            cc_subcellular_location=_get(\"Subcellular location [CC]\"),\n",
    "            pubmed_ids=pmids,\n",
    "        )\n",
    "    return out\n",
    "def fetch_pubmed_abstracts(\n",
    "    session: requests.Session,\n",
    "    pmids: list[str],\n",
    "    sleep_s: float,\n",
    "    email: str,\n",
    "    api_key: str,\n",
    "    _depth: int = 0,\n",
    ") -> dict[str, tuple[str, str]]:\n",
    "    \"\"\"Fetch PubMed title+abstract via NCBI E-utilities (XML).\"\"\"\n",
    "    if not pmids:\n",
    "        return {}\n",
    "    pmids = [str(p).strip() for p in pmids if str(p).strip().isdigit()]\n",
    "    if not pmids:\n",
    "        return {}\n",
    "    pmids = list(dict.fromkeys(pmids))\n",
    "    def _looks_like_html(text: str) -> bool:\n",
    "        t = (text or \"\").lstrip().lower()\n",
    "        return t.startswith(\"<!doctype html\") or t.startswith(\"<html\") or \"<html\" in t[:2000]\n",
    "    def _sanitize_xml(text: str) -> str:\n",
    "        # Remove illegal XML control chars (keep \\t \\n \\r).\n",
    "        return re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\", \" \", text or \"\")\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"id\": \",\".join(pmids),\n",
    "        \"retmode\": \"xml\",\n",
    "    }\n",
    "    if email:\n",
    "        params[\"email\"] = email\n",
    "    if api_key:\n",
    "        params[\"api_key\"] = api_key\n",
    "    root = None\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = session.get(url, params=params, timeout=60)\n",
    "            if resp.status_code in {429, 500, 502, 503, 504}:\n",
    "                raise requests.HTTPError(f\"PubMed transient HTTP {resp.status_code}\")\n",
    "            resp.raise_for_status()\n",
    "            if sleep_s:\n",
    "                time.sleep(sleep_s)\n",
    "            text = resp.text or \"\"\n",
    "            if _looks_like_html(text):\n",
    "                snippet = text[:300].replace(\"\\n\", \" \" )\n",
    "                raise ValueError(f\"PubMed returned HTML instead of XML: {snippet}\")\n",
    "            text = _sanitize_xml(text)\n",
    "            root = ET.fromstring(text)\n",
    "            break\n",
    "        except Exception:\n",
    "            if attempt < 2:\n",
    "                time.sleep(1.5 * (attempt + 1))\n",
    "            continue\n",
    "    if root is None:\n",
    "        if len(pmids) > 1 and _depth < 10:\n",
    "            mid = len(pmids) // 2\n",
    "            left = fetch_pubmed_abstracts(session, pmids[:mid], sleep_s=sleep_s, email=email, api_key=api_key, _depth=_depth + 1)\n",
    "            right = fetch_pubmed_abstracts(session, pmids[mid:], sleep_s=sleep_s, email=email, api_key=api_key, _depth=_depth + 1)\n",
    "            left.update(right)\n",
    "            return left\n",
    "        return {}\n",
    "    out: dict[str, tuple[str, str]] = {}\n",
    "    for article in root.findall(\".//PubmedArticle\"):\n",
    "        pmid_el = article.find(\".//PMID\")\n",
    "        if pmid_el is None or pmid_el.text is None:\n",
    "            continue\n",
    "        pmid = pmid_el.text.strip()\n",
    "        title_el = article.find(\".//ArticleTitle\")\n",
    "        abs_el = article.find(\".//Abstract\")\n",
    "        title = (\"\" if title_el is None or title_el.text is None else title_el.text.strip())\n",
    "        abstract = \"\"\n",
    "        if abs_el is not None:\n",
    "            parts = []\n",
    "            for t in abs_el.findall(\".//AbstractText\"):\n",
    "                if t.text:\n",
    "                    parts.append(t.text.strip())\n",
    "            abstract = \" \".join(parts).strip()\n",
    "        out[pmid] = (title, abstract)\n",
    "    return out\n",
    "def main() -> int:\n",
    "    p = argparse.ArgumentParser(description=\"Build EntryID->text corpus from UniProt (fields) + PubMed abstracts for --mode text.\")\n",
    "    p.add_argument(\"--artefacts-dir\", default=Path(\"artefacts_local\") / \"artefacts\", type=Path, help=\"Artefacts root\")\n",
    "    p.add_argument(\"--out-path\", default=Path(\"artefacts_local\") / \"artefacts\" / \"external\" / \"entryid_text.tsv\", type=Path, help=\"Output TSV path (default: artefacts_local/artefacts/external/entryid_text.tsv)\")\n",
    "    p.add_argument(\"--cache-dir\", default=Path(\"artefacts_local\") / \"artefacts\" / \"external\" / \"uniprot_pubmed_cache\", type=Path, help=\"Cache dir for UniProt+PubMed lookups\")\n",
    "    p.add_argument(\"--max-ids\", type=int, default=0, help=\"If >0, cap number of proteins (debug)\")\n",
    "    p.add_argument(\"--uniprot-batch-size\", type=int, default=80)\n",
    "    p.add_argument(\"--pubmed-batch-size\", type=int, default=50)\n",
    "    p.add_argument(\"--max-pubmed-per-protein\", type=int, default=3)\n",
    "    p.add_argument(\"--max-abstract-chars\", type=int, default=2000)\n",
    "    p.add_argument(\"--sleep-uniprot\", type=float, default=0.1)\n",
    "    p.add_argument(\"--sleep-pubmed\", type=float, default=0.34)\n",
    "    p.add_argument(\"--email\", type=str, default=os.environ.get(\"NCBI_EMAIL\", \"\"))\n",
    "    p.add_argument(\"--api-key\", type=str, default=os.environ.get(\"NCBI_API_KEY\", \"\"))\n",
    "    p.add_argument(\"--strip-go\", action=\"store_true\")\n",
    "    args = p.parse_args()\n",
    "    args.artefacts_dir = Path(args.artefacts_dir)\n",
    "    args.out_path = Path(args.out_path)\n",
    "    args.cache_dir = Path(args.cache_dir)\n",
    "    args.out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    args.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_feather = args.artefacts_dir / \"parsed\" / \"train_seq.feather\"\n",
    "    test_feather = args.artefacts_dir / \"parsed\" / \"test_seq.feather\"\n",
    "    ids = []\n",
    "    if train_feather.exists():\n",
    "        ids.extend(_read_ids(train_feather))\n",
    "    if test_feather.exists():\n",
    "        ids.extend(_read_ids(test_feather))\n",
    "    ids = [_norm_uniprot_accession(i) for i in ids]\n",
    "    uniq = sorted(set(ids))\n",
    "    if args.max_ids and args.max_ids > 0:\n",
    "        uniq = uniq[: int(args.max_ids)]\n",
    "    print(f\"Unique accessions: {len(uniq)}\")\n",
    "    fields = \",\".join(\n",
    "        [\n",
    "            \"accession\",\n",
    "            \"protein_name\",\n",
    "            \"organism_name\",\n",
    "            \"keyword\",\n",
    "            \"protein_families\",\n",
    "            \"cc_function\",\n",
    "            \"cc_subcellular_location\",\n",
    "            \"lit_pubmed_id\",\n",
    "        ]\n",
    "    )\n",
    "    session = _make_session()\n",
    "    # UniProt rows cache\n",
    "    uniprot_cache = args.cache_dir / \"uniprot_rows.tsv\"\n",
    "    uniprot_rows: dict[str, UniProtRow] = {}\n",
    "    if uniprot_cache.exists():\n",
    "        print(f\"Loading UniProt cache: {uniprot_cache}\")\n",
    "        df_u = pd.read_csv(uniprot_cache, sep=\"\\t\", dtype=str)\n",
    "        for _, r in df_u.iterrows():\n",
    "            acc = str(r[\"accession\"])\n",
    "            pmids = [p for p in str(r.get(\"pubmed_ids\", \"\")).split(\";\") if p]\n",
    "            uniprot_rows[acc] = UniProtRow(\n",
    "                accession=acc,\n",
    "                protein_name=str(r.get(\"protein_name\", \"\")) if r.get(\"protein_name\") is not None else \"\",\n",
    "                organism_name=str(r.get(\"organism_name\", \"\")) if r.get(\"organism_name\") is not None else \"\",\n",
    "                keywords=str(r.get(\"keywords\", \"\")) if r.get(\"keywords\") is not None else \"\",\n",
    "                protein_families=str(r.get(\"protein_families\", \"\")) if r.get(\"protein_families\") is not None else \"\",\n",
    "                cc_function=str(r.get(\"cc_function\", \"\")) if r.get(\"cc_function\") is not None else \"\",\n",
    "                cc_subcellular_location=str(r.get(\"cc_subcellular_location\", \"\")) if r.get(\"cc_subcellular_location\") is not None else \"\",\n",
    "                pubmed_ids=pmids,\n",
    "            )\n",
    "    missing = [a for a in uniq if a not in uniprot_rows]\n",
    "    if missing:\n",
    "        print(f\"Fetching UniProt rows (missing={len(missing)})...\")\n",
    "        write_header = not uniprot_cache.exists()\n",
    "        with uniprot_cache.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(\n",
    "                f,\n",
    "                delimiter=\"\\t\",\n",
    "                fieldnames=[\n",
    "                    \"accession\",\n",
    "                    \"protein_name\",\n",
    "                    \"organism_name\",\n",
    "                    \"keywords\",\n",
    "                    \"protein_families\",\n",
    "                    \"cc_function\",\n",
    "                    \"cc_subcellular_location\",\n",
    "                    \"pubmed_ids\",\n",
    "                ],\n",
    "            )\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            for i in tqdm(range(0, len(missing), args.uniprot_batch_size), desc=\"UniProt\"):\n",
    "                batch = missing[i : i + args.uniprot_batch_size]\n",
    "                got = fetch_uniprot_tsv(\n",
    "                    session=session,\n",
    "                    accessions=batch,\n",
    "                    sleep_s=args.sleep_uniprot,\n",
    "                    fields=fields,\n",
    "                )\n",
    "                for acc, r in got.items():\n",
    "                    uniprot_rows[acc] = r\n",
    "                    writer.writerow(\n",
    "                        {\n",
    "                            \"accession\": r.accession,\n",
    "                            \"protein_name\": r.protein_name,\n",
    "                            \"organism_name\": r.organism_name,\n",
    "                            \"keywords\": r.keywords,\n",
    "                            \"protein_families\": r.protein_families,\n",
    "                            \"cc_function\": r.cc_function,\n",
    "                            \"cc_subcellular_location\": r.cc_subcellular_location,\n",
    "                            \"pubmed_ids\": \";\".join(r.pubmed_ids),\n",
    "                        }\n",
    "                    )\n",
    "    # PubMed cache\n",
    "    pubmed_cache = args.cache_dir / \"pubmed_abstracts.tsv\"\n",
    "    pmid_sets: dict[str, list[str]] = {}\n",
    "    for acc, r in uniprot_rows.items():\n",
    "        pmids = [p for p in r.pubmed_ids if p]\n",
    "        if args.max_pubmed_per_protein and len(pmids) > args.max_pubmed_per_protein:\n",
    "            pmids = pmids[: int(args.max_pubmed_per_protein)]\n",
    "        pmid_sets[acc] = pmids\n",
    "    pmids_uniq = sorted({p for ps in pmid_sets.values() for p in ps})\n",
    "    pubmed_map: dict[str, tuple[str, str]] = {}\n",
    "    if pubmed_cache.exists():\n",
    "        print(f\"Loading PubMed cache: {pubmed_cache}\")\n",
    "        df_p = pd.read_csv(pubmed_cache, sep=\"\\t\", dtype=str)\n",
    "        for _, r in df_p.iterrows():\n",
    "            pmid = str(r[\"pmid\"])\n",
    "            pubmed_map[pmid] = (\n",
    "                str(r.get(\"title\", \"\")) if r.get(\"title\") is not None else \"\",\n",
    "                str(r.get(\"abstract\", \"\")) if r.get(\"abstract\") is not None else \"\",\n",
    "            )\n",
    "    missing_pmids = [p for p in pmids_uniq if p not in pubmed_map]\n",
    "    if missing_pmids:\n",
    "        print(f\"Fetching PubMed abstracts (missing={len(missing_pmids)})...\")\n",
    "        write_header = not pubmed_cache.exists()\n",
    "        with pubmed_cache.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, delimiter=\"\\t\", fieldnames=[\"pmid\", \"title\", \"abstract\"])\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            for i in tqdm(range(0, len(missing_pmids), args.pubmed_batch_size), desc=\"PubMed\"):\n",
    "                batch = missing_pmids[i : i + args.pubmed_batch_size]\n",
    "                got = fetch_pubmed_abstracts(\n",
    "                    session=session,\n",
    "                    pmids=batch,\n",
    "                    sleep_s=args.sleep_pubmed,\n",
    "                    email=args.email,\n",
    "                    api_key=args.api_key,\n",
    "                )\n",
    "                for pmid, (title, abstract) in got.items():\n",
    "                    if args.max_abstract_chars and abstract:\n",
    "                        abstract = abstract[: int(args.max_abstract_chars)]\n",
    "                    pubmed_map[pmid] = (title, abstract)\n",
    "                    writer.writerow({\"pmid\": pmid, \"title\": title, \"abstract\": abstract})\n",
    "    # Build final EntryID -> text\n",
    "    print(\"Writing entryid_text.tsv...\")\n",
    "    with args.out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f, delimiter=\"\\t\")\n",
    "        w.writerow([\"EntryID\", \"text\"])\n",
    "        n_with_uniprot = 0\n",
    "        n_with_pubmed = 0\n",
    "        for acc in tqdm(uniq, desc=\"Assemble\"):\n",
    "            r = uniprot_rows.get(acc)\n",
    "            if r is None:\n",
    "                w.writerow([acc, \"\"])\n",
    "                continue\n",
    "            parts: list[str] = []\n",
    "            # UniProt text (short, curated)\n",
    "            uniprot_bits = [\n",
    "                r.protein_name,\n",
    "                r.organism_name,\n",
    "                r.keywords,\n",
    "                r.protein_families,\n",
    "                r.cc_function,\n",
    "                r.cc_subcellular_location,\n",
    "            ]\n",
    "            uniprot_text = \" \".join([b for b in uniprot_bits if b])\n",
    "            if uniprot_text:\n",
    "                n_with_uniprot += 1\n",
    "                parts.append(uniprot_text)\n",
    "            # PubMed abstracts (richer)\n",
    "            pmids = pmid_sets.get(acc, [])\n",
    "            abs_parts: list[str] = []\n",
    "            for pmid in pmids:\n",
    "                title, abstract = pubmed_map.get(pmid, (\"\", \"\"))\n",
    "                if title or abstract:\n",
    "                    abs_parts.append(f\"{title}. {abstract}\".strip(\" .\"))\n",
    "            if abs_parts:\n",
    "                n_with_pubmed += 1\n",
    "                parts.append(\" \".join(abs_parts))\n",
    "            text = \" \".join(parts).strip()\n",
    "            if args.strip_go and text:\n",
    "                text = _strip_go_leakage(text)\n",
    "            w.writerow([acc, text])\n",
    "    print(f\"Saved: {args.out_path}\")\n",
    "    print(f\"Coverage: UniProt text {n_with_uniprot}/{len(uniq)} = {n_with_uniprot/ max(1,len(uniq)):.3f}\")\n",
    "    print(f\"Coverage: PubMed text  {n_with_pubmed}/{len(uniq)} = {n_with_pubmed/ max(1,len(uniq)):.3f}\")\n",
    "    return 0\n",
    "# Notebook safety:\n",
    "# Jupyter/IPython runs with __name__ == '__main__', which would trigger argparse via main() and fail\n",
    "# due to extra kernel arguments (e.g. '-f <kernel.json>').\n",
    "# This cell is intentionally library-only; run the next notebook cell to execute it.\n",
    "print('EntryID→text corpus builder loaded. Run Notebook Cell 7 to build WORK_ROOT/external/entryid_text.tsv.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e700ea1",
   "metadata": {
    "id": "run_corpus",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 06 - Run: build EntryID->text corpus (UniProt+PubMed)\n",
    "# Milestone checkpoint: stage_02_external_text\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print('Stage stage_02_external_text: start')\n",
    "out_path = WORK_ROOT / 'external' / 'entryid_text.tsv'\n",
    "cache_dir = CACHE_ROOT / 'uniprot_pubmed_cache'\n",
    "print(f'  out_path: {out_path} (exists={out_path.exists()})')\n",
    "if out_path.exists():\n",
    "    print('  out_path already exists; skipping rebuild')\n",
    "else:\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sys.argv = [\n",
    "        '03_build_entryid_text_from_uniprot_pubmed.py',\n",
    "        '--artefacts-dir', str(WORK_ROOT),\n",
    "        '--out-path', str(out_path),\n",
    "        '--cache-dir', str(cache_dir),\n",
    "        '--max-ids', '0',\n",
    "        '--uniprot-batch-size', '80',\n",
    "        '--max-pubmed-per-protein', '3',\n",
    "        '--strip-go',\n",
    "        '--sleep-uniprot', '0.1',\n",
    "        '--sleep-pubmed', '0.34',\n",
    "    ]\n",
    "    _ = main()\n",
    "if out_path.exists():\n",
    "    print('Stage stage_02_external_text: attempting checkpoint push (may be skipped if unchanged).')\n",
    "    STORE.maybe_push('stage_02_external_text', [out_path], note='entryid->text corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730761d",
   "metadata": {
    "id": "inline_embed_script",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 07 - Inline: embeddings generator (used by TF-IDF; also supports extras)\n",
    "print('Cell 7: embeddings generator loaded')\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def _assert_all_finite_np(arr: np.ndarray, *, name: str) -> None:\n",
    "    if not np.isfinite(arr).all():\n",
    "        nan = int(np.isnan(arr).sum())\n",
    "        inf = int(np.isinf(arr).sum())\n",
    "        raise ValueError(f'{name}: non-finite values detected (nan={nan}, inf={inf}). Refusing to save.')\n",
    "\n",
    "def _read_sequences(feather_path: Path) -> tuple[list[str], list[str]]:\n",
    "    df = pd.read_feather(feather_path)\n",
    "    if \"id\" not in df.columns or \"sequence\" not in df.columns:\n",
    "        raise ValueError(f\"Expected columns id, sequence in {feather_path}; got {list(df.columns)}\")\n",
    "    ids = df[\"id\"].astype(str).tolist()\n",
    "    seqs = df[\"sequence\"].astype(str).tolist()\n",
    "    return ids, seqs\n",
    "def _smart_order(seqs: list[str]) -> np.ndarray:\n",
    "    lengths = np.fromiter((len(s) for s in seqs), dtype=np.int64)\n",
    "    return np.argsort(lengths)[::-1]\n",
    "def _mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    # last_hidden_state: (B, L, D), attention_mask: (B, L)\n",
    "    mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "    x = last_hidden_state * mask\n",
    "    denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "    return x.sum(dim=1) / denom\n",
    "@torch.no_grad()\n",
    "def embed_esm2(\n",
    "    seqs: list[str],\n",
    "    model_name: str,\n",
    "    batch_size: int,\n",
    "    max_len: int,\n",
    "    device: torch.device,\n",
    ") -> np.ndarray:\n",
    "    from transformers import EsmModel, EsmTokenizer\n",
    "    tok = EsmTokenizer.from_pretrained(model_name)\n",
    "    model = EsmModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "    order = _smart_order(seqs)\n",
    "    seqs_sorted = [seqs[i] for i in order]\n",
    "    outs: list[np.ndarray] = []\n",
    "    use_amp = device.type == \"cuda\"\n",
    "    for i in range(0, len(seqs_sorted), batch_size):\n",
    "        batch = seqs_sorted[i : i + batch_size]\n",
    "        ids = tok.batch_encode_plus(\n",
    "            batch,\n",
    "            add_special_tokens=True,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        ids = {k: v.to(device) for k, v in ids.items()}\n",
    "        ctx = torch.amp.autocast(\"cuda\") if use_amp else torch.autocast(\"cpu\", enabled=False)\n",
    "        with ctx:\n",
    "            out = model(**ids)\n",
    "        pooled = _mean_pool(out.last_hidden_state.float(), ids[\"attention_mask\"])\n",
    "        outs.append(pooled.cpu().numpy())\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    embs_sorted = np.vstack(outs).astype(np.float32)\n",
    "    embs = np.zeros_like(embs_sorted)\n",
    "    embs[order] = embs_sorted\n",
    "    return embs\n",
    "@torch.no_grad()\n",
    "def embed_ankh(\n",
    "    seqs: list[str],\n",
    "    model_name: str,\n",
    "    batch_size: int,\n",
    "    max_len: int,\n",
    "    device: torch.device,\n",
    "    trust_remote_code: bool,\n",
    ") -> np.ndarray:\n",
    "    # Ankh models are HuggingFace-hosted and may require trust_remote_code.\n",
    "    # We treat them as encoder-style models and mean-pool hidden states over attention_mask.\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "    model = AutoModel.from_pretrained(model_name, trust_remote_code=trust_remote_code).to(device)\n",
    "    model.eval()\n",
    "    order = _smart_order(seqs)\n",
    "    seqs_sorted = [seqs[i] for i in order]\n",
    "    outs: list[np.ndarray] = []\n",
    "    amp_enabled = device.type == \"cuda\"\n",
    "    for i in range(0, len(seqs_sorted), batch_size):\n",
    "        batch = seqs_sorted[i : i + batch_size]\n",
    "        # Many protein LMs expect space-separated amino acids; if the tokenizer has a small vocab,\n",
    "        # this typically helps. If it hurts, you can disable via --ankh-space-sep 0.\n",
    "        batch = [\" \".join(list(s.replace(\"U\", \"X\").replace(\"Z\", \"X\").replace(\"O\", \"X\").replace(\"B\", \"X\"))) for s in batch]\n",
    "        ids = tok(\n",
    "            batch,\n",
    "            add_special_tokens=True,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        ids = {k: v.to(device) for k, v in ids.items()}\n",
    "        if \"attention_mask\" not in ids:\n",
    "            raise RuntimeError(f\"Tokenizer for {model_name} did not return attention_mask\")\n",
    "        if (ids[\"attention_mask\"].sum(dim=1) == 0).any():\n",
    "            raise RuntimeError(f\"Ankh tokenisation produced an all-zero attention_mask (batch_start={i}).\")\n",
    "        def _forward(use_amp: bool):\n",
    "            ctx = (\n",
    "                torch.amp.autocast(\"cuda\")\n",
    "                if (use_amp and device.type == \"cuda\")\n",
    "                else torch.autocast(\"cpu\", enabled=False)\n",
    "            )\n",
    "            with ctx:\n",
    "                return model(**ids)\n",
    "        out = _forward(amp_enabled)\n",
    "        # HuggingFace convention: encoder outputs `last_hidden_state`\n",
    "        last = getattr(out, \"last_hidden_state\", None)\n",
    "        if last is None:\n",
    "            raise RuntimeError(f\"Model {model_name} did not return last_hidden_state\")\n",
    "        pooled = _mean_pool(last.float(), ids[\"attention_mask\"])\n",
    "        if not bool(torch.isfinite(pooled).all().item()):\n",
    "            if amp_enabled:\n",
    "                print(f\"WARNING: Non-finite Ankh embeddings under AMP at batch_start={i}; retrying without AMP.\")\n",
    "                amp_enabled = False\n",
    "                out = _forward(False)\n",
    "                last = getattr(out, \"last_hidden_state\", None)\n",
    "                if last is None:\n",
    "                    raise RuntimeError(f\"Model {model_name} did not return last_hidden_state\")\n",
    "                pooled = _mean_pool(last.float(), ids[\"attention_mask\"])\n",
    "            if not bool(torch.isfinite(pooled).all().item()):\n",
    "                raise RuntimeError(f\"Ankh produced non-finite embeddings even without AMP (batch_start={i}).\")\n",
    "        outs.append(pooled.cpu().numpy())\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    embs_sorted = np.vstack(outs).astype(np.float32)\n",
    "    _assert_all_finite_np(embs_sorted, name=\"ankh_embeds\")\n",
    "    embs = np.zeros_like(embs_sorted)\n",
    "    embs[order] = embs_sorted\n",
    "    return embs\n",
    "def main() -> int:\n",
    "    ap = argparse.ArgumentParser(description=\"Generate optional multimodal embeddings as .npy artefacts.\")\n",
    "    ap.add_argument(\n",
    "        \"--artefacts-dir\",\n",
    "        type=Path,\n",
    "        default=Path(\"artefacts_local\") / \"artefacts\",\n",
    "        help=\"Path containing parsed/ and features/ (default: artefacts_local/artefacts)\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--mode\",\n",
    "        choices=[\"esm2_3b\", \"ankh\", \"text\"],\n",
    "        required=True,\n",
    "        help=\"Which optional embedding to generate.\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=\"cuda\",\n",
    "        help=\"cuda|cpu (default: cuda)\",\n",
    "    )\n",
    "    ap.add_argument(\"--batch-size\", type=int, default=2)\n",
    "    ap.add_argument(\"--max-len\", type=int, default=1024)\n",
    "    ap.add_argument(\n",
    "        \"--esm2-3b-model\",\n",
    "        type=str,\n",
    "        default=\"facebook/esm2_t36_3B_UR50D\",\n",
    "        help=\"HF model id for ESM2-3B\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--ankh-model\",\n",
    "        type=str,\n",
    "        default=\"ElnaggarLab/ankh-large\",\n",
    "        help=\"HF model id for Ankh (large is typically 1536D)\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--trust-remote-code\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Pass trust_remote_code=True for Ankh loading (often required).\",\n",
    "    )\n",
    "    # Text mode (fixed-width sparse->dense features; default dimension = 10279)\n",
    "    ap.add_argument(\n",
    "        \"--text-path\",\n",
    "        type=Path,\n",
    "        default=None,\n",
    "        help=\"Path to a 2-column TSV/CSV with EntryID and text (required for --mode text).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-sep\",\n",
    "        type=str,\n",
    "        default=\"\\t\",\n",
    "        help=\"Separator for --text-path (default: tab).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-id-col\",\n",
    "        type=str,\n",
    "        default=\"EntryID\",\n",
    "        help=\"ID column name in --text-path (default: EntryID).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-col\",\n",
    "        type=str,\n",
    "        default=\"text\",\n",
    "        help=\"Text column name in --text-path (default: text).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-dim\",\n",
    "        type=int,\n",
    "        default=10279,\n",
    "        help=\"Output feature dimension for text TF-IDF (default: 10279).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-dtype\",\n",
    "        choices=[\"float16\", \"float32\"],\n",
    "        default=\"float16\",\n",
    "        help=\"On-disk dtype for text .npy (default: float16 to keep size sane).\",\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"--text-ngram-max\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Max n-gram for TF-IDF (default: 2).\",\n",
    "    )\n",
    "    args = ap.parse_args()\n",
    "    artefacts_dir: Path = args.artefacts_dir\n",
    "    parsed_dir = artefacts_dir / \"parsed\"\n",
    "    feat_dir = artefacts_dir / \"features\"\n",
    "    feat_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_feather = parsed_dir / \"train_seq.feather\"\n",
    "    test_feather = parsed_dir / \"test_seq.feather\"\n",
    "    if not train_feather.exists() or not test_feather.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Expected {train_feather} and {test_feather}. Run Phase 1 parsing first.\"\n",
    "        )\n",
    "    # Make HF caches relocatable (handy on Colab/Kaggle)\n",
    "    os.environ.setdefault(\"HF_HOME\", str(artefacts_dir / \"hf_cache\"))\n",
    "    os.environ.setdefault(\"TRANSFORMERS_CACHE\", str(artefacts_dir / \"hf_cache\"))\n",
    "    os.environ.setdefault(\"TORCH_HOME\", str(artefacts_dir / \"torch_cache\"))\n",
    "    device = torch.device(\"cuda\" if (args.device == \"cuda\" and torch.cuda.is_available()) else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"Loading sequences...\")\n",
    "    train_ids, train_seqs = _read_sequences(train_feather)\n",
    "    test_ids, test_seqs = _read_sequences(test_feather)\n",
    "    if args.mode == \"esm2_3b\":\n",
    "        print(f\"Embedding ESM2-3B: {args.esm2_3b_model}\")\n",
    "        train_emb = embed_esm2(train_seqs, args.esm2_3b_model, args.batch_size, args.max_len, device)\n",
    "        test_emb = embed_esm2(test_seqs, args.esm2_3b_model, args.batch_size, args.max_len, device)\n",
    "        np.save(feat_dir / \"train_embeds_esm2_3b.npy\", train_emb)\n",
    "        np.save(feat_dir / \"test_embeds_esm2_3b.npy\", test_emb)\n",
    "        print(f\"Saved: {feat_dir / 'train_embeds_esm2_3b.npy'}\")\n",
    "        print(f\"Saved: {feat_dir / 'test_embeds_esm2_3b.npy'}\")\n",
    "        return 0\n",
    "    if args.mode == \"ankh\":\n",
    "        print(f\"Embedding Ankh: {args.ankh_model}\")\n",
    "        train_emb = embed_ankh(\n",
    "            train_seqs,\n",
    "            args.ankh_model,\n",
    "            args.batch_size,\n",
    "            args.max_len,\n",
    "            device,\n",
    "            trust_remote_code=args.trust_remote_code,\n",
    "        )\n",
    "        test_emb = embed_ankh(\n",
    "            test_seqs,\n",
    "            args.ankh_model,\n",
    "            args.batch_size,\n",
    "            args.max_len,\n",
    "            device,\n",
    "            trust_remote_code=args.trust_remote_code,\n",
    "        )\n",
    "        np.save(feat_dir / \"train_embeds_ankh.npy\", train_emb)\n",
    "        np.save(feat_dir / \"test_embeds_ankh.npy\", test_emb)\n",
    "        print(f\"Saved: {feat_dir / 'train_embeds_ankh.npy'}\")\n",
    "        print(f\"Saved: {feat_dir / 'test_embeds_ankh.npy'}\")\n",
    "        return 0\n",
    "    if args.mode == \"text\":\n",
    "        if args.text_path is None:\n",
    "            raise ValueError(\"--text-path is required for --mode text\")\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        import joblib\n",
    "        if not args.text_path.exists():\n",
    "            raise FileNotFoundError(f\"text-path not found: {args.text_path}\")\n",
    "        print(f\"Loading text corpus: {args.text_path}\")\n",
    "        df = pd.read_csv(args.text_path, sep=args.text_sep, dtype=str)\n",
    "        if args.text_id_col not in df.columns or args.text_col not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"Expected columns {args.text_id_col!r}, {args.text_col!r} in {args.text_path}; got {list(df.columns)}\"\n",
    "            )\n",
    "        # Many-to-one mapping is possible (multiple pubs per EntryID). Concatenate.\n",
    "        df = df[[args.text_id_col, args.text_col]].dropna()\n",
    "        df[args.text_id_col] = df[args.text_id_col].astype(str)\n",
    "        df[args.text_col] = df[args.text_col].astype(str)\n",
    "        grouped = df.groupby(args.text_id_col, sort=False)[args.text_col].apply(lambda x: \" \\n \".join(x.tolist()))\n",
    "        text_map = grouped.to_dict()\n",
    "        def _norm_uniprot_id(pid: str) -> str:\n",
    "            # Common UniProt FASTA headers: sp|P12345|NAME_HUMAN ...\n",
    "            parts = str(pid).split(\"|\")\n",
    "            if len(parts) >= 2 and parts[0] in {\"sp\", \"tr\"}:\n",
    "                return parts[1]\n",
    "            if len(parts) >= 3:\n",
    "                return parts[1]\n",
    "            return str(pid)\n",
    "        # Support both raw IDs and normalised accessions for joining.\n",
    "        text_map_norm: dict[str, str] = {}\n",
    "        for k, v in text_map.items():\n",
    "            text_map_norm[str(k)] = v\n",
    "            text_map_norm[_norm_uniprot_id(k)] = v\n",
    "        train_texts = [text_map_norm.get(str(pid), \"\") for pid in train_ids]\n",
    "        test_texts = [text_map_norm.get(str(pid), \"\") for pid in test_ids]\n",
    "        # TF-IDF gives a fixed-width, high-dimensional text modality with controllable size.\n",
    "        # This is the most practical way to realise a 10279D \"text embedding\" without a bespoke LLM pipeline.\n",
    "        print(f\"Fitting TF-IDF (dim={args.text_dim}, ngram<= {args.text_ngram_max})...\")\n",
    "        vec = TfidfVectorizer(\n",
    "            max_features=args.text_dim,\n",
    "            ngram_range=(1, max(1, int(args.text_ngram_max))),\n",
    "            min_df=2,\n",
    "            strip_accents=\"unicode\",\n",
    "            lowercase=True,\n",
    "        )\n",
    "        vec.fit(train_texts + test_texts)\n",
    "        n_features = len(vec.get_feature_names_out())\n",
    "        print(f\"TF-IDF vocab size: {n_features} (padded to {args.text_dim})\")\n",
    "        # Persist vectorizer for reproducibility\n",
    "        joblib.dump(vec, feat_dir / \"text_vectorizer.joblib\")\n",
    "        print(f\"Saved: {feat_dir / 'text_vectorizer.joblib'}\")\n",
    "        out_dtype = np.float16 if args.text_dtype == \"float16\" else np.float32\n",
    "        def _write_memmap(name: str, texts: list[str]):\n",
    "            path = feat_dir / name\n",
    "            mm = np.lib.format.open_memmap(path, mode=\"w+\", dtype=out_dtype, shape=(len(texts), args.text_dim))\n",
    "            bs = 2048\n",
    "            for i in range(0, len(texts), bs):\n",
    "                chunk = texts[i : i + bs]\n",
    "                X = vec.transform(chunk)  # sparse\n",
    "                # Convert per-chunk to dense to write to .npy\n",
    "                arr = X.toarray().astype(out_dtype, copy=False)\n",
    "                if arr.shape[1] == args.text_dim:\n",
    "                    mm[i : i + arr.shape[0], :] = arr\n",
    "                else:\n",
    "                    dense = np.zeros((arr.shape[0], args.text_dim), dtype=out_dtype)\n",
    "                    if arr.shape[1] > 0:\n",
    "                        dense[:, : arr.shape[1]] = arr\n",
    "                    mm[i : i + dense.shape[0], :] = dense\n",
    "            mm.flush()\n",
    "            return path\n",
    "        train_path = _write_memmap(\"train_embeds_text.npy\", train_texts)\n",
    "        test_path = _write_memmap(\"test_embeds_text.npy\", test_texts)\n",
    "        print(f\"Saved: {train_path}\")\n",
    "        print(f\"Saved: {test_path}\")\n",
    "        return 0\n",
    "    raise RuntimeError(\"unreachable\")\n",
    "# Notebook safety:\n",
    "# This cell is intentionally library-only (no auto-run).\n",
    "# Jupyter passes extra kernel args (e.g. '-f <kernel.json>') that break argparse-based mains.\n",
    "# Use the dedicated run cell that sets sys.argv explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f593b",
   "metadata": {
    "id": "run_tfidf",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 08 - Run: TF-IDF embeddings (10279D)\n",
    "# Milestone checkpoint: stage_03_tfidf_text\n",
    "# Option B strictness: TF-IDF is required, but we allow building it in-place when missing.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "STRICT_OPTION_B = True\n",
    "USE_TFIDF_TEXT = True\n",
    "feat_dir = WORK_ROOT / 'features'\n",
    "text_vect = feat_dir / 'text_vectorizer.joblib'\n",
    "train_text_npy = feat_dir / 'train_embeds_text.npy'\n",
    "test_text_npy = feat_dir / 'test_embeds_text.npy'\n",
    "generated = False\n",
    "if USE_TFIDF_TEXT:\n",
    "    have_all = text_vect.exists() and train_text_npy.exists() and test_text_npy.exists()\n",
    "    if have_all:\n",
    "        print('TF-IDF artefacts already exist; skipping generation')\n",
    "    else:\n",
    "        text_path = WORK_ROOT / 'external' / 'entryid_text.tsv'\n",
    "        if not text_path.exists():\n",
    "            raise FileNotFoundError(f'Missing corpus file: {text_path}. Run the corpus milestone first.')\n",
    "        sys.argv = [\n",
    "            '02_generate_optional_embeddings.py',\n",
    "            '--artefacts-dir', str(WORK_ROOT),\n",
    "            '--mode', 'text',\n",
    "            '--text-path', str(text_path),\n",
    "            '--text-dim', '10279',\n",
    "        ]\n",
    "        _ = main()\n",
    "        generated = True\n",
    "if text_vect.exists() and train_text_npy.exists() and test_text_npy.exists():\n",
    "    if generated:\n",
    "        print('TF-IDF generated; attempting checkpoint push (may be skipped if unchanged).')\n",
    "    else:\n",
    "        print('TF-IDF present; attempting checkpoint push (may be skipped if unchanged).')\n",
    "    STORE.maybe_push('stage_03_tfidf_text', [text_vect, train_text_npy, test_text_npy], note='tf-idf 10279D')\n",
    "    # Diagnostics: TF-IDF sparsity + row norms (sampled)\n",
    "    try:\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        def _sample_rows(x: np.ndarray, n: int = 20000) -> np.ndarray:\n",
    "            m = min(int(x.shape[0]), int(n))\n",
    "            if m <= 0:\n",
    "                return np.zeros((0, int(x.shape[1])), dtype=np.float32)\n",
    "            idx = np.linspace(0, int(x.shape[0]) - 1, num=m, dtype=np.int64)\n",
    "            return np.asarray(x[idx], dtype=np.float32)\n",
    "        x_tr = np.load(train_text_npy, mmap_mode='r')\n",
    "        x_te = np.load(test_text_npy, mmap_mode='r')\n",
    "        tr = _sample_rows(x_tr)\n",
    "        te = _sample_rows(x_te)\n",
    "        tr_nnz = (tr > 0).sum(axis=1)\n",
    "        te_nnz = (te > 0).sum(axis=1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(tr_nnz, bins=60, alpha=0.6, label=f'train nnz/row (n={len(tr_nnz)})')\n",
    "        plt.hist(te_nnz, bins=60, alpha=0.6, label=f'test nnz/row (n={len(te_nnz)})')\n",
    "        plt.title('TF-IDF: non-zeros per row (sampled)')\n",
    "        plt.xlabel('nnz per row')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        tr_norm = np.linalg.norm(tr, axis=1)\n",
    "        te_norm = np.linalg.norm(te, axis=1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(tr_norm, bins=60, alpha=0.6, label='train L2')\n",
    "        plt.hist(te_norm, bins=60, alpha=0.6, label='test L2')\n",
    "        plt.title('TF-IDF: L2 norm per row (sampled)')\n",
    "        plt.xlabel('L2 norm')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print('TF-IDF diagnostics skipped:', repr(e))\n",
    "elif STRICT_OPTION_B:\n",
    "    missing = [p for p in [text_vect, train_text_npy, test_text_npy] if not p.exists()]\n",
    "    raise FileNotFoundError('Option B requires TF-IDF artefacts, but these are missing:\\n' + '\\n'.join([f' - {m}' for m in missing]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f35a2df",
   "metadata": {
    "id": "2f35a2df",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 09 - Solution: 2.1 PHASE 1 (Step 3): EXTERNAL DATA & EVIDENCE CODES (ARTEFACT-FIRST)\n",
    "# 2.1 PHASE 1 (Step 3): EXTERNAL DATA & EVIDENCE CODES (ARTEFACT-FIRST)\n",
    "# ================================================================\n",
    "# Option B strictness: external GOA artefacts are REQUIRED (no silent fallback).\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "STRICT_OPTION_B = True\n",
    "# Option B: always enable external features\n",
    "PROCESS_EXTERNAL = True\n",
    "# Recommended (Kaggle): set CAFA_GOA_DATASET_DIR to your dataset folder,\n",
    "# e.g. /kaggle/input/goa-filtered-all-tsv-gz\n",
    "GOA_ENV_DIR = os.getenv('CAFA_GOA_DATASET_DIR', '').strip()\n",
    "GOA_ARTEFACT_DIRS = [\n",
    "    Path(GOA_ENV_DIR) if GOA_ENV_DIR else None,\n",
    "    # Path('/kaggle/input/<dataset-folder>'),\n",
    "]\n",
    "if PROCESS_EXTERNAL:\n",
    "    EXT_DIR = WORK_ROOT / 'external'\n",
    "    EXT_DIR.mkdir(exist_ok=True)\n",
    "    def _existing(dirs: list[Path]) -> list[Path]:\n",
    "        return [Path(d) for d in dirs if d is not None and str(d).strip() and Path(d).exists()]\n",
    "    def _discover(dirs: list[Path]) -> list[Path]:\n",
    "        patterns = ('**/goa_filtered_*.tsv.gz', '**/goa_filtered_*.tsv')\n",
    "        found: list[Path] = []\n",
    "        for d in dirs:\n",
    "            found += [\n",
    "                d / 'goa_filtered_all.tsv.gz', d / 'goa_filtered_iea.tsv.gz',\n",
    "                d / 'goa_filtered_all.tsv', d / 'goa_filtered_iea.tsv',\n",
    "            ]\n",
    "            for pat in patterns:\n",
    "                found += sorted(Path(d).glob(pat))\n",
    "        out: list[Path] = []\n",
    "        seen = set()\n",
    "        for p in found:\n",
    "            p = Path(p)\n",
    "            if p in seen:\n",
    "                continue\n",
    "            seen.add(p)\n",
    "            if p.exists() and p.is_file():\n",
    "                out.append(p)\n",
    "        return out\n",
    "    def _pick_best(paths: list[Path]) -> Path:\n",
    "        def score(p: Path) -> tuple[int, int]:\n",
    "            name = p.name.lower()\n",
    "            ext_rank = 0 if name.endswith('.tsv.gz') else 1\n",
    "            if 'goa_filtered_all' in name:\n",
    "                return (0, ext_rank)\n",
    "            if 'goa_filtered_iea' in name:\n",
    "                return (1, ext_rank)\n",
    "            if 'all' in name:\n",
    "                return (2, ext_rank)\n",
    "            if 'iea' in name:\n",
    "                return (3, ext_rank)\n",
    "            return (4, ext_rank)\n",
    "        return sorted(paths, key=score)[0]\n",
    "    roots = _existing(GOA_ARTEFACT_DIRS)\n",
    "    if not roots and Path('/kaggle/input').exists():\n",
    "        # fallback discovery, but still strict if nothing found\n",
    "        roots = [Path('/kaggle/input')]\n",
    "    elif not roots:\n",
    "        roots = [EXT_DIR, Path('artefacts_local/artefacts/external')]\n",
    "    goa_paths = _discover(roots)\n",
    "    if not goa_paths:\n",
    "        msg = (\n",
    "            'Option B requires precomputed GOA artefacts, but none were found.\\n'\n",
    "            'Fix:\\n'\n",
    "            ' - Publish goa_filtered_all.tsv.gz (and/or goa_filtered_iea.tsv.gz) as a Kaggle Dataset\\n'\n",
    "            ' - Attach it to this notebook\\n'\n",
    "            ' - Set env var CAFA_GOA_DATASET_DIR=/kaggle/input/<dataset-folder> (recommended)\\n'\n",
    "            ' - Or place the file under artefacts/external locally'\n",
    "        )\n",
    "        if STRICT_OPTION_B:\n",
    "            raise FileNotFoundError(msg)\n",
    "        print(msg)\n",
    "        PROCESS_EXTERNAL = False\n",
    "    else:\n",
    "        GOA_FEATURE_PATH = _pick_best(goa_paths)\n",
    "        print('Using GOA artefact:', GOA_FEATURE_PATH)\n",
    "        print('Format expected: EntryID<TAB>term<TAB>evidence (header included)')\n",
    "else:\n",
    "    raise RuntimeError('Option B requires PROCESS_EXTERNAL=True (unexpected).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d39e4",
   "metadata": {
    "id": "821d39e4",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 10 - Solution: 2.2 PHASE 1 (Step 4): HIERARCHY PROPAGATION FOR EXTERNAL GOA (NO-KAGGLE / IEA)\n",
    "# Milestone checkpoint: stage_04_external_goa_priors\n",
    "# Produces:\n",
    "# - external/prop_train_no_kaggle.tsv.gz\n",
    "# - external/prop_test_no_kaggle.tsv.gz\n",
    "# Option B strictness: this step MUST be satisfied when PROCESS_EXTERNAL=True.\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "EXTERNAL_TOP_K = 1500\n",
    "EXTERNAL_PRIOR_SCORE = 1.0  # binary prior (later we down-weight when injecting into models)\n",
    "if PROCESS_EXTERNAL:\n",
    "    if 'GOA_FEATURE_PATH' not in locals():\n",
    "        raise RuntimeError('Option B requires GOA_FEATURE_PATH. Run the external GOA discovery cell first.')\n",
    "    # Ensure we have GO parents (hierarchy)\n",
    "    if 'go_parents' not in locals():\n",
    "        def parse_obo(path: Path):\n",
    "            parents = {}\n",
    "            namespaces = {}\n",
    "            cur_id, cur_ns = None, None\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line == '[Term]':\n",
    "                        if cur_id and cur_ns:\n",
    "                            namespaces[cur_id] = cur_ns\n",
    "                        cur_id, cur_ns = None, None\n",
    "                    elif line.startswith('id: GO:'):\n",
    "                        cur_id = line.split('id: ', 1)[1]\n",
    "                    elif line.startswith('namespace:'):\n",
    "                        cur_ns = line.split('namespace: ', 1)[1]\n",
    "                    elif line.startswith('is_a:') and cur_id:\n",
    "                        parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                        parents.setdefault(cur_id, set()).add(parent)\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "            return parents, namespaces\n",
    "        go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "    # Ensure train/test IDs are available\n",
    "    train_seq_path = WORK_ROOT / 'parsed' / 'train_seq.feather'\n",
    "    test_seq_path = WORK_ROOT / 'parsed' / 'test_seq.feather'\n",
    "    if not train_seq_path.exists():\n",
    "        raise FileNotFoundError('Missing train_seq.feather. Run Phase 1 Step 2 (FASTA parse) first.')\n",
    "    if not test_seq_path.exists():\n",
    "        raise FileNotFoundError('Missing test_seq.feather. Run Phase 1 Step 2 (FASTA parse) first.')\n",
    "    # IMPORTANT: GOA artefacts usually use UniProt accessions, but FASTA IDs may be like sp|P12345|NAME.\n",
    "    # Normalise for matching, but write the original IDs so downstream joins (reindex on train_ids/test_ids) still work.\n",
    "    def _norm_entry_id(raw: str) -> str:\n",
    "        s = str(raw).strip()\n",
    "        parts = s.split('|')\n",
    "        if len(parts) >= 2 and parts[0] in {'sp', 'tr'}:\n",
    "            return parts[1]\n",
    "        if len(parts) >= 3:\n",
    "            return parts[1]\n",
    "        return s\n",
    "    train_ids_raw = pd.read_feather(train_seq_path)['id'].astype(str).tolist()\n",
    "    test_ids_raw = pd.read_feather(test_seq_path)['id'].astype(str).tolist()\n",
    "    train_id_map = {_norm_entry_id(i): i for i in train_ids_raw}\n",
    "    test_id_map = {_norm_entry_id(i): i for i in test_ids_raw}\n",
    "    train_ids = set(train_id_map.keys())\n",
    "    test_ids = set(test_id_map.keys())\n",
    "    # Get top-K train terms (defines the external feature space)\n",
    "    train_terms_path = WORK_ROOT / 'parsed' / 'train_terms.parquet'\n",
    "    if not train_terms_path.exists():\n",
    "        raise FileNotFoundError('Missing train_terms.parquet. Run Phase 1 Step 2 (targets parse) first.')\n",
    "    train_terms = pd.read_parquet(train_terms_path)\n",
    "    top_terms = train_terms['term'].value_counts().head(EXTERNAL_TOP_K).index.tolist()\n",
    "    top_terms_set = set(top_terms)\n",
    "    print(f'External propagation restricted to top {len(top_terms)} train terms.')\n",
    "    EXT_DIR = WORK_ROOT / 'external'\n",
    "    EXT_DIR.mkdir(exist_ok=True)\n",
    "    out_train = EXT_DIR / 'prop_train_no_kaggle.tsv.gz'\n",
    "    out_test = EXT_DIR / 'prop_test_no_kaggle.tsv.gz'\n",
    "    if out_train.exists() and out_test.exists():\n",
    "        print('Propagated GOA priors already present; skipping propagation.')\n",
    "    else:\n",
    "        # Ancestor closure with memoisation\n",
    "        _anc_cache: dict[str, set[str]] = {}\n",
    "        def ancestors(term: str) -> set[str]:\n",
    "            if term in _anc_cache:\n",
    "                return _anc_cache[term]\n",
    "            seen = {term}\n",
    "            stack = [term]\n",
    "            while stack:\n",
    "                t = stack.pop()\n",
    "                for p in go_parents.get(t, ()):\n",
    "                    if p not in seen:\n",
    "                        seen.add(p)\n",
    "                        stack.append(p)\n",
    "            _anc_cache[term] = seen\n",
    "            return seen\n",
    "        cols = ['EntryID', 'term', 'evidence']\n",
    "        print('Streaming GOA artefact:', GOA_FEATURE_PATH)\n",
    "        print('Writing:', out_train)\n",
    "        print('Writing:', out_test)\n",
    "        n_train = 0\n",
    "        n_test = 0\n",
    "        with gzip.open(out_train, 'wt', encoding='utf-8') as ftr, gzip.open(out_test, 'wt', encoding='utf-8') as fte:\n",
    "            ftr.write('EntryID\\tterm\\tscore\\n')\n",
    "            fte.write('EntryID\\tterm\\tscore\\n')\n",
    "            for chunk in pd.read_csv(\n",
    "                GOA_FEATURE_PATH,\n",
    "                sep='\\t',\n",
    "                dtype=str,\n",
    "                usecols=lambda c: c in cols,\n",
    "                chunksize=500_000,\n",
    "            ):\n",
    "                missing_cols = [c for c in cols if c not in chunk.columns]\n",
    "                if missing_cols:\n",
    "                    raise ValueError(f'GOA artefact missing columns: {missing_cols}. Found: {list(chunk.columns)}')\n",
    "                chunk = chunk[cols].dropna()\n",
    "                chunk = chunk[chunk['evidence'] == 'IEA']\n",
    "                if chunk.empty:\n",
    "                    continue\n",
    "                chunk = chunk.drop_duplicates(subset=['EntryID', 'term'])\n",
    "                for entry_id_raw, term in zip(chunk['EntryID'].tolist(), chunk['term'].tolist()):\n",
    "                    entry_norm = _norm_entry_id(entry_id_raw)\n",
    "                    if entry_norm in train_ids:\n",
    "                        target = ftr\n",
    "                        out_entry = train_id_map[entry_norm]\n",
    "                    elif entry_norm in test_ids:\n",
    "                        target = fte\n",
    "                        out_entry = test_id_map[entry_norm]\n",
    "                    else:\n",
    "                        continue\n",
    "                    keep = ancestors(term) & top_terms_set\n",
    "                    if not keep:\n",
    "                        continue\n",
    "                    for t in keep:\n",
    "                        target.write(f'{out_entry}\\t{t}\\t{EXTERNAL_PRIOR_SCORE}\\n')\n",
    "                    if target is ftr:\n",
    "                        n_train += len(keep)\n",
    "                    else:\n",
    "                        n_test += len(keep)\n",
    "        print(f'Wrote propagated IEA edges: train={n_train:,} test={n_test:,}')\n",
    "        print('Outputs are intentionally sparse priors (score=1.0) and will be down-weighted when injected.')\n",
    "    if out_train.exists() and out_test.exists():\n",
    "        STORE.maybe_push('stage_04_external_goa_priors', [out_train, out_test], note='propagated IEA priors')\n",
    "else:\n",
    "    raise RuntimeError('Option B requires PROCESS_EXTERNAL=True (unexpected).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea612e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10b - Diagnostics: artefact manifest (existence + sizes)\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "def _mb(p: Path) -> float:\n",
    "    return p.stat().st_size / (1024**2)\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "# Minimal contract: Phase 1 outputs + Option B required artefacts + later model outputs\n",
    "paths = {\n",
    "    # Phase 1 parsed\n",
    "    'parsed/train_seq.feather': WORK_ROOT / 'parsed' / 'train_seq.feather',\n",
    "    'parsed/test_seq.feather': WORK_ROOT / 'parsed' / 'test_seq.feather',\n",
    "    'parsed/train_terms.parquet': WORK_ROOT / 'parsed' / 'train_terms.parquet',\n",
    "    'parsed/term_priors.parquet': WORK_ROOT / 'parsed' / 'term_priors.parquet',\n",
    "    'parsed/train_taxa.feather': WORK_ROOT / 'parsed' / 'train_taxa.feather',\n",
    "    'parsed/test_taxa.feather': WORK_ROOT / 'parsed' / 'test_taxa.feather',\n",
    "    # Text pipeline\n",
    "    'external/entryid_text.tsv': WORK_ROOT / 'external' / 'entryid_text.tsv',\n",
    "    'features/text_vectorizer.joblib': WORK_ROOT / 'features' / 'text_vectorizer.joblib',\n",
    "    'features/train_embeds_text.npy': WORK_ROOT / 'features' / 'train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy': WORK_ROOT / 'features' / 'test_embeds_text.npy',\n",
    "    # Sequence embeddings (core)\n",
    "    'features/train_embeds_t5.npy': WORK_ROOT / 'features' / 'train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy': WORK_ROOT / 'features' / 'test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy': WORK_ROOT / 'features' / 'train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy': WORK_ROOT / 'features' / 'test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy': WORK_ROOT / 'features' / 'train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy': WORK_ROOT / 'features' / 'test_embeds_esm2_3b.npy',\n",
    "    'features/train_embeds_ankh.npy': WORK_ROOT / 'features' / 'train_embeds_ankh.npy',\n",
    "    'features/test_embeds_ankh.npy': WORK_ROOT / 'features' / 'test_embeds_ankh.npy',\n",
    "    # External priors\n",
    "    'external/prop_train_no_kaggle.tsv.gz': WORK_ROOT / 'external' / 'prop_train_no_kaggle.tsv.gz',\n",
    "    'external/prop_test_no_kaggle.tsv.gz': WORK_ROOT / 'external' / 'prop_test_no_kaggle.tsv.gz',\n",
    "    # Downstream expectations\n",
    "    'features/top_terms_13500.json': WORK_ROOT / 'features' / 'top_terms_13500.json',\n",
    "    'features/oof_pred_logreg.npy': WORK_ROOT / 'features' / 'oof_pred_logreg.npy',\n",
    "    'features/oof_pred_gbdt.npy': WORK_ROOT / 'features' / 'oof_pred_gbdt.npy',\n",
    "    'features/oof_pred_dnn.npy': WORK_ROOT / 'features' / 'oof_pred_dnn.npy',\n",
    "    'features/test_pred_logreg.npy': WORK_ROOT / 'features' / 'test_pred_logreg.npy',\n",
    "    'features/test_pred_gbdt.npy': WORK_ROOT / 'features' / 'test_pred_gbdt.npy',\n",
    "    'features/test_pred_dnn.npy': WORK_ROOT / 'features' / 'test_pred_dnn.npy',\n",
    "    'features/test_pred_gcn.npy': WORK_ROOT / 'features' / 'test_pred_gcn.npy',\n",
    "}\n",
    "rows = []\n",
    "for name, p in paths.items():\n",
    "    rows.append({'artefact': name, 'exists': p.exists(), 'mb': _mb(p) if p.exists() else 0.0, 'path': str(p)})\n",
    "df = pd.DataFrame(rows).sort_values(['exists', 'mb'], ascending=[True, False])\n",
    "print('WORK_ROOT:', WORK_ROOT)\n",
    "try:\n",
    "    from IPython.display import display  # type: ignore\n",
    "except Exception:\n",
    "    def display(x):\n",
    "        print(x)\n",
    "display(df)\n",
    "display(df)\n",
    "# Visual: top 25 largest artefacts\n",
    "df2 = df[df['exists']].sort_values('mb', ascending=False).head(25)\n",
    "if len(df2) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df2, y='artefact', x='mb')\n",
    "    plt.title('Largest artefacts (MB)')\n",
    "    plt.xlabel('MB')\n",
    "    plt.ylabel('artefact')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Option B strict check (what must exist before the stacker)\n",
    "option_b_required = [\n",
    "    'external/prop_train_no_kaggle.tsv.gz',\n",
    "    'external/prop_test_no_kaggle.tsv.gz',\n",
    "    'features/text_vectorizer.joblib',\n",
    "    'features/train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy',\n",
    "    'features/train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy',\n",
    "    'features/train_embeds_ankh.npy',\n",
    "    'features/test_embeds_ankh.npy',\n",
    "    'parsed/train_taxa.feather',\n",
    "    'parsed/test_taxa.feather',\n",
    "    'features/top_terms_13500.json',\n",
    "]\n",
    "missing = [a for a in option_b_required if not paths[a].exists()]\n",
    "if missing:\n",
    "    print('\\nOption B missing artefacts:')\n",
    "    for m in missing:\n",
    "        print(' -', m)\n",
    "else:\n",
    "    print('\\nOption B artefacts OK: priors + embeddings + TF-IDF + taxonomy + top terms present.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23459f67",
   "metadata": {
    "id": "23459f67",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 11 - Solution: 3a. PHASE 1: EMBEDDINGS GENERATION (T5 only)\n",
    "# 3a. PHASE 1: EMBEDDINGS GENERATION (T5 only)\n",
    "# ============================================\n",
    "# HARDWARE: GPU recommended\n",
    "# ============================================\n",
    "# Split from ESM2 so you can run each independently on Kaggle.\n",
    "import os\n",
    "\n",
    "# IMPORTANT: pushing checkpoints republishes a whole Kaggle Dataset version (but only the staged flat files we include),\n",
    "# which can mean re-uploading multi-GB zips even when embeddings already exist.\n",
    "t5_train_path = WORK_ROOT / 'features' / 'train_embeds_t5.npy'\n",
    "t5_test_path = WORK_ROOT / 'features' / 'test_embeds_t5.npy'\n",
    "has_test = (WORK_ROOT / 'parsed' / 'test_seq.feather').exists()\n",
    "train_ready = t5_train_path.exists()\n",
    "test_ready = (not has_test) or t5_test_path.exists()\n",
    "COMPUTE_T5 = True  # <--- enable/disable T5 run\n",
    "FORCE_REBUILD = False  # manual toggle; keep default strict\n",
    "train_needed = COMPUTE_T5 and (FORCE_REBUILD or (not train_ready))\n",
    "train_needed = COMPUTE_T5 and (FORCE_REBUILD or (not train_ready))\n",
    "test_needed = COMPUTE_T5 and has_test and (FORCE_REBUILD or (not t5_test_path.exists()))\n",
    "# If artefacts already exist, skip compute unless forced.\n",
    "if COMPUTE_T5 and (not train_needed) and (not test_needed):\n",
    "    COMPUTE_T5 = False\n",
    "    if not PUSH_EXISTING_CHECKPOINTS:\n",
    "        print('T5 embeddings already exist; skipping compute and not pushing checkpoints (PUSH_EXISTING_CHECKPOINTS=False).')\n",
    "    if PUSH_EXISTING_CHECKPOINTS and 'STORE' in globals() and STORE is not None:\n",
    "        # Granular checkpoints: allow resuming test without losing train.\n",
    "        try:\n",
    "            STORE.maybe_push('stage_05a_embeddings_t5_train', [t5_train_path], note='ProtT5 mean-pool embeddings (train only, existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_05a_embeddings_t5_train: {e}')\n",
    "        if t5_test_path.exists():\n",
    "            try:\n",
    "                STORE.maybe_push('stage_05b_embeddings_t5_test', [t5_test_path], note='ProtT5 mean-pool embeddings (test only, existing)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_05b_embeddings_t5_test: {e}')\n",
    "        # Backwards-compatible combined stage.\n",
    "        paths = [t5_train_path]\n",
    "        if t5_test_path.exists():\n",
    "            paths.append(t5_test_path)\n",
    "        try:\n",
    "            STORE.maybe_push('stage_05_embeddings_t5', paths, note='ProtT5 mean-pool embeddings (existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_05_embeddings_t5: {e}')\n",
    "if COMPUTE_T5:\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from transformers import T5Tokenizer, T5EncoderModel\n",
    "    from tqdm.auto import tqdm\n",
    "    import contextlib\n",
    "    # Optimise CUDA memory allocation\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    # Fix Protobuf 'GetPrototype' error\n",
    "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    amp_ctx = torch.amp.autocast('cuda') if device.type == 'cuda' else contextlib.nullcontext()\n",
    "    def get_t5_model():\n",
    "        print(\"Loading T5 Model...\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\n",
    "            \"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case=False, legacy=True\n",
    "        )\n",
    "        model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "        model.eval()\n",
    "        return tokenizer, model\n",
    "    def generate_embeddings_t5(model, tokenizer, sequences, batch_size=4, max_len=1024):\n",
    "        seq_lens = [len(s) for s in sequences]\n",
    "        sort_idx = np.argsort(seq_lens)[::-1]\n",
    "        sorted_seqs = [sequences[i] for i in sort_idx]\n",
    "        embeddings_list = []\n",
    "        for i in tqdm(range(0, len(sorted_seqs), batch_size), desc=\"Embedding T5 (Smart Batch)\"):\n",
    "            batch_seqs = sorted_seqs[i : i + batch_size]\n",
    "            batch_seqs = [seq.replace('U', 'X').replace('Z', 'X').replace('O', 'X').replace('B', 'X') for seq in batch_seqs]\n",
    "            batch_seqs = [\" \".join(list(seq)) for seq in batch_seqs]\n",
    "            ids = tokenizer.batch_encode_plus(\n",
    "                batch_seqs,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                with amp_ctx:\n",
    "                    embedding_repr = model(input_ids=ids['input_ids'], attention_mask=ids['attention_mask'])\n",
    "            emb = embedding_repr.last_hidden_state.float().detach().cpu().numpy()\n",
    "            mask = ids['attention_mask'].detach().cpu().numpy()\n",
    "            for j in range(len(batch_seqs)):\n",
    "                seq_len = int(mask[j].sum())\n",
    "                valid_emb = emb[j, :seq_len]\n",
    "                embeddings_list.append(valid_emb.mean(axis=0))\n",
    "            del ids, embedding_repr, emb, mask\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        sorted_embeddings = np.vstack(embeddings_list)\n",
    "        original_order_embeddings = np.zeros_like(sorted_embeddings)\n",
    "        original_order_embeddings[sort_idx] = sorted_embeddings\n",
    "        return original_order_embeddings\n",
    "    tokenizer, model = get_t5_model()\n",
    "    if train_needed:\n",
    "        print(\"Loading train sequences for T5 embedding...\")\n",
    "        train_df = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')\n",
    "        print(f\"Generating Train Embeddings T5 ({len(train_df)})...\")\n",
    "        train_emb = generate_embeddings_t5(model, tokenizer, train_df['sequence'].tolist())\n",
    "        np.save(t5_train_path, train_emb)\n",
    "        # Push immediately after train completes so a crash during test doesn't lose progress.\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            try:\n",
    "                STORE.maybe_push('stage_05a_embeddings_t5_train', [t5_train_path], note='ProtT5 mean-pool embeddings (train only)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_05a_embeddings_t5_train: {e}')\n",
    "        del train_emb, train_df\n",
    "        gc.collect()\n",
    "    else:\n",
    "        print(f\"Train embeddings already exist at {t5_train_path}; skipping train compute.\")\n",
    "    if test_needed:\n",
    "        print(\"Loading test sequences for T5 embedding...\")\n",
    "        test_df = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')\n",
    "        print(f\"Generating Test Embeddings T5 ({len(test_df)})...\")\n",
    "        test_emb = generate_embeddings_t5(model, tokenizer, test_df['sequence'].tolist())\n",
    "        np.save(t5_test_path, test_emb)\n",
    "        # Push immediately after test completes as well.\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            try:\n",
    "                STORE.maybe_push('stage_05b_embeddings_t5_test', [t5_test_path], note='ProtT5 mean-pool embeddings (test only)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_05b_embeddings_t5_test: {e}')\n",
    "        del test_emb, test_df\n",
    "        gc.collect()\n",
    "    else:\n",
    "        if has_test:\n",
    "            print(f\"Test embeddings already exist at {t5_test_path}; skipping test compute.\")\n",
    "        else:\n",
    "            print(\"No test sequences found; skipping test embedding.\")\n",
    "    del model, tokenizer\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"T5 embeddings ready.\")\n",
    "    # Backwards-compatible combined stage.\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        paths = [t5_train_path]\n",
    "        if t5_test_path.exists():\n",
    "            paths.append(t5_test_path)\n",
    "        try:\n",
    "            STORE.maybe_push('stage_05_embeddings_t5', paths, note='ProtT5 mean-pool embeddings')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_05_embeddings_t5: {e}')\n",
    "else:\n",
    "    print(\"Skipping T5 embedding generation (COMPUTE_T5=False).\")\n",
    "# Diagnostics: embedding norms (train vs test; sampled)\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    def _plot_embed_norms(name: str, train_path: Path, test_path: Path):\n",
    "        if not Path(train_path).exists():\n",
    "            print(f'[{name}] missing train embeddings: {train_path}')\n",
    "            return\n",
    "        x_tr = np.load(train_path, mmap_mode='r')\n",
    "        x_te = np.load(test_path, mmap_mode='r') if Path(test_path).exists() else None\n",
    "        def _sample_norms(x: np.ndarray, n: int = 20000) -> np.ndarray:\n",
    "            m = min(int(x.shape[0]), int(n))\n",
    "            if m <= 0:\n",
    "                return np.zeros((0,), dtype=np.float32)\n",
    "            idx = np.linspace(0, int(x.shape[0]) - 1, num=m, dtype=np.int64)\n",
    "            return np.linalg.norm(np.asarray(x[idx], dtype=np.float32), axis=1).astype(np.float32)\n",
    "        tr_norm = _sample_norms(x_tr)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(tr_norm, bins=60, alpha=0.6, label=f'train (n={len(tr_norm)})')\n",
    "        if x_te is not None:\n",
    "            te_norm = _sample_norms(x_te)\n",
    "            plt.hist(te_norm, bins=60, alpha=0.6, label=f'test (n={len(te_norm)})')\n",
    "        plt.title(f'{name}: L2 norm distribution (sampled)')\n",
    "        plt.xlabel('L2 norm')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        def _stats(a: np.ndarray) -> dict:\n",
    "            if a.size == 0:\n",
    "                return {}\n",
    "            return {\n",
    "                'min': float(np.min(a)),\n",
    "                'p50': float(np.median(a)),\n",
    "                'p95': float(np.quantile(a, 0.95)),\n",
    "                'max': float(np.max(a)),\n",
    "            }\n",
    "        print(f'[{name}] train norm stats:', _stats(tr_norm))\n",
    "        if x_te is not None:\n",
    "            print(f'[{name}] test norm stats:', _stats(te_norm))\n",
    "    _plot_embed_norms('ProtT5', t5_train_path, t5_test_path)\n",
    "except Exception as e:\n",
    "    print('T5 embedding diagnostics skipped:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d11c2e",
   "metadata": {
    "id": "01d11c2e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 12 - Solution: 3b. PHASE 1: EMBEDDINGS GENERATION (ESM2 only)\n",
    "# 3b. PHASE 1: EMBEDDINGS GENERATION (ESM2 only)\n",
    "# ==============================================\n",
    "# HARDWARE: GPU recommended\n",
    "# ==============================================\n",
    "# Split from T5 so you can run each independently on Kaggle.\n",
    "import os\n",
    "\n",
    "# IMPORTANT: pushing checkpoints republishes a whole Kaggle Dataset version (but only the staged flat files we include),\n",
    "# which can mean re-uploading multi-GB zips even when embeddings already exist.\n",
    "esm2_train_path = WORK_ROOT / 'features' / 'train_embeds_esm2.npy'\n",
    "esm2_test_path = WORK_ROOT / 'features' / 'test_embeds_esm2.npy'\n",
    "esm2_3b_train_path = WORK_ROOT / 'features' / 'train_embeds_esm2_3b.npy'\n",
    "esm2_3b_test_path = WORK_ROOT / 'features' / 'test_embeds_esm2_3b.npy'\n",
    "# Ankh model + output naming\n",
    "# - Default model keeps backwards-compatible filenames so downstream cells still work.\n",
    "# - Non-default model writes to a new filename (no overwriting of existing Ankh artefacts).\n",
    "DEFAULT_ANKH_MODEL = 'ElnaggarLab/ankh-large'\n",
    "ANKH_MODEL = os.getenv('CAFA_ANKH_MODEL', DEFAULT_ANKH_MODEL).strip()\n",
    "\n",
    "def _safe_slug(s: str, max_len: int = 40) -> str:\n",
    "    s = (s or '').strip().lower()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        if ch.isalnum():\n",
    "            out.append(ch)\n",
    "        else:\n",
    "            out.append('_')\n",
    "    slug = ''.join(out).strip('_')\n",
    "    while '__' in slug:\n",
    "        slug = slug.replace('__', '_')\n",
    "    return slug[:max_len] if slug else 'unk'\n",
    "ankh_suffix = '' if ANKH_MODEL == DEFAULT_ANKH_MODEL else f'__{_safe_slug(ANKH_MODEL)}'\n",
    "ankh_train_path = WORK_ROOT / 'features' / f'train_embeds_ankh{ankh_suffix}.npy'\n",
    "ankh_test_path = WORK_ROOT / 'features' / f'test_embeds_ankh{ankh_suffix}.npy'\n",
    "has_test = (WORK_ROOT / 'parsed' / 'test_seq.feather').exists()\n",
    "# These are core modalities for this project. We still skip compute if artefacts exist (unless forced),\n",
    "# but they are not treated as optional downstream.\n",
    "COMPUTE_ESM2 = True  # ESM2-650M\n",
    "COMPUTE_ESM2_3B = True  # ESM2-3B\n",
    "COMPUTE_ANKH = True  # Ankh\n",
    "# Granular \"needed\" flags so we can resume mid-cell without recomputing train.\n",
    "esm2_train_needed = COMPUTE_ESM2 and (FORCE_REBUILD or (not esm2_train_path.exists()))\n",
    "esm2_test_needed = COMPUTE_ESM2 and has_test and (FORCE_REBUILD or (not esm2_test_path.exists()))\n",
    "esm2_3b_train_needed = COMPUTE_ESM2_3B and (FORCE_REBUILD or (not esm2_3b_train_path.exists()))\n",
    "esm2_3b_test_needed = COMPUTE_ESM2_3B and has_test and (FORCE_REBUILD or (not esm2_3b_test_path.exists()))\n",
    "ankh_train_needed = COMPUTE_ANKH and (FORCE_REBUILD or (not ankh_train_path.exists()))\n",
    "ankh_test_needed = COMPUTE_ANKH and has_test and (FORCE_REBUILD or (not ankh_test_path.exists()))\n",
    "# If artefacts already exist, skip compute unless forced.\n",
    "if COMPUTE_ESM2 and (not esm2_train_needed) and (not esm2_test_needed):\n",
    "    COMPUTE_ESM2 = False\n",
    "    if not PUSH_EXISTING_CHECKPOINTS:\n",
    "        pass\n",
    "    if PUSH_EXISTING_CHECKPOINTS and 'STORE' in globals() and STORE is not None:\n",
    "        try:\n",
    "            STORE.maybe_push('stage_06_embeddings_esm2_train', [esm2_train_path], note='ESM2-650M mean-pool embeddings (train only, existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06_embeddings_esm2_train: {e}')\n",
    "        if esm2_test_path.exists():\n",
    "            try:\n",
    "                STORE.maybe_push('stage_06_embeddings_esm2_test', [esm2_test_path], note='ESM2-650M mean-pool embeddings (test only, existing)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06_embeddings_esm2_test: {e}')\n",
    "        paths = [esm2_train_path]\n",
    "        if esm2_test_path.exists():\n",
    "            paths.append(esm2_test_path)\n",
    "        try:\n",
    "            STORE.maybe_push('stage_06_embeddings_esm2', paths, note='ESM2-650M mean-pool embeddings (existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06_embeddings_esm2: {e}')\n",
    "if COMPUTE_ESM2_3B and (not esm2_3b_train_needed) and (not esm2_3b_test_needed):\n",
    "    COMPUTE_ESM2_3B = False\n",
    "    if not PUSH_EXISTING_CHECKPOINTS:\n",
    "        pass\n",
    "    if PUSH_EXISTING_CHECKPOINTS and 'STORE' in globals() and STORE is not None:\n",
    "        try:\n",
    "            STORE.maybe_push('stage_06b_embeddings_esm2_3b_train', [esm2_3b_train_path], note='ESM2-3B mean-pool embeddings (train only, existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06b_embeddings_esm2_3b_train: {e}')\n",
    "        if esm2_3b_test_path.exists():\n",
    "            try:\n",
    "                STORE.maybe_push('stage_06b_embeddings_esm2_3b_test', [esm2_3b_test_path], note='ESM2-3B mean-pool embeddings (test only, existing)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06b_embeddings_esm2_3b_test: {e}')\n",
    "        paths = [esm2_3b_train_path]\n",
    "        if esm2_3b_test_path.exists():\n",
    "            paths.append(esm2_3b_test_path)\n",
    "        try:\n",
    "            STORE.maybe_push('stage_06b_embeddings_esm2_3b', paths, note='ESM2-3B mean-pool embeddings (existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06b_embeddings_esm2_3b: {e}')\n",
    "if COMPUTE_ANKH and (not ankh_train_needed) and (not ankh_test_needed):\n",
    "    COMPUTE_ANKH = False\n",
    "    if not PUSH_EXISTING_CHECKPOINTS:\n",
    "        pass\n",
    "    if PUSH_EXISTING_CHECKPOINTS and 'STORE' in globals() and STORE is not None:\n",
    "        stage_tag = 'ankh' if ANKH_MODEL == DEFAULT_ANKH_MODEL else f\"ankh_{_safe_slug(ANKH_MODEL)}\"\n",
    "        try:\n",
    "            STORE.maybe_push(f'stage_06c_embeddings_{stage_tag}_train', [ankh_train_path], note=f'Ankh mean-pool embeddings ({ANKH_MODEL}) (train only, existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06c embeddings train: {e}')\n",
    "        if ankh_test_path.exists():\n",
    "            try:\n",
    "                STORE.maybe_push(f'stage_06c_embeddings_{stage_tag}_test', [ankh_test_path], note=f'Ankh mean-pool embeddings ({ANKH_MODEL}) (test only, existing)')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06c embeddings test: {e}')\n",
    "        # Combined stage includes all Ankh embedding variants present so newly-created files can't be missed.\n",
    "        feat_dir = (WORK_ROOT / 'features')\n",
    "        paths = []\n",
    "        paths += sorted(feat_dir.glob('train_embeds_ankh*.npy'))\n",
    "        paths += sorted(feat_dir.glob('test_embeds_ankh*.npy'))\n",
    "        if ankh_train_path not in paths:\n",
    "            paths.append(ankh_train_path)\n",
    "        if ankh_test_path.exists() and ankh_test_path not in paths:\n",
    "            paths.append(ankh_test_path)\n",
    "        try:\n",
    "            STORE.maybe_push(f'stage_06c_embeddings_{stage_tag}', paths, note=f'Ankh mean-pool embeddings ({ANKH_MODEL}) (existing)')\n",
    "        except Exception as e:\n",
    "            print(f'WARN: failed to push stage_06c embeddings combined: {e}')\n",
    "if COMPUTE_ESM2 or COMPUTE_ESM2_3B or COMPUTE_ANKH:\n",
    "    import gc\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    from transformers import EsmTokenizer, EsmModel, AutoTokenizer, AutoModel\n",
    "    from tqdm.auto import tqdm\n",
    "    import contextlib\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    amp_ctx = torch.amp.autocast('cuda') if device.type == 'cuda' else contextlib.nullcontext()\n",
    "    def _mean_pool(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        mask = attention_mask.unsqueeze(-1).to(last_hidden_state.dtype)\n",
    "        x = last_hidden_state * mask\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "        return x.sum(dim=1) / denom\n",
    "    def get_esm2_model(model_name: str):\n",
    "        print(f\"Loading ESM2 model: {model_name}\")\n",
    "        tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "        model = EsmModel.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        return tokenizer, model\n",
    "    def generate_embeddings_esm2(model, tokenizer, sequences, batch_size=16, max_len=1024):\n",
    "        seq_lens = [len(s) for s in sequences]\n",
    "        sort_idx = np.argsort(seq_lens)[::-1]\n",
    "        sorted_seqs = [sequences[i] for i in sort_idx]\n",
    "        embeddings_list = []\n",
    "        for i in tqdm(range(0, len(sorted_seqs), batch_size), desc=\"Embedding ESM2 (Smart Batch)\"):\n",
    "            batch_seqs = sorted_seqs[i : i + batch_size]\n",
    "            ids = tokenizer.batch_encode_plus(\n",
    "                batch_seqs,\n",
    "                add_special_tokens=True,\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "            with torch.no_grad():\n",
    "                with amp_ctx:\n",
    "                    output = model(input_ids=ids['input_ids'], attention_mask=ids['attention_mask'])\n",
    "            pooled = _mean_pool(output.last_hidden_state.float(), ids['attention_mask']).detach().cpu().numpy().astype(np.float32)\n",
    "            embeddings_list.append(pooled)\n",
    "            del ids, output, pooled\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        sorted_embeddings = np.vstack(embeddings_list)\n",
    "        original_order_embeddings = np.zeros_like(sorted_embeddings)\n",
    "        original_order_embeddings[sort_idx] = sorted_embeddings\n",
    "        return original_order_embeddings\n",
    "    def generate_embeddings_ankh(model_name: str, sequences, batch_size=2, max_len=1024, trust_remote_code=True):\n",
    "        print(f\"Loading Ankh model: {model_name} (trust_remote_code={trust_remote_code})\")\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)\n",
    "        model = AutoModel.from_pretrained(model_name, trust_remote_code=trust_remote_code).to(device)\n",
    "        model.eval()\n",
    "        # Some Ankh checkpoints load as encoder-decoder (T5-like). For embeddings we only need the encoder.\n",
    "        is_enc_dec = bool(getattr(getattr(model, 'config', None), 'is_encoder_decoder', False))\n",
    "        if is_enc_dec and not hasattr(model, 'encoder'):\n",
    "            raise RuntimeError(f'Ankh model appears encoder-decoder but has no .encoder: {type(model)}')\n",
    "        encoder = model.encoder if is_enc_dec else model\n",
    "        seq_lens = [len(s) for s in sequences]\n",
    "        sort_idx = np.argsort(seq_lens)[::-1]\n",
    "        sorted_seqs = [sequences[i] for i in sort_idx]\n",
    "        outs = []\n",
    "        amp_enabled = device.type == 'cuda'\n",
    "        for i in tqdm(range(0, len(sorted_seqs), batch_size), desc=\"Embedding Ankh (Smart Batch)\"):\n",
    "            batch = sorted_seqs[i : i + batch_size]\n",
    "            batch = [s.replace('U','X').replace('Z','X').replace('O','X').replace('B','X') for s in batch]\n",
    "            batch = [' '.join(list(s)) for s in batch]\n",
    "            ids = tok(batch, add_special_tokens=True, padding=True, truncation=True, max_length=max_len, return_tensors='pt')\n",
    "            ids = {k: v.to(device) for k, v in ids.items()}\n",
    "            if ids.get('attention_mask') is None:\n",
    "                raise RuntimeError('Tokenizer did not return attention_mask for Ankh')\n",
    "            if (ids['attention_mask'].sum(dim=1) == 0).any():\n",
    "                raise RuntimeError(f'Ankh tokenisation produced an all-zero attention_mask (batch_start={i}).')\n",
    "            def _forward(use_amp: bool):\n",
    "                ctx = (torch.amp.autocast('cuda') if (use_amp and device.type == 'cuda') else torch.autocast('cpu', enabled=False))\n",
    "                with torch.no_grad():\n",
    "                    with ctx:\n",
    "                        return encoder(input_ids=ids['input_ids'], attention_mask=ids.get('attention_mask'))\n",
    "            out = _forward(amp_enabled)\n",
    "            last = getattr(out, 'last_hidden_state', None)\n",
    "            if last is None and isinstance(out, (tuple, list)) and len(out) > 0:\n",
    "                last = out[0]\n",
    "            if last is None:\n",
    "                raise RuntimeError(f'Model {model_name} did not return last_hidden_state')\n",
    "            pooled_t = _mean_pool(last.float(), ids['attention_mask'])\n",
    "            if not bool(torch.isfinite(pooled_t).all().item()):\n",
    "                if amp_enabled:\n",
    "                    print(f'WARNING: Non-finite Ankh embeddings under AMP at batch_start={i}; retrying without AMP.')\n",
    "                    amp_enabled = False\n",
    "                    out = _forward(False)\n",
    "                    last = getattr(out, 'last_hidden_state', None)\n",
    "                    if last is None and isinstance(out, (tuple, list)) and len(out) > 0:\n",
    "                        last = out[0]\n",
    "                    if last is None:\n",
    "                        raise RuntimeError(f'Model {model_name} did not return last_hidden_state')\n",
    "                    pooled_t = _mean_pool(last.float(), ids['attention_mask'])\n",
    "                if not bool(torch.isfinite(pooled_t).all().item()):\n",
    "                    raise RuntimeError(f'Ankh produced non-finite embeddings even without AMP (batch_start={i}).')\n",
    "            pooled = pooled_t.detach().cpu().numpy().astype(np.float32)\n",
    "            outs.append(pooled)\n",
    "            del ids, out, last, pooled_t, pooled\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "        sorted_embeddings = np.vstack(outs)\n",
    "        original_order_embeddings = np.zeros_like(sorted_embeddings)\n",
    "        original_order_embeddings[sort_idx] = sorted_embeddings\n",
    "        if not np.isfinite(original_order_embeddings).all():\n",
    "            nan = int(np.isnan(original_order_embeddings).sum())\n",
    "            inf = int(np.isinf(original_order_embeddings).sum())\n",
    "            raise ValueError(f'Ankh embeddings are non-finite (nan={nan}, inf={inf}). Refusing to save.')\n",
    "        del model, tok\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        return original_order_embeddings\n",
    "    # Only load sequences if we still need to compute something.\n",
    "    if esm2_train_needed or esm2_test_needed or esm2_3b_train_needed or esm2_3b_test_needed or ankh_train_needed or ankh_test_needed:\n",
    "        print(\"Loading sequences...\")\n",
    "        train_df = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')\n",
    "        test_df = None\n",
    "        if (WORK_ROOT / 'parsed' / 'test_seq.feather').exists():\n",
    "            test_df = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')\n",
    "        train_seqs = train_df['sequence'].tolist()\n",
    "        test_seqs = test_df['sequence'].tolist() if test_df is not None else None\n",
    "    else:\n",
    "        train_df = None\n",
    "        test_df = None\n",
    "        train_seqs = None\n",
    "        test_seqs = None\n",
    "    if COMPUTE_ESM2:\n",
    "        tokenizer, model = get_esm2_model('facebook/esm2_t33_650M_UR50D')\n",
    "        if esm2_train_needed:\n",
    "            print(f\"Generating Train Embeddings ESM2-650M ({len(train_seqs)})...\")\n",
    "            train_emb = generate_embeddings_esm2(model, tokenizer, train_seqs, batch_size=16)\n",
    "            np.save(esm2_train_path, train_emb)\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "                try:\n",
    "                    STORE.maybe_push('stage_06_embeddings_esm2_train', [esm2_train_path], note='ESM2-650M mean-pool embeddings (train only)')\n",
    "                except Exception as e:\n",
    "                    print(f'WARN: failed to push stage_06_embeddings_esm2_train: {e}')\n",
    "            del train_emb\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"Train embeddings already exist at {esm2_train_path}; skipping train compute.\")\n",
    "        if esm2_test_needed and test_seqs is not None:\n",
    "            print(f\"Generating Test Embeddings ESM2-650M ({len(test_seqs)})...\")\n",
    "            test_emb = generate_embeddings_esm2(model, tokenizer, test_seqs, batch_size=16)\n",
    "            np.save(esm2_test_path, test_emb)\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "                try:\n",
    "                    STORE.maybe_push('stage_06_embeddings_esm2_test', [esm2_test_path], note='ESM2-650M mean-pool embeddings (test only)')\n",
    "                except Exception as e:\n",
    "                    print(f'WARN: failed to push stage_06_embeddings_esm2_test: {e}')\n",
    "            del test_emb\n",
    "        else:\n",
    "            if has_test:\n",
    "                print(f\"Test embeddings already exist at {esm2_test_path}; skipping test compute.\")\n",
    "            else:\n",
    "                print(\"No test sequences found; skipping test embedding.\")\n",
    "        del model, tokenizer\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"ESM2-650M embeddings ready.\")\n",
    "        # Backwards-compatible combined stage.\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            paths = [esm2_train_path]\n",
    "            if esm2_test_path.exists():\n",
    "                paths.append(esm2_test_path)\n",
    "            try:\n",
    "                STORE.maybe_push('stage_06_embeddings_esm2', paths, note='ESM2-650M mean-pool embeddings')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06_embeddings_esm2: {e}')\n",
    "    else:\n",
    "        print(\"Skipping ESM2-650M embedding generation (COMPUTE_ESM2=False).\")\n",
    "    if COMPUTE_ESM2_3B:\n",
    "        tokenizer, model = get_esm2_model('facebook/esm2_t36_3B_UR50D')\n",
    "        if esm2_3b_train_needed:\n",
    "            print(f\"Generating Train Embeddings ESM2-3B ({len(train_seqs)})...\")\n",
    "            train_emb = generate_embeddings_esm2(model, tokenizer, train_seqs, batch_size=2)\n",
    "            np.save(esm2_3b_train_path, train_emb)\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "                try:\n",
    "                    STORE.maybe_push('stage_06b_embeddings_esm2_3b_train', [esm2_3b_train_path], note='ESM2-3B mean-pool embeddings (train only)')\n",
    "                except Exception as e:\n",
    "                    print(f'WARN: failed to push stage_06b_embeddings_esm2_3b_train: {e}')\n",
    "            del train_emb\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"Train embeddings already exist at {esm2_3b_train_path}; skipping train compute.\")\n",
    "        if esm2_3b_test_needed and test_seqs is not None:\n",
    "            print(f\"Generating Test Embeddings ESM2-3B ({len(test_seqs)})...\")\n",
    "            test_emb = generate_embeddings_esm2(model, tokenizer, test_seqs, batch_size=2)\n",
    "            np.save(esm2_3b_test_path, test_emb)\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "                try:\n",
    "                    STORE.maybe_push('stage_06b_embeddings_esm2_3b_test', [esm2_3b_test_path], note='ESM2-3B mean-pool embeddings (test only)')\n",
    "                except Exception as e:\n",
    "                    print(f'WARN: failed to push stage_06b_embeddings_esm2_3b_test: {e}')\n",
    "            del test_emb\n",
    "        else:\n",
    "            if has_test:\n",
    "                print(f\"Test embeddings already exist at {esm2_3b_test_path}; skipping test compute.\")\n",
    "            else:\n",
    "                print(\"No test sequences found; skipping test embedding.\")\n",
    "        del model, tokenizer\n",
    "        gc.collect()\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"ESM2-3B embeddings ready.\")\n",
    "        # Backwards-compatible combined stage.\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            paths = [esm2_3b_train_path]\n",
    "            if esm2_3b_test_path.exists():\n",
    "                paths.append(esm2_3b_test_path)\n",
    "            try:\n",
    "                STORE.maybe_push('stage_06b_embeddings_esm2_3b', paths, note='ESM2-3B mean-pool embeddings')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06b_embeddings_esm2_3b: {e}')\n",
    "    else:\n",
    "        print(\"Skipping ESM2-3B embedding generation (COMPUTE_ESM2_3B=False).\")\n",
    "    if COMPUTE_ANKH:\n",
    "        stage_tag = 'ankh' if ANKH_MODEL == DEFAULT_ANKH_MODEL else f\"ankh_{_safe_slug(ANKH_MODEL)}\"\n",
    "        if ankh_train_needed:\n",
    "            print(f\"Generating Train Embeddings Ankh ({ANKH_MODEL}) ({len(train_seqs)})...\")\n",
    "            train_emb = generate_embeddings_ankh(ANKH_MODEL, train_seqs, batch_size=2, trust_remote_code=True)\n",
    "            np.save(ankh_train_path, train_emb)\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "                try:\n",
    "                    STORE.maybe_push(f'stage_06c_embeddings_{stage_tag}_train', [ankh_train_path], note=f'Ankh mean-pool embeddings ({ANKH_MODEL}) (train only)')\n",
    "                except Exception as e:\n",
    "                    print(f'WARN: failed to push stage_06c embeddings train: {e}')\n",
    "            del train_emb\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(f\"Train embeddings already exist at {ankh_train_path}; skipping train compute.\")\n",
    "        if ankh_test_needed and test_seqs is not None:\n",
    "            print(f\"Generating Test Embeddings Ankh ({ANKH_MODEL}) ({len(test_seqs)})...\")\n",
    "            test_emb = generate_embeddings_ankh(ANKH_MODEL, test_seqs, batch_size=2, trust_remote_code=True)\n",
    "            np.save(ankh_test_path, test_emb)\n",
    "            if 'STORE' in globals() and STORE is not None:\n",
    "                try:\n",
    "                    STORE.maybe_push(f'stage_06c_embeddings_{stage_tag}_test', [ankh_test_path], note=f'Ankh mean-pool embeddings ({ANKH_MODEL}) (test only)')\n",
    "                except Exception as e:\n",
    "                    print(f'WARN: failed to push stage_06c embeddings test: {e}')\n",
    "            del test_emb\n",
    "            gc.collect()\n",
    "        else:\n",
    "            if has_test:\n",
    "                print(f\"Test embeddings already exist at {ankh_test_path}; skipping test compute.\")\n",
    "            else:\n",
    "                print(\"No test sequences found; skipping test embedding.\")\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Ankh embeddings ready.\")\n",
    "        # Combined stage includes all Ankh embedding variants present so newly-created files can't be missed.\n",
    "        if 'STORE' in globals() and STORE is not None:\n",
    "            feat_dir = (WORK_ROOT / 'features')\n",
    "            paths = []\n",
    "            paths += sorted(feat_dir.glob('train_embeds_ankh*.npy'))\n",
    "            paths += sorted(feat_dir.glob('test_embeds_ankh*.npy'))\n",
    "            if ankh_train_path not in paths:\n",
    "                paths.append(ankh_train_path)\n",
    "            if ankh_test_path.exists() and ankh_test_path not in paths:\n",
    "                paths.append(ankh_test_path)\n",
    "            try:\n",
    "                STORE.maybe_push(f'stage_06c_embeddings_{stage_tag}', paths, note=f'Ankh mean-pool embeddings ({ANKH_MODEL})')\n",
    "            except Exception as e:\n",
    "                print(f'WARN: failed to push stage_06c embeddings combined: {e}')\n",
    "    else:\n",
    "        print(\"Skipping Ankh embedding generation (COMPUTE_ANKH=False).\")\n",
    "    del train_df, test_df, train_seqs, test_seqs\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"Skipping embeddings generation (all COMPUTE_* flags False).\")\n",
    "# Diagnostics: embedding norms (train vs test; sampled)\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    def _plot_embed_norms(name: str, train_path: Path, test_path: Path):\n",
    "        if not Path(train_path).exists():\n",
    "            print(f'[{name}] missing train embeddings: {train_path}')\n",
    "            return\n",
    "        x_tr = np.load(train_path, mmap_mode='r')\n",
    "        x_te = np.load(test_path, mmap_mode='r') if Path(test_path).exists() else None\n",
    "        def _sample_norms(x: np.ndarray, n: int = 20000) -> np.ndarray:\n",
    "            m = min(int(x.shape[0]), int(n))\n",
    "            if m <= 0:\n",
    "                return np.zeros((0,), dtype=np.float32)\n",
    "            idx = np.linspace(0, int(x.shape[0]) - 1, num=m, dtype=np.int64)\n",
    "            return np.linalg.norm(np.asarray(x[idx], dtype=np.float32), axis=1).astype(np.float32)\n",
    "        tr_norm = _sample_norms(x_tr)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(tr_norm, bins=60, alpha=0.6, label=f'train (n={len(tr_norm)})')\n",
    "        if x_te is not None:\n",
    "            te_norm = _sample_norms(x_te)\n",
    "            plt.hist(te_norm, bins=60, alpha=0.6, label=f'test (n={len(te_norm)})')\n",
    "        plt.title(f'{name}: L2 norm distribution (sampled)')\n",
    "        plt.xlabel('L2 norm')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        def _stats(a: np.ndarray) -> dict:\n",
    "            if a.size == 0:\n",
    "                return {}\n",
    "            return {\n",
    "                'min': float(np.min(a)),\n",
    "                'p50': float(np.median(a)),\n",
    "                'p95': float(np.quantile(a, 0.95)),\n",
    "                'max': float(np.max(a)),\n",
    "            }\n",
    "        print(f'[{name}] train norm stats:', _stats(tr_norm))\n",
    "        if x_te is not None:\n",
    "            print(f'[{name}] test norm stats:', _stats(te_norm))\n",
    "    _plot_embed_norms('ESM2-650M', esm2_train_path, esm2_test_path)\n",
    "    _plot_embed_norms('ESM2-3B', esm2_3b_train_path, esm2_3b_test_path)\n",
    "    _plot_embed_norms('Ankh', ankh_train_path, ankh_test_path)\n",
    "except Exception as e:\n",
    "    print('Embedding diagnostics skipped:', repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547acee5",
   "metadata": {
    "id": "547acee5",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 13 - Solution: 4. PHASE 2: LEVEL-1 MODELS (DIVERSE ENSEMBLE)\n",
    "# 4. PHASE 2: LEVEL-1 MODELS (DIVERSE ENSEMBLE)\n",
    "# =============================================\n",
    "# HARDWARE: GPU (32GB+ recommended for full run)\n",
    "# =============================================\n",
    "# We train a diverse set of models:\n",
    "# 1. Logistic Regression (Baseline)\n",
    "# 2. Py-Boost (GBDT) - Requires 'py-boost' package\n",
    "# 3. DNN Ensemble (Deep Learning)\n",
    "TRAIN_LEVEL1 = True\n",
    "if TRAIN_LEVEL1:\n",
    "    import joblib\n",
    "    import json\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import f1_score\n",
    "    # -----------------------------\n",
    "    # Load targets + ids\n",
    "    # -----------------------------\n",
    "    print(\"Loading targets...\")\n",
    "    train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "    train_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id'].astype(str)\n",
    "    # Target Matrix Construction (Champion Strategy: 13,500 Terms)\n",
    "    # - 10,000 BP + 2,000 MF + 1,500 CC\n",
    "    # - Robust to train_terms.aspect encoding differences (namespace strings vs BP/MF/CC)\n",
    "    print(\"Selecting Top-K terms per aspect (Champion Strategy)...\")\n",
    "    # FIX: Clean IDs in train_ids to match EntryID format (some artefacts store pipe-wrapped UniProt IDs)\n",
    "    train_ids_clean = train_ids.str.extract(r'\\|(.*?)\\|')[0]\n",
    "    train_ids_clean = train_ids_clean.fillna(train_ids)\n",
    "    try:\n",
    "        import obonet\n",
    "        # Robust OBO Path Search\n",
    "        possible_paths = [\n",
    "            WORK_ROOT / 'go-basic.obo',\n",
    "            WORK_ROOT.parent / 'go-basic.obo',\n",
    "            Path('go-basic.obo'),\n",
    "            Path('Train/go-basic.obo'),\n",
    "            Path('../Train/go-basic.obo'),\n",
    "            Path('/content/cafa6_data/Train/go-basic.obo')\n",
    "        ]\n",
    "        obo_path = None\n",
    "        for p in possible_paths:\n",
    "            if p.exists():\n",
    "                obo_path = p\n",
    "                break\n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError(f\"CRITICAL: go-basic.obo not found. Searched: {[str(p) for p in possible_paths]}\")\n",
    "        # Export for other cells\n",
    "        global PATH_GO_OBO\n",
    "        PATH_GO_OBO = obo_path\n",
    "        print(f\"Global PATH_GO_OBO set to: {PATH_GO_OBO}\")\n",
    "        print(f\"Loading OBO from {obo_path}...\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        term_to_ns = {node: data.get('namespace', 'unknown') for node, data in graph.nodes(data=True)}\n",
    "        # Keep compatibility with downstream diagnostic code that expects go_namespaces\n",
    "        go_namespaces = term_to_ns\n",
    "        ns_map = {'biological_process': 'BP', 'molecular_function': 'MF', 'cellular_component': 'CC'}\n",
    "        # Normalise any existing aspect column (some artefacts store full namespace strings)\n",
    "        aspect_aliases = {\n",
    "            'biological_process': 'BP', 'molecular_function': 'MF', 'cellular_component': 'CC',\n",
    "            'BP': 'BP', 'MF': 'MF', 'CC': 'CC'\n",
    "        }\n",
    "        if 'aspect' in train_terms.columns:\n",
    "            train_terms['aspect'] = train_terms['aspect'].map(lambda a: aspect_aliases.get(str(a), 'UNK'))\n",
    "        else:\n",
    "            train_terms['aspect'] = train_terms['term'].map(lambda t: ns_map.get(term_to_ns.get(t), 'UNK'))\n",
    "    except ImportError:\n",
    "        raise RuntimeError('obonet not installed. Please install it.')\n",
    "    term_counts = train_terms.groupby(['aspect', 'term']).size().reset_index(name='count')\n",
    "    targets_bp = term_counts[term_counts['aspect'] == 'BP'].nlargest(10000, 'count')['term'].tolist()\n",
    "    targets_mf = term_counts[term_counts['aspect'] == 'MF'].nlargest(2000, 'count')['term'].tolist()\n",
    "    targets_cc = term_counts[term_counts['aspect'] == 'CC'].nlargest(1500, 'count')['term'].tolist()\n",
    "    # Guardrail: avoid silently switching target strategy due to aspect encoding mismatch\n",
    "    ALLOW_GLOBAL_FALLBACK = False\n",
    "    if len(targets_bp) == 0 and len(targets_mf) == 0 and len(targets_cc) == 0:\n",
    "        aspect_vc = train_terms['aspect'].value_counts().to_dict() if 'aspect' in train_terms.columns else {}\n",
    "        msg = f\"No BP/MF/CC aspect split found after normalisation. aspect_vc={aspect_vc}. This would fall back to global Top-13,500; set ALLOW_GLOBAL_FALLBACK=True to override.\"\n",
    "        if ALLOW_GLOBAL_FALLBACK:\n",
    "            print('  [WARNING] ' + msg)\n",
    "            top_terms = train_terms['term'].value_counts().head(13500).index.tolist()\n",
    "        else:\n",
    "            raise RuntimeError(msg)\n",
    "    else:\n",
    "        # Stable, deterministic ordering: BP then MF then CC with de-dup preserving order\n",
    "        top_terms = []\n",
    "        seen = set()\n",
    "        for t in (targets_bp + targets_mf + targets_cc):\n",
    "            if t not in seen:\n",
    "                top_terms.append(t)\n",
    "                seen.add(t)\n",
    "        print(f\"  Selected: {len(targets_bp)} BP + {len(targets_mf)} MF + {len(targets_cc)} CC\")\n",
    "    # Persist label contract for downstream stages\n",
    "    top_terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "    top_terms_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(top_terms_path, 'w') as f:\n",
    "        json.dump(list(top_terms), f)\n",
    "    print('Saved: top_terms_13500.json')\n",
    "    train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids_clean, fill_value=0)\n",
    "    Y = Y_df.values.astype(np.float32)\n",
    "    print(f\"Targets: Y={Y.shape}\")\n",
    "    # -----------------------------\n",
    "    # -----------------------------\n",
    "    # Feature loading (multimodal)\n",
    "    # -----------------------------\n",
    "    print(\"Loading multimodal features...\")\n",
    "    FEAT_DIR = WORK_ROOT / 'features'\n",
    "    def _load_pair(stem):\n",
    "        tr = FEAT_DIR / f'train_embeds_{stem}.npy'\n",
    "        te = FEAT_DIR / f'test_embeds_{stem}.npy'\n",
    "        if tr.exists() and te.exists():\n",
    "            return np.load(tr).astype(np.float32), np.load(te).astype(np.float32)\n",
    "        return None, None\n",
    "    features_train = {}\n",
    "    features_test = {}\n",
    "    # Core modalities (NOT OPTIONAL): T5, ESM2-650M, ESM2-3B, Ankh, TF-IDF text, taxonomy\n",
    "    for stem, key in [\n",
    "        ('t5', 't5'),\n",
    "        ('esm2', 'esm2_650m'),\n",
    "        ('esm2_3b', 'esm2_3b'),\n",
    "        ('ankh', 'ankh'),\n",
    "        ('text', 'text'),\n",
    "    ]:\n",
    "        a_tr, a_te = _load_pair(stem)\n",
    "        if a_tr is not None:\n",
    "            features_train[key] = a_tr\n",
    "            features_test[key] = a_te\n",
    "    # Taxonomy (encode as one-hot / bag-of-taxa)\n",
    "    taxa_train_path = WORK_ROOT / 'parsed' / 'train_taxa.feather'\n",
    "    taxa_test_path = WORK_ROOT / 'parsed' / 'test_taxa.feather'\n",
    "    if taxa_train_path.exists() and taxa_test_path.exists():\n",
    "        from sklearn.preprocessing import OneHotEncoder\n",
    "        tax_tr = pd.read_feather(taxa_train_path).astype({'id': str})\n",
    "        tax_te = pd.read_feather(taxa_test_path).astype({'id': str})\n",
    "        tax_tr = tax_tr.set_index('id').reindex(train_ids, fill_value=0).reset_index()\n",
    "        tax_te = tax_te.set_index('id').reindex(test_ids, fill_value=0).reset_index()\n",
    "        enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32)\n",
    "        enc.fit(pd.concat([tax_tr[['taxon_id']], tax_te[['taxon_id']]], axis=0))\n",
    "        X_tax_tr = enc.transform(tax_tr[['taxon_id']]).astype(np.float32)\n",
    "        X_tax_te = enc.transform(tax_te[['taxon_id']]).astype(np.float32)\n",
    "        features_train['taxa'] = X_tax_tr\n",
    "        features_test['taxa'] = X_tax_te\n",
    "        print(f\"Taxa features: train={X_tax_tr.shape} test={X_tax_te.shape}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Missing required taxonomy artefacts: parsed/train_taxa.feather and parsed/test_taxa.feather\")\n",
    "    # Sanity checks\n",
    "    n_train = len(train_ids)\n",
    "    n_test = len(test_ids)\n",
    "    for k, v in features_train.items():\n",
    "        if v.shape[0] != n_train:\n",
    "            raise ValueError(f\"Feature {k} train rows mismatch: {v.shape[0]} vs {n_train}\")\n",
    "    for k, v in features_test.items():\n",
    "        if v.shape[0] != n_test:\n",
    "            raise ValueError(f\"Feature {k} test rows mismatch: {v.shape[0]} vs {n_test}\")\n",
    "    required_keys = ['t5', 'esm2_650m', 'esm2_3b', 'ankh', 'text', 'taxa']\n",
    "    missing_keys = [k for k in required_keys if k not in features_train]\n",
    "    if missing_keys:\n",
    "        exp = {\n",
    "            't5': ('features/train_embeds_t5.npy', 'features/test_embeds_t5.npy'),\n",
    "            'esm2_650m': ('features/train_embeds_esm2.npy', 'features/test_embeds_esm2.npy'),\n",
    "            'esm2_3b': ('features/train_embeds_esm2_3b.npy', 'features/test_embeds_esm2_3b.npy'),\n",
    "            'ankh': ('features/train_embeds_ankh.npy', 'features/test_embeds_ankh.npy'),\n",
    "            'text': ('features/train_embeds_text.npy', 'features/test_embeds_text.npy'),\n",
    "            'taxa': ('parsed/train_taxa.feather', 'parsed/test_taxa.feather'),\n",
    "        }\n",
    "        msg = ['Missing required modalities (not optional):']\n",
    "        for k in missing_keys:\n",
    "            a, b = exp.get(k, ('?', '?'))\n",
    "            msg.append(f' - {k}: expected {a} and {b}')\n",
    "        raise FileNotFoundError('\\n'.join(msg))\n",
    "    # Flat concatenation for classical models (LR/GBDT)\n",
    "    FLAT_KEYS = [k for k in ['t5', 'esm2_650m', 'esm2_3b', 'ankh', 'taxa', 'text'] if k in features_train]\n",
    "    X = np.hstack([features_train[k] for k in FLAT_KEYS]).astype(np.float32)\n",
    "    X_test = np.hstack([features_test[k] for k in FLAT_KEYS]).astype(np.float32)\n",
    "    print(f\"Flat X keys={FLAT_KEYS}\")\n",
    "    print(f\"Flat shapes: X={X.shape}, X_test={X_test.shape}\")\n",
    "    # -----------------------------\n",
    "    # CAFA-like IA-weighted diagnostic F1 (vectorised)\n",
    "    # -----------------------------\n",
    "    if 'ia' in locals():\n",
    "        ia_df = ia[['term', 'ia']].copy()\n",
    "    elif PATH_IA.exists():\n",
    "        ia_df = pd.read_csv(PATH_IA, sep='\\t', names=['term', 'ia'])\n",
    "    else:\n",
    "        ia_df = pd.DataFrame({'term': [], 'ia': []})\n",
    "    ia_map = dict(zip(ia_df['term'], ia_df['ia']))\n",
    "    def _ia_weight(term):\n",
    "        v = ia_map.get(term, 0.0)\n",
    "        if pd.isna(v):\n",
    "            return 0.0\n",
    "        return float(v)\n",
    "    weights = np.array([_ia_weight(t) for t in top_terms], dtype=np.float32)\n",
    "    ns_to_aspect = {\n",
    "        'molecular_function': 'MF',\n",
    "        'biological_process': 'BP',\n",
    "        'cellular_component': 'CC',\n",
    "    }\n",
    "    if 'go_namespaces' in locals():\n",
    "        term_aspects = np.array([ns_to_aspect.get(go_namespaces.get(t, ''), 'UNK') for t in top_terms])\n",
    "    else:\n",
    "        term_aspects = np.array(['UNK'] * len(top_terms))\n",
    "    def ia_weighted_f1(y_true, y_score, thr=0.3):\n",
    "        y_true = (y_true > 0).astype(np.int8)\n",
    "        y_pred = (y_score >= thr).astype(np.int8)\n",
    "        tp = (y_pred & y_true).sum(axis=0).astype(np.float64)\n",
    "        pred = y_pred.sum(axis=0).astype(np.float64)\n",
    "        true = y_true.sum(axis=0).astype(np.float64)\n",
    "        def _score(mask=None):\n",
    "            w = weights if mask is None else (weights * mask)\n",
    "            w_tp = float((w * tp).sum())\n",
    "            w_pred = float((w * pred).sum())\n",
    "            w_true = float((w * true).sum())\n",
    "            p = (w_tp / w_pred) if w_pred > 0 else 0.0\n",
    "            r = (w_tp / w_true) if w_true > 0 else 0.0\n",
    "            return (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "        out = {'ALL': _score(None)}\n",
    "        for asp in ['MF', 'BP', 'CC']:\n",
    "            mask = (term_aspects == asp).astype(np.float32)\n",
    "            out[asp] = _score(mask)\n",
    "        return out\n",
    "    # ------------------------------------------\n",
    "    # A. Logistic Regression (Baseline)\n",
    "    # ------------------------------------------\n",
    "    print(\"\\n--- Training Logistic Regression ---\")\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds_logreg = np.zeros(Y.shape, dtype=np.float32)\n",
    "    test_preds_logreg = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "    for fold, (idx_tr, idx_val) in enumerate(kf.split(X)):\n",
    "        print(f\"LogReg Fold {fold+1}/5\")\n",
    "        X_tr, X_val = X[idx_tr], X[idx_val]\n",
    "        Y_tr, Y_val = Y[idx_tr], Y[idx_val]\n",
    "        clf_logreg = OneVsRestClassifier(\n",
    "            LogisticRegression(max_iter=500, solver='sag', n_jobs=1, C=1.0)\n",
    "        )\n",
    "        clf_logreg.n_jobs = -1\n",
    "        clf_logreg.fit(X_tr, Y_tr)\n",
    "        val_probs = clf_logreg.predict_proba(X_val)\n",
    "        oof_preds_logreg[idx_val] = val_probs\n",
    "        test_preds_logreg += clf_logreg.predict_proba(X_test) / kf.get_n_splits()\n",
    "        val_preds = (val_probs > 0.3).astype(int)\n",
    "        f1 = f1_score(Y_val, val_preds, average='micro')\n",
    "        ia_f1 = ia_weighted_f1(Y_val, val_probs, thr=0.3)\n",
    "        print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "        print(f\"  >> Fold {fold+1} IA-F1@0.30: ALL={ia_f1['ALL']:.4f} MF={ia_f1['MF']:.4f} BP={ia_f1['BP']:.4f} CC={ia_f1['CC']:.4f}\")\n",
    "        joblib.dump(clf_logreg, WORK_ROOT / 'features' / f'level1_logreg_fold{fold}.pkl')\n",
    "    np.save(WORK_ROOT / 'features' / 'oof_pred_logreg.npy', oof_preds_logreg)\n",
    "    np.save(WORK_ROOT / 'features' / 'test_pred_logreg.npy', test_preds_logreg)\n",
    "    print(\"LogReg OOF + test preds saved.\")\n",
    "    # ------------------------------------------\n",
    "    # B. Py-Boost (GBDT)\n",
    "    # ------------------------------------------\n",
    "    try:\n",
    "        from py_boost import GradientBoosting\n",
    "        HAS_PYBOOST = True\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError('py_boost is REQUIRED for this pipeline (GBDT is mandatory). Install it with: pip install py-boost') from e\n",
    "    if HAS_PYBOOST:\n",
    "        print(\"\\n--- Training Py-Boost GBDT ---\")\n",
    "        oof_preds_gbdt = np.zeros(Y.shape, dtype=np.float32)\n",
    "        test_preds_gbdt = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf.split(X)):\n",
    "            print(f\"GBDT Fold {fold+1}/5\")\n",
    "            X_tr, X_val = X[idx_tr], X[idx_val]\n",
    "            Y_tr, Y_val = Y[idx_tr], Y[idx_val]\n",
    "            model = GradientBoosting(\n",
    "                loss='bce',\n",
    "                ntrees=1000,\n",
    "                lr=0.05,\n",
    "                max_depth=6,\n",
    "                verbose=100,\n",
    "                es=50,\n",
    "            )\n",
    "            model.fit(X_tr, Y_tr, eval_sets=[{'X': X_val, 'y': Y_val}])\n",
    "            val_probs = model.predict(X_val)\n",
    "            oof_preds_gbdt[idx_val] = val_probs\n",
    "            test_preds_gbdt += model.predict(X_test) / kf.get_n_splits()\n",
    "            val_preds = (val_probs > 0.3).astype(int)\n",
    "            f1 = f1_score(Y_val, val_preds, average='micro')\n",
    "            ia_f1 = ia_weighted_f1(Y_val, val_probs, thr=0.3)\n",
    "            print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "            print(f\"  >> Fold {fold+1} IA-F1@0.30: ALL={ia_f1['ALL']:.4f} MF={ia_f1['MF']:.4f} BP={ia_f1['BP']:.4f} CC={ia_f1['CC']:.4f}\")\n",
    "            model.save(str(WORK_ROOT / 'features' / f'level1_gbdt_fold{fold}.json'))\n",
    "        np.save(WORK_ROOT / 'features' / 'oof_pred_gbdt.npy', oof_preds_gbdt)\n",
    "        np.save(WORK_ROOT / 'features' / 'test_pred_gbdt.npy', test_preds_gbdt)\n",
    "        print(\"GBDT OOF + test preds saved.\")\n",
    "    # ------------------------------------------\n",
    "    # C. DNN Ensemble (PyTorch, IA-weighted, multi-input + multi-state)\n",
    "    # ------------------------------------------\n",
    "    print(\"\\n--- Training DNN Ensemble (IA-weighted, multimodal, multi-state) ---\")\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Build a stable per-label IA weight vector for the current TOP_K targets\n",
    "    ia_w = weights.copy()\n",
    "    ia_w = np.where(np.isfinite(ia_w) & (ia_w > 0), ia_w, 1.0).astype(np.float32)\n",
    "    ia_w = ia_w / float(np.mean(ia_w))\n",
    "    ia_w = np.clip(ia_w, 0.5, 5.0)\n",
    "    ia_w_t = torch.tensor(ia_w, dtype=torch.float32, device=device).view(1, -1)\n",
    "    # Optional: include other model predictions as an input stream (PB OOFs analogue)\n",
    "    USE_BASE_OOFS_IN_DNN = True\n",
    "    if USE_BASE_OOFS_IN_DNN and (WORK_ROOT / 'features' / 'oof_pred_logreg.npy').exists():\n",
    "        oof_stream = [np.load(WORK_ROOT / 'features' / 'oof_pred_logreg.npy').astype(np.float32)]\n",
    "        test_stream = [np.load(WORK_ROOT / 'features' / 'test_pred_logreg.npy').astype(np.float32)]\n",
    "        if (WORK_ROOT / 'features' / 'oof_pred_gbdt.npy').exists():\n",
    "            oof_stream.append(np.load(WORK_ROOT / 'features' / 'oof_pred_gbdt.npy').astype(np.float32))\n",
    "            test_stream.append(np.load(WORK_ROOT / 'features' / 'test_pred_gbdt.npy').astype(np.float32))\n",
    "        base_oof = np.hstack(oof_stream)\n",
    "        base_test = np.hstack(test_stream)\n",
    "        features_train['base_oof'] = base_oof\n",
    "        features_test['base_oof'] = base_test\n",
    "        print(f\"Base OOF stream: train={base_oof.shape} test={base_test.shape}\")\n",
    "    # Select modality keys for the DNN (towers)\n",
    "    DNN_KEYS = [k for k in ['t5', 'esm2_650m', 'esm2_3b', 'ankh', 'taxa', 'text', 'base_oof'] if k in features_train]\n",
    "    print(f\"DNN modality keys={DNN_KEYS}\")\n",
    "    class Tower(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim=512, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(1024, out_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    class ColossalMultiModalDNN(nn.Module):\n",
    "        def __init__(self, dims: dict, output_dim: int):\n",
    "            super().__init__()\n",
    "            self.keys = list(dims.keys())\n",
    "            self.towers = nn.ModuleDict({k: Tower(dims[k]) for k in self.keys})\n",
    "            fused_dim = 512 * len(self.keys)\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(fused_dim, 2048),\n",
    "                nn.BatchNorm1d(2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1024, output_dim),\n",
    "            )\n",
    "        def forward(self, batch: dict):\n",
    "            hs = [self.towers[k](batch[k]) for k in self.keys]\n",
    "            h = torch.cat(hs, dim=1)\n",
    "            return self.head(h)\n",
    "    # Prepare torch tensors per modality\n",
    "    train_t = {k: torch.tensor(features_train[k], dtype=torch.float32, device=device) for k in DNN_KEYS}\n",
    "    test_t = {k: torch.tensor(features_test[k], dtype=torch.float32, device=device) for k in DNN_KEYS}\n",
    "    def _batch_dict(tensors: dict, idx):\n",
    "        return {k: v[idx] for k, v in tensors.items()}\n",
    "    # Multi-state ensembling\n",
    "    DNN_SEEDS = [42, 43, 44, 45, 46]\n",
    "    DNN_EPOCHS = 10\n",
    "    BATCH_SIZE = 256\n",
    "    oof_sum = np.zeros(Y.shape, dtype=np.float32)\n",
    "    test_sum = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "    n_states = len(DNN_SEEDS)\n",
    "    for state_i, seed in enumerate(DNN_SEEDS, 1):\n",
    "        print(f\"\\n[DNN] Random state {state_i}/{n_states}: seed={seed}\")\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        kf_state = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        oof_state = np.zeros(Y.shape, dtype=np.float32)\n",
    "        test_state = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "        dims = {k: int(features_train[k].shape[1]) for k in DNN_KEYS}\n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf_state.split(train_ids)):\n",
    "            print(f\"DNN Fold {fold+1}/5\")\n",
    "            model = ColossalMultiModalDNN(dims=dims, output_dim=Y.shape[1]).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "            Y_full_t = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "            n_samples = len(idx_tr)\n",
    "            model.train()\n",
    "            idx_tr_t = torch.tensor(idx_tr, dtype=torch.long, device=device)\n",
    "            for _epoch in range(DNN_EPOCHS):\n",
    "                perm = idx_tr_t[torch.randperm(n_samples, device=device)]\n",
    "                for i in range(0, n_samples, BATCH_SIZE):\n",
    "                    b = perm[i:i + BATCH_SIZE]\n",
    "                    optimizer.zero_grad()\n",
    "                    logits = model(_batch_dict(train_t, b))\n",
    "                    yb = Y_full_t[b]\n",
    "                    loss_el = F.binary_cross_entropy_with_logits(logits, yb, reduction='none')\n",
    "                    loss = (loss_el * ia_w_t).mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                idx_val_t = torch.tensor(idx_val, dtype=torch.long, device=device)\n",
    "                val_probs = torch.sigmoid(model(_batch_dict(train_t, idx_val_t))).cpu().numpy()\n",
    "                oof_state[idx_val] = val_probs\n",
    "                # test prediction (average over folds)\n",
    "                test_probs = torch.sigmoid(model(test_t)).cpu().numpy()\n",
    "                test_state += test_probs / kf_state.get_n_splits()\n",
    "            val_preds = (val_probs > 0.3).astype(int)\n",
    "            f1 = f1_score(Y[idx_val], val_preds, average='micro')\n",
    "            ia_f1 = ia_weighted_f1(Y[idx_val], val_probs, thr=0.3)\n",
    "            print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "            print(f\"  >> Fold {fold+1} IA-F1@0.30: ALL={ia_f1['ALL']:.4f} MF={ia_f1['MF']:.4f} BP={ia_f1['BP']:.4f} CC={ia_f1['CC']:.4f}\")\n",
    "            torch.save(model.state_dict(), WORK_ROOT / 'features' / f'level1_dnn_seed{seed}_fold{fold}.pth')\n",
    "        oof_sum += oof_state\n",
    "        test_sum += test_state\n",
    "    oof_preds_dnn = (oof_sum / n_states).astype(np.float32)\n",
    "    test_preds_dnn = (test_sum / n_states).astype(np.float32)\n",
    "    np.save(WORK_ROOT / 'features' / 'oof_pred_dnn.npy', oof_preds_dnn)\n",
    "    np.save(WORK_ROOT / 'features' / 'test_pred_dnn.npy', test_preds_dnn)\n",
    "    print(\"DNN OOF + test preds saved (multi-state averaged).\")\n",
    "    # Persist term list for downstream stacker/submission consistency\n",
    "    with open(WORK_ROOT / 'features' / 'top_terms_13500.json', 'w') as f:\n",
    "        json.dump(top_terms, f)\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        req = [WORK_ROOT / 'features' / 'top_terms_13500.json', WORK_ROOT / 'features' / 'oof_pred_logreg.npy', WORK_ROOT / 'features' / 'test_pred_logreg.npy', WORK_ROOT / 'features' / 'oof_pred_dnn.npy', WORK_ROOT / 'features' / 'test_pred_dnn.npy']\n",
    "        req += [WORK_ROOT / 'features' / 'oof_pred_gbdt.npy', WORK_ROOT / 'features' / 'test_pred_gbdt.npy']\n",
    "        STORE.maybe_push('stage_07_level1_preds', req, note='Level-1 OOF + test preds (LR/GBDT/DNN)')\n",
    "    # Diagnostic: IA-F1 vs threshold curve (OOF)\n",
    "    thrs = np.linspace(0.05, 0.60, 23)\n",
    "    curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "    for thr in thrs:\n",
    "        s = ia_weighted_f1(Y, oof_preds_dnn, thr=float(thr))\n",
    "        for k in curves.keys():\n",
    "            curves[k].append(s[k])\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "        plt.plot(thrs, curves[k], label=k)\n",
    "    plt.title('DNN OOF: IA-weighted F1 vs threshold (multi-state)')\n",
    "    plt.xlabel('threshold')\n",
    "    plt.ylabel('IA-F1')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    print(\"Phase 2 Complete. OOF + test predictions generated.\")\n",
    "else:\n",
    "    print(\"Skipping Phase 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67aaabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13B - Logistic Regression (RAM-safe, RAPIDS when feasible) + checkpoint push\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping LogReg (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import os\n",
    "    import time\n",
    "    import gc\n",
    "    import psutil\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    import torch\n",
    "    from tqdm.auto import tqdm\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    import warnings\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "    PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    lr_oof_path = PRED_DIR / 'oof_pred_logreg.npy'\n",
    "    lr_test_path = PRED_DIR / 'test_pred_logreg.npy'\n",
    "    def _log_mem(msg: str = ''):\n",
    "        try:\n",
    "            p = psutil.Process(os.getpid())\n",
    "            ram_gb = p.memory_info().rss / (1024**3)\n",
    "            gpu_msg = ''\n",
    "            if torch.cuda.is_available():\n",
    "                alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "                res = torch.cuda.memory_reserved() / (1024**3)\n",
    "                gpu_msg = f' | GPU Alloc: {alloc:.2f}GB Res: {res:.2f}GB'\n",
    "            print(f\"[MEM] {msg:<22} | RAM: {ram_gb:.2f}GB{gpu_msg}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if lr_oof_path.exists() and lr_test_path.exists():\n",
    "        oof_pred_logreg = np.load(lr_oof_path, mmap_mode='r')\n",
    "        test_pred_logreg = np.load(lr_test_path, mmap_mode='r')\n",
    "    else:\n",
    "        print(\"\\n--- Training Logistic Regression (RAM-safe) ---\")\n",
    "        _log_mem('start')\n",
    "        try:\n",
    "            import cuml  # noqa: F401\n",
    "            from cuml.linear_model import LogisticRegression as cuLogReg\n",
    "            from cuml.multiclass import OneVsRestClassifier as cuOVR\n",
    "            import cupy as cp\n",
    "            HAS_RAPIDS = True\n",
    "            print('[AUDITOR] RAPIDS (cuML) detected.')\n",
    "        except Exception:\n",
    "            HAS_RAPIDS = False\n",
    "            cp = None  # type: ignore\n",
    "            print('[AUDITOR] RAPIDS NOT detected.')\n",
    "        # Default ON (will auto-disable per-fold if VRAM is clearly insufficient).\n",
    "        USE_RAPIDS_LOGREG = True\n",
    "        def _gpu_mem_okay(n_rows: int, n_cols: int, safety: float = 1.35) -> bool:\n",
    "            if not (HAS_RAPIDS and torch.cuda.is_available()):\n",
    "                return False\n",
    "            try:\n",
    "                free_b, total_b = cp.cuda.runtime.memGetInfo()\n",
    "                need_b = int(n_rows) * int(n_cols) * 4\n",
    "                ok = free_b > int(safety * need_b)\n",
    "                print(f\"[AUDITOR] VRAM free={free_b/1e9:.1f}GB total={total_b/1e9:.1f}GB need≈{need_b/1e9:.1f}GB ok={ok}\")\n",
    "                return bool(ok)\n",
    "            except Exception:\n",
    "                return True\n",
    "        def materialise_rows_to_memmap(src, idx, out_path, dtype=np.float32, chunk_rows: int = 2048):\n",
    "            out_path = str(out_path)\n",
    "            n_rows = int(len(idx))\n",
    "            n_cols = int(src.shape[1])\n",
    "            mm = np.lib.format.open_memmap(out_path, mode='w+', dtype=dtype, shape=(n_rows, n_cols))\n",
    "            for start in range(0, n_rows, chunk_rows):\n",
    "                end = min(start + chunk_rows, n_rows)\n",
    "                rows = idx[start:end]\n",
    "                mm[start:end] = np.asarray(src[rows], dtype=dtype)\n",
    "            mm.flush()\n",
    "            return mm\n",
    "        def fit_scaler_state_from_memmap(X_mm, chunk_rows: int = 2048):\n",
    "            scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "            for start in range(0, X_mm.shape[0], chunk_rows):\n",
    "                end = min(start + chunk_rows, X_mm.shape[0])\n",
    "                xb = np.asarray(X_mm[start:end], dtype=np.float32)\n",
    "                scaler.partial_fit(xb)\n",
    "            return {'mean': scaler.mean_.astype(np.float32), 'scale': scaler.scale_.astype(np.float32)}\n",
    "        def transform_memmap_to_memmap(X_mm, state, out_path, chunk_rows: int = 2048):\n",
    "            out = np.lib.format.open_memmap(str(out_path), mode='w+', dtype=np.float32, shape=X_mm.shape)\n",
    "            mean = state['mean']\n",
    "            scale = state['scale']\n",
    "            for start in range(0, X_mm.shape[0], chunk_rows):\n",
    "                end = min(start + chunk_rows, X_mm.shape[0])\n",
    "                xb = np.asarray(X_mm[start:end], dtype=np.float32)\n",
    "                xb -= mean\n",
    "                xb /= (scale + 1e-12)\n",
    "                out[start:end] = xb\n",
    "            out.flush()\n",
    "            return out\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        # Write predictions directly to disk-backed arrays (avoids massive RAM spikes).\n",
    "        oof_pred_logreg = np.lib.format.open_memmap(str(lr_oof_path), mode='w+', dtype=np.float32, shape=(X.shape[0], Y.shape[1]))\n",
    "        test_pred_logreg = np.lib.format.open_memmap(str(lr_test_path), mode='w+', dtype=np.float32, shape=(X_test.shape[0], Y.shape[1]))\n",
    "        # Initialise (memmap is not guaranteed zeroed).\n",
    "        oof_pred_logreg[:] = 0.0\n",
    "        test_pred_logreg[:] = 0.0\n",
    "        oof_pred_logreg.flush()\n",
    "        test_pred_logreg.flush()\n",
    "        tmp_dir = WORK_ROOT / 'features' / 'tmp_folds_logreg'\n",
    "        tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        t0_all = time.time()\n",
    "        fold_iter = tqdm(kf.split(np.arange(X.shape[0])), total=kf.get_n_splits(), desc='LogReg folds', unit='fold')\n",
    "        for fold, (idx_tr, idx_val) in enumerate(fold_iter):\n",
    "            t0_fold = time.time()\n",
    "            print(f\"LogReg Fold {fold+1}/{kf.get_n_splits()}\")\n",
    "            _log_mem(f\"fold {fold+1} start\")\n",
    "            idx_tr = np.asarray(idx_tr)\n",
    "            idx_val = np.asarray(idx_val)\n",
    "            X_tr_path = tmp_dir / f'X_tr_fold{fold}.npy'\n",
    "            X_val_path = tmp_dir / f'X_val_fold{fold}.npy'\n",
    "            Y_tr_path = tmp_dir / f'Y_tr_fold{fold}.npy'\n",
    "            X_tr = materialise_rows_to_memmap(X, idx_tr, X_tr_path, dtype=np.float32)\n",
    "            X_val = materialise_rows_to_memmap(X, idx_val, X_val_path, dtype=np.float32)\n",
    "            Y_tr = materialise_rows_to_memmap(Y, idx_tr, Y_tr_path, dtype=np.float32)\n",
    "            scaler_state = fit_scaler_state_from_memmap(X_tr)\n",
    "            scaler_path = PRED_DIR / f'logreg_scaler_fold{fold}.pkl'\n",
    "            joblib.dump(scaler_state, scaler_path)\n",
    "            X_trs_path = tmp_dir / f'X_tr_scaled_fold{fold}.npy'\n",
    "            X_vals_path = tmp_dir / f'X_val_scaled_fold{fold}.npy'\n",
    "            X_trs = transform_memmap_to_memmap(X_tr, scaler_state, X_trs_path)\n",
    "            X_vals = transform_memmap_to_memmap(X_val, scaler_state, X_vals_path)\n",
    "            del X_tr, X_val\n",
    "            gc.collect()\n",
    "            _log_mem(f\"fold {fold+1} scaled\")\n",
    "            use_gpu = bool(HAS_RAPIDS and USE_RAPIDS_LOGREG and _gpu_mem_okay(int(X_trs.shape[0] + X_vals.shape[0]), int(X_trs.shape[1])))\n",
    "            if use_gpu:\n",
    "                print('[AUDITOR] Using RAPIDS/cuML for LogReg')\n",
    "                try:\n",
    "                    import rmm\n",
    "                    rmm.reinitialize(managed_memory=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                mean_gpu = cp.asarray(scaler_state['mean'])\n",
    "                scale_gpu = cp.asarray(scaler_state['scale'])\n",
    "                try:\n",
    "                    X_tr_gpu = cp.asarray(np.asarray(X_trs))\n",
    "                    X_val_gpu = cp.asarray(np.asarray(X_vals))\n",
    "                except Exception as e:\n",
    "                    print('[AUDITOR] GPU transfer failed; falling back to CPU:', repr(e))\n",
    "                    use_gpu = False\n",
    "                if use_gpu:\n",
    "                    n_targets = int(Y.shape[1])\n",
    "                    chunk_size = int(os.getenv('CAFA_LOGREG_TARGET_CHUNK', '2000'))\n",
    "                    test_bs = int(os.getenv('CAFA_LOGREG_TEST_BS', '2048'))\n",
    "                    all_coefs = []\n",
    "                    all_intercepts = []\n",
    "                    for start in tqdm(range(0, n_targets, chunk_size), total=(n_targets + chunk_size - 1)//chunk_size, desc=f'Fold {fold+1} target chunks', unit='chunk', leave=False):\n",
    "                        end = min(start + chunk_size, n_targets)\n",
    "                        Y_tr_chunk = cp.asarray(np.asarray(Y_tr[:, start:end]))\n",
    "                        clf_chunk = cuOVR(cuLogReg(solver='qn', penalty='l2', C=1.0, max_iter=1000, tol=1e-3))\n",
    "                        clf_chunk.fit(X_tr_gpu, Y_tr_chunk)\n",
    "                        # Validation probs (fits on GPU).\n",
    "                        p_val = clf_chunk.predict_proba(X_val_gpu)\n",
    "                        if hasattr(p_val, 'get'):\n",
    "                            p_val = p_val.get()\n",
    "                        elif hasattr(p_val, 'to_numpy'):\n",
    "                            p_val = p_val.to_numpy()\n",
    "                        oof_pred_logreg[idx_val, start:end] = np.asarray(p_val, dtype=np.float32)\n",
    "                        # Test probs (GPU-batched to avoid VRAM spikes).\n",
    "                        for b0 in range(0, int(X_test.shape[0]), test_bs):\n",
    "                            b1 = min(b0 + test_bs, int(X_test.shape[0]))\n",
    "                            xb = np.asarray(X_test[b0:b1], dtype=np.float32)\n",
    "                            xb_gpu = cp.asarray(xb)\n",
    "                            xb_gpu = (xb_gpu - mean_gpu) / (scale_gpu + 1e-12)\n",
    "                            p_te = clf_chunk.predict_proba(xb_gpu)\n",
    "                            if hasattr(p_te, 'get'):\n",
    "                                p_te = p_te.get()\n",
    "                            elif hasattr(p_te, 'to_numpy'):\n",
    "                                p_te = p_te.to_numpy()\n",
    "                            test_pred_logreg[b0:b1, start:end] += (np.asarray(p_te, dtype=np.float32) / float(n_splits))\n",
    "                            del xb, xb_gpu, p_te\n",
    "                        for est in clf_chunk.estimators_:\n",
    "                            all_coefs.append(est.coef_.to_numpy() if hasattr(est.coef_, 'to_numpy') else est.coef_)\n",
    "                            all_intercepts.append(est.intercept_.to_numpy() if hasattr(est.intercept_, 'to_numpy') else est.intercept_)\n",
    "                        del Y_tr_chunk, clf_chunk, p_val\n",
    "                        cp.get_default_memory_pool().free_all_blocks()\n",
    "                    model_data = {'coef': np.vstack(all_coefs), 'intercept': np.hstack(all_intercepts)}\n",
    "                    joblib.dump(model_data, PRED_DIR / f'logreg_weights_fold{fold}.pkl')\n",
    "                    del X_tr_gpu, X_val_gpu, mean_gpu, scale_gpu\n",
    "                    cp.get_default_memory_pool().free_all_blocks()\n",
    "            if not use_gpu:\n",
    "                print('[AUDITOR] Using CPU SGD (fallback) for LogReg')\n",
    "                clf_logreg = OneVsRestClassifier(\n",
    "                    SGDClassifier(loss='log_loss', penalty='l2', alpha=0.0001, max_iter=1, tol=None, n_jobs=4),\n",
    "                    n_jobs=-1,\n",
    "                )\n",
    "                clf_logreg.fit(X_trs, Y_tr)\n",
    "                joblib.dump(clf_logreg, PRED_DIR / f'logreg_sgd_fold{fold}.pkl')\n",
    "                # Validation preds (batched).\n",
    "                bs = int(os.getenv('CAFA_LOGREG_VAL_BS', '2048'))\n",
    "                for b0 in tqdm(range(0, int(X_vals.shape[0]), bs), desc=f'Fold {fold+1} val predict', unit='batch', leave=False):\n",
    "                    b1 = min(b0 + bs, int(X_vals.shape[0]))\n",
    "                    xb = np.asarray(X_vals[b0:b1], dtype=np.float32)\n",
    "                    pb = clf_logreg.predict_proba(xb).astype(np.float32)\n",
    "                    oof_pred_logreg[idx_val[b0:b1]] = pb\n",
    "                # Test preds (scaled on the fly, batched).\n",
    "                test_bs = int(os.getenv('CAFA_LOGREG_TEST_BS', '2048'))\n",
    "                mean = scaler_state['mean']\n",
    "                scale = scaler_state['scale']\n",
    "                for b0 in tqdm(range(0, int(X_test.shape[0]), test_bs), desc=f'Fold {fold+1} test predict', unit='batch', leave=False):\n",
    "                    b1 = min(b0 + test_bs, int(X_test.shape[0]))\n",
    "                    xb = np.asarray(X_test[b0:b1], dtype=np.float32)\n",
    "                    xb = (xb - mean) / (scale + 1e-12)\n",
    "                    pb = clf_logreg.predict_proba(xb).astype(np.float32)\n",
    "                    test_pred_logreg[b0:b1] += pb / float(n_splits)\n",
    "            # Fold diagnostics (sampled).\n",
    "            try:\n",
    "                sample_n = int(min(20000, len(idx_val)))\n",
    "                if sample_n > 0:\n",
    "                    sample_probs = np.asarray(oof_pred_logreg[idx_val[:sample_n]], dtype=np.float32)\n",
    "                    sample_true = np.asarray(Y[idx_val[:sample_n]], dtype=np.float32)\n",
    "                    best_f1 = 0.0\n",
    "                    best_thr = 0.0\n",
    "                    for thr in np.linspace(0.01, 0.20, 20):\n",
    "                        vp = (sample_probs > thr).astype(int)\n",
    "                        from sklearn.metrics import f1_score\n",
    "                        score = f1_score(sample_true, vp, average='micro')\n",
    "                        if score > best_f1:\n",
    "                            best_f1, best_thr = score, thr\n",
    "                    if 'ia_weighted_f1' in globals():\n",
    "                        ia_f1 = ia_weighted_f1(sample_true, sample_probs, thr=float(best_thr))\n",
    "                        print(f\"  >> Fold {fold+1} (sample) micro-F1={best_f1:.4f} best_thr={best_thr:.2f} IA-F1(ALL)={ia_f1['ALL']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print('  [WARNING] Diagnostics skipped:', repr(e))\n",
    "            oof_pred_logreg.flush()\n",
    "            test_pred_logreg.flush()\n",
    "            # Cleanup fold artefacts\n",
    "            del Y_tr, scaler_state\n",
    "            if 'clf_logreg' in locals():\n",
    "                del clf_logreg\n",
    "            if 'X_trs' in locals():\n",
    "                del X_trs\n",
    "            if 'X_vals' in locals():\n",
    "                del X_vals\n",
    "            gc.collect()\n",
    "            print(f\"[TIMER] Fold {fold+1} wall: {time.time() - t0_fold:.1f}s\")\n",
    "            _log_mem(f\"fold {fold+1} end\")\n",
    "            for p in [X_tr_path, X_val_path, Y_tr_path, X_trs_path, X_vals_path]:\n",
    "                try:\n",
    "                    os.remove(p)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        oof_pred_logreg.flush()\n",
    "        test_pred_logreg.flush()\n",
    "        del oof_pred_logreg, test_pred_logreg\n",
    "        gc.collect()\n",
    "        print(f\"LogReg saved: {lr_oof_path}\")\n",
    "        print(f\"LogReg saved: {lr_test_path}\")\n",
    "        print(f\"[TIMER] LogReg total wall: {time.time() - t0_all:.1f}s\")\n",
    "        # Reload as mmap for downstream cells/diagnostics\n",
    "        oof_pred_logreg = np.load(lr_oof_path, mmap_mode='r')\n",
    "        test_pred_logreg = np.load(lr_test_path, mmap_mode='r')\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07a_level1_logreg',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str(lr_oof_path.as_posix()),\n",
    "            str(lr_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 Logistic Regression predictions (OOF + test).',\n",
    "    )\n",
    "    # Diagnostics: probability histograms + IA-F1 vs threshold (sampled)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "        def _sub(y_true: np.ndarray, y_score):\n",
    "            n = int(y_true.shape[0])\n",
    "            m = min(n, int(DIAG_N))\n",
    "            if m <= 0:\n",
    "                return y_true[:0], y_score[:0]\n",
    "            idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "            return y_true[idx], y_score[idx]\n",
    "        y_t, y_s = _sub(Y, oof_pred_logreg)\n",
    "        row_max_oof = np.asarray(y_s).max(axis=1)\n",
    "        row_mean_oof = np.asarray(y_s).mean(axis=1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_max_oof, bins=60, alpha=0.7)\n",
    "        plt.title('LogReg OOF: max probability per protein (sampled)')\n",
    "        plt.xlabel('max prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_mean_oof, bins=60, alpha=0.7)\n",
    "        plt.title('LogReg OOF: mean probability per protein (sampled)')\n",
    "        plt.xlabel('mean prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        if test_pred_logreg is not None:\n",
    "            te_s = test_pred_logreg\n",
    "            te_m = min(int(te_s.shape[0]), int(DIAG_N))\n",
    "            te_idx = np.linspace(0, int(te_s.shape[0]) - 1, num=te_m, dtype=np.int64) if te_m > 0 else np.array([], dtype=np.int64)\n",
    "            row_max_te = np.asarray(te_s[te_idx]).max(axis=1) if te_m > 0 else np.array([], dtype=np.float32)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(row_max_oof, bins=60, alpha=0.5, label='OOF')\n",
    "            plt.hist(row_max_te, bins=60, alpha=0.5, label='test')\n",
    "            plt.title('LogReg: max probability per protein (OOF vs test; sampled)')\n",
    "            plt.xlabel('max prob')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(y_t, np.asarray(y_s), thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('LogReg OOF: IA-weighted F1 vs threshold (sampled)')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('LogReg diagnostics skipped:', repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13C - GBDT (py-boost) + checkpoint push\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping GBDT (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    gbdt_oof_path = PRED_DIR / 'oof_pred_gbdt.npy'\n",
    "    gbdt_test_path = PRED_DIR / 'test_pred_gbdt.npy'\n",
    "    if gbdt_oof_path.exists() and gbdt_test_path.exists():\n",
    "        oof_pred_gbdt = np.load(gbdt_oof_path)\n",
    "        test_pred_gbdt = np.load(gbdt_test_path)\n",
    "    else:\n",
    "        try:\n",
    "            from py_boost import GradientBoosting\n",
    "            from py_boost.multioutput import MultiOutputRegressor\n",
    "        except Exception as e:\n",
    "            raise RuntimeError('py_boost is REQUIRED for this pipeline (GBDT is mandatory). Install it with: pip install py-boost') from e\n",
    "        # Note: Multi-label probabilities via regressor outputs squashed to [0,1]\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        oof_pred_gbdt = np.zeros((X.shape[0], Y.shape[1]), dtype=np.float32)\n",
    "        test_pred_gbdt = np.zeros((X_test.shape[0], Y.shape[1]), dtype=np.float32)\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X), start=1):\n",
    "            print(f'Fold {fold}/{n_splits}')\n",
    "            base = GradientBoosting(loss='logloss', ntrees=150, lr=0.05, max_depth=6, subsample=0.8, colsample=0.8, random_state=42 + fold)\n",
    "            model = MultiOutputRegressor(base)\n",
    "            model.fit(X[tr_idx], Y[tr_idx])\n",
    "            va = model.predict(X[va_idx]).astype(np.float32)\n",
    "            te = model.predict(X_test).astype(np.float32)\n",
    "            # safety clamp\n",
    "            va = np.clip(va, 0.0, 1.0)\n",
    "            te = np.clip(te, 0.0, 1.0)\n",
    "            oof_pred_gbdt[va_idx] = va\n",
    "            test_pred_gbdt += te / float(n_splits)\n",
    "            print('  IA-F1:', ia_weighted_f1(Y[va_idx], va, thr=0.3))\n",
    "        np.save(gbdt_oof_path, oof_pred_gbdt)\n",
    "        np.save(gbdt_test_path, test_pred_gbdt)\n",
    "        print('Saved:', gbdt_oof_path)\n",
    "        print('Saved:', gbdt_test_path)\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07b_level1_gbdt',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str(gbdt_oof_path.as_posix()),\n",
    "            str(gbdt_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 GBDT predictions (OOF + test).',\n",
    "    )\n",
    "    # Diagnostics: probability histograms + IA-F1 vs threshold (sampled)\n",
    "    try:\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "        def _sub(y_true: np.ndarray, y_score: np.ndarray):\n",
    "            n = int(y_true.shape[0])\n",
    "            m = min(n, int(DIAG_N))\n",
    "            if m <= 0:\n",
    "                return y_true[:0], y_score[:0]\n",
    "            idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "            return y_true[idx], y_score[idx]\n",
    "        y_t, y_s = _sub(Y, oof_pred_gbdt)\n",
    "        row_max_oof = y_s.max(axis=1)\n",
    "        row_mean_oof = y_s.mean(axis=1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_max_oof, bins=60, alpha=0.7)\n",
    "        plt.title('GBDT OOF: max probability per protein (sampled)')\n",
    "        plt.xlabel('max prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_mean_oof, bins=60, alpha=0.7)\n",
    "        plt.title('GBDT OOF: mean probability per protein (sampled)')\n",
    "        plt.xlabel('mean prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        if test_pred_gbdt is not None:\n",
    "            te_s = test_pred_gbdt\n",
    "            te_m = min(int(te_s.shape[0]), int(DIAG_N))\n",
    "            te_idx = np.linspace(0, int(te_s.shape[0]) - 1, num=te_m, dtype=np.int64) if te_m > 0 else np.array([], dtype=np.int64)\n",
    "            row_max_te = te_s[te_idx].max(axis=1) if te_m > 0 else np.array([], dtype=np.float32)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(row_max_oof, bins=60, alpha=0.5, label='OOF')\n",
    "            plt.hist(row_max_te, bins=60, alpha=0.5, label='test')\n",
    "            plt.title('GBDT: max probability per protein (OOF vs test; sampled)')\n",
    "            plt.xlabel('max prob')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(y_t, y_s, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('GBDT OOF: IA-weighted F1 vs threshold (sampled)')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('GBDT diagnostics skipped:', repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3933b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13D - DNN (PyTorch) + checkpoint push\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping DNN (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    dnn_oof_path = PRED_DIR / 'oof_pred_dnn.npy'\n",
    "    dnn_test_path = PRED_DIR / 'test_pred_dnn.npy'\n",
    "    if dnn_oof_path.exists() and dnn_test_path.exists():\n",
    "        oof_pred_dnn = np.load(dnn_oof_path)\n",
    "        test_pred_dnn = np.load(dnn_test_path)\n",
    "    else:\n",
    "        torch.manual_seed(42)\n",
    "        np.random.seed(42)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print('DNN device:', device)\n",
    "        # Keep DNN input identical to flat concatenation (for now)\n",
    "        X_dnn = X.astype(np.float32)\n",
    "        X_dnn_test = X_test.astype(np.float32)\n",
    "        y_dnn = Y.astype(np.float32)\n",
    "        class NumpyDataset(Dataset):\n",
    "            def __init__(self, X, y=None):\n",
    "                self.X = torch.from_numpy(X)\n",
    "                self.y = None if y is None else torch.from_numpy(y)\n",
    "            def __len__(self):\n",
    "                return self.X.shape[0]\n",
    "            def __getitem__(self, idx):\n",
    "                if self.y is None:\n",
    "                    return self.X[idx]\n",
    "                return self.X[idx], self.y[idx]\n",
    "        class MLP(nn.Module):\n",
    "            def __init__(self, in_dim, out_dim, hidden=2048, p=0.2):\n",
    "                super().__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(in_dim, hidden),\n",
    "                    nn.BatchNorm1d(hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p),\n",
    "                    nn.Linear(hidden, hidden),\n",
    "                    nn.BatchNorm1d(hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(p),\n",
    "                    nn.Linear(hidden, out_dim),\n",
    "                )\n",
    "            def forward(self, x):\n",
    "                return self.net(x)\n",
    "        def train_one_fold(train_idx, val_idx, seed=0, epochs=3, batch_size=256, lr=1e-3):\n",
    "            torch.manual_seed(42 + seed)\n",
    "            model = MLP(X_dnn.shape[1], y_dnn.shape[1]).to(device)\n",
    "            opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "            # IA-weighted BCE (weights broadcast over classes)\n",
    "            w = torch.from_numpy(weights).to(device)\n",
    "            bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "            dl_tr = DataLoader(\n",
    "                NumpyDataset(X_dnn[train_idx], y_dnn[train_idx]),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=False,\n",
    "            )\n",
    "            dl_va = DataLoader(\n",
    "                NumpyDataset(X_dnn[val_idx], y_dnn[val_idx]),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "            )\n",
    "            for ep in range(1, epochs + 1):\n",
    "                model.train()\n",
    "                for xb, yb in dl_tr:\n",
    "                    xb = xb.to(device, non_blocking=True).float()\n",
    "                    yb = yb.to(device, non_blocking=True).float()\n",
    "                    logits = model(xb)\n",
    "                    loss_per = bce(logits, yb)\n",
    "                    loss = (loss_per * w).mean()\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                # quick val metric\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    va_scores = []\n",
    "                    va_true = []\n",
    "                    for xb, yb in dl_va:\n",
    "                        xb = xb.to(device, non_blocking=True).float()\n",
    "                        logits = model(xb)\n",
    "                        va_scores.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                        va_true.append(yb.numpy())\n",
    "                    va_scores = np.vstack(va_scores)\n",
    "                    va_true = np.vstack(va_true)\n",
    "                    s = ia_weighted_f1(va_true, va_scores, thr=0.3)\n",
    "                print(f'  seed={seed} ep={ep}/{epochs} IA-F1={s}')\n",
    "            return model\n",
    "        n_splits = 5\n",
    "        n_seeds = 2\n",
    "        epochs = 3\n",
    "        batch_size = 256\n",
    "        lr = 1e-3\n",
    "        oof_pred_dnn = np.zeros((X_dnn.shape[0], y_dnn.shape[1]), dtype=np.float32)\n",
    "        test_pred_dnn = np.zeros((X_dnn_test.shape[0], y_dnn.shape[1]), dtype=np.float32)\n",
    "        counts = np.zeros((X_dnn.shape[0], 1), dtype=np.float32)\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_dnn), start=1):\n",
    "            print(f'Fold {fold}/{n_splits}')\n",
    "            fold_test = np.zeros_like(test_pred_dnn)\n",
    "            for seed in range(n_seeds):\n",
    "                model = train_one_fold(tr_idx, va_idx, seed=seed, epochs=epochs, batch_size=batch_size, lr=lr)\n",
    "                model.eval()\n",
    "                # OOF\n",
    "                dl_va = DataLoader(NumpyDataset(X_dnn[va_idx], None), batch_size=1024, shuffle=False)\n",
    "                preds_va = []\n",
    "                with torch.no_grad():\n",
    "                    for xb in dl_va:\n",
    "                        xb = xb.to(device, non_blocking=True).float()\n",
    "                        preds_va.append(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "                preds_va = np.vstack(preds_va).astype(np.float32)\n",
    "                oof_pred_dnn[va_idx] += preds_va\n",
    "                counts[va_idx] += 1.0\n",
    "                # TEST\n",
    "                dl_te = DataLoader(NumpyDataset(X_dnn_test, None), batch_size=1024, shuffle=False)\n",
    "                preds_te = []\n",
    "                with torch.no_grad():\n",
    "                    for xb in dl_te:\n",
    "                        xb = xb.to(device, non_blocking=True).float()\n",
    "                        preds_te.append(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "                preds_te = np.vstack(preds_te).astype(np.float32)\n",
    "                fold_test += preds_te\n",
    "            test_pred_dnn += (fold_test / float(n_seeds))\n",
    "        oof_pred_dnn = (oof_pred_dnn / np.maximum(counts, 1.0)).astype(np.float32)\n",
    "        test_pred_dnn = (test_pred_dnn / float(n_splits)).astype(np.float32)\n",
    "        np.save(dnn_oof_path, oof_pred_dnn)\n",
    "        np.save(dnn_test_path, test_pred_dnn)\n",
    "        print('Saved:', dnn_oof_path)\n",
    "        print('Saved:', dnn_test_path)\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07c_level1_dnn',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str(dnn_oof_path.as_posix()),\n",
    "            str(dnn_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 DNN predictions (OOF + test).',\n",
    "    )\n",
    "    # Diagnostics: probability histograms + IA-F1 vs threshold (sampled)\n",
    "    try:\n",
    "        import os\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "        def _sub(y_true: np.ndarray, y_score: np.ndarray):\n",
    "            n = int(y_true.shape[0])\n",
    "            m = min(n, int(DIAG_N))\n",
    "            if m <= 0:\n",
    "                return y_true[:0], y_score[:0]\n",
    "            idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "            return y_true[idx], y_score[idx]\n",
    "        y_t, y_s = _sub(Y, oof_pred_dnn)\n",
    "        row_max_oof = y_s.max(axis=1)\n",
    "        row_mean_oof = y_s.mean(axis=1)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_max_oof, bins=60, alpha=0.7)\n",
    "        plt.title('DNN OOF: max probability per protein (sampled)')\n",
    "        plt.xlabel('max prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.hist(row_mean_oof, bins=60, alpha=0.7)\n",
    "        plt.title('DNN OOF: mean probability per protein (sampled)')\n",
    "        plt.xlabel('mean prob')\n",
    "        plt.ylabel('count')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        if test_pred_dnn is not None:\n",
    "            te_s = test_pred_dnn\n",
    "            te_m = min(int(te_s.shape[0]), int(DIAG_N))\n",
    "            te_idx = np.linspace(0, int(te_s.shape[0]) - 1, num=te_m, dtype=np.int64) if te_m > 0 else np.array([], dtype=np.int64)\n",
    "            row_max_te = te_s[te_idx].max(axis=1) if te_m > 0 else np.array([], dtype=np.float32)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(row_max_oof, bins=60, alpha=0.5, label='OOF')\n",
    "            plt.hist(row_max_te, bins=60, alpha=0.5, label='test')\n",
    "            plt.title('DNN: max probability per protein (OOF vs test; sampled)')\n",
    "            plt.xlabel('max prob')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(y_t, y_s, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('DNN OOF: IA-weighted F1 vs threshold (sampled)')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('DNN diagnostics skipped:', repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13E - KNN (cosine; ESM2-3B) + checkpoint push\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping KNN (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from sklearn.model_selection import KFold\n",
    "    import json\n",
    "    PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    knn_oof_path = PRED_DIR / 'oof_pred_knn.npy'\n",
    "    knn_test_path = PRED_DIR / 'test_pred_knn.npy'\n",
    "    # Backwards-compatible copies (some downstream code loads from WORK_ROOT/features)\n",
    "    knn_oof_compat = WORK_ROOT / 'features' / 'oof_pred_knn.npy'\n",
    "    knn_test_compat = WORK_ROOT / 'features' / 'test_pred_knn.npy'\n",
    "    if knn_oof_path.exists() and knn_test_path.exists():\n",
    "        oof_pred_knn = np.load(knn_oof_path)\n",
    "        test_pred_knn = np.load(knn_test_path)\n",
    "        oof_max_sim = None\n",
    "    else:\n",
    "        if 'features_train' not in globals() or 'features_test' not in globals():\n",
    "            raise RuntimeError('Missing `features_train`/`features_test`. Run the Phase 2 feature load cell first.')\n",
    "        if 'esm2_3b' not in features_train:\n",
    "            raise FileNotFoundError(\"Missing required modality 'esm2_3b' in features_train. Ensure features/train_embeds_esm2_3b.npy exists.\")\n",
    "        X_knn = features_train['esm2_3b'].astype(np.float32)\n",
    "        X_knn_test = features_test['esm2_3b'].astype(np.float32)\n",
    "        # Enforce TOP_K alignment using the persisted term list.\n",
    "        top_terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "        if top_terms_path.exists():\n",
    "            top_terms_knn = json.loads(top_terms_path.read_text())\n",
    "            if Y.shape[1] != len(top_terms_knn):\n",
    "                raise ValueError(\n",
    "                    f'KNN shape mismatch: Y has {Y.shape[1]} cols but top_terms_13500.json has {len(top_terms_knn)} terms.'\n",
    "                )\n",
    "        else:\n",
    "            # Fall back to in-memory top_terms if present (should be created in Phase 2 cell).\n",
    "            if 'top_terms' not in globals():\n",
    "                raise RuntimeError('Missing top_terms_13500.json and in-memory top_terms. Run the Phase 2 cell first.')\n",
    "            top_terms_knn = list(top_terms)\n",
    "            if Y.shape[1] != len(top_terms_knn):\n",
    "                raise ValueError(\n",
    "                    f'KNN shape mismatch: Y has {Y.shape[1]} cols but top_terms has {len(top_terms_knn)} terms.'\n",
    "                )\n",
    "        # KNN needs binary targets (presence/absence), not counts.\n",
    "        Y_knn = (Y > 0).astype(np.float32)\n",
    "        def _l2_norm(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "            n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "            return x / np.maximum(n, eps)\n",
    "        # Cosine distance is best-behaved on L2-normalised vectors\n",
    "        X_knn = _l2_norm(X_knn)\n",
    "        X_knn_test = _l2_norm(X_knn_test)\n",
    "        KNN_K = int(globals().get('KNN_K', 50))\n",
    "        KNN_BATCH = int(globals().get('KNN_BATCH', 256))\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        oof_pred_knn = np.zeros((X_knn.shape[0], Y_knn.shape[1]), dtype=np.float32)\n",
    "        test_pred_knn = np.zeros((X_knn_test.shape[0], Y_knn.shape[1]), dtype=np.float32)\n",
    "        oof_max_sim = np.zeros((X_knn.shape[0],), dtype=np.float32)\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_knn), start=1):\n",
    "            print(f'Fold {fold}/{n_splits} (KNN)')\n",
    "            knn = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=-1)\n",
    "            knn.fit(X_knn[tr_idx])\n",
    "            dists, neigh = knn.kneighbors(X_knn[va_idx], return_distance=True)\n",
    "            sims = np.clip((1.0 - dists).astype(np.float32), 0.0, 1.0)\n",
    "            oof_max_sim[va_idx] = sims.max(axis=1)\n",
    "            neigh_global = tr_idx[neigh]  # map to global row indices into Y_knn\n",
    "            for i in range(0, len(va_idx), KNN_BATCH):\n",
    "                j = min(i + KNN_BATCH, len(va_idx))\n",
    "                neigh_b = neigh_global[i:j]\n",
    "                sims_b = sims[i:j]\n",
    "                denom = np.maximum(sims_b.sum(axis=1, keepdims=True), 1e-8)\n",
    "                Y_nei = Y_knn[neigh_b]  # (B, K, L)\n",
    "                scores = (np.einsum('bk,bkl->bl', sims_b, Y_nei) / denom).astype(np.float32)\n",
    "                oof_pred_knn[va_idx[i:j]] = scores\n",
    "            if 'ia_weighted_f1' in globals():\n",
    "                print('  IA-F1:', ia_weighted_f1(Y_knn[va_idx], oof_pred_knn[va_idx], thr=0.3))\n",
    "        # Final model on full train -> test\n",
    "        knn_final = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=-1)\n",
    "        knn_final.fit(X_knn)\n",
    "        dists_te, neigh_te = knn_final.kneighbors(X_knn_test, return_distance=True)\n",
    "        sims_te = np.clip((1.0 - dists_te).astype(np.float32), 0.0, 1.0)\n",
    "        denom_te = np.maximum(sims_te.sum(axis=1, keepdims=True), 1e-8)\n",
    "        for i in range(0, X_knn_test.shape[0], KNN_BATCH):\n",
    "            j = min(i + KNN_BATCH, X_knn_test.shape[0])\n",
    "            neigh_b = neigh_te[i:j]\n",
    "            sims_b = sims_te[i:j]\n",
    "            Y_nei = Y_knn[neigh_b]\n",
    "            scores = (np.einsum('bk,bkl->bl', sims_b, Y_nei) / denom_te[i:j]).astype(np.float32)\n",
    "            test_pred_knn[i:j] = scores\n",
    "        np.save(knn_oof_path, oof_pred_knn)\n",
    "        np.save(knn_test_path, test_pred_knn)\n",
    "        np.save(knn_oof_compat, oof_pred_knn)\n",
    "        np.save(knn_test_compat, test_pred_knn)\n",
    "        print('Saved:', knn_oof_path)\n",
    "        print('Saved:', knn_test_path)\n",
    "        print('Saved:', knn_oof_compat)\n",
    "        print('Saved:', knn_test_compat)\n",
    "    # Checkpoint push (always)\n",
    "    STORE.maybe_push(\n",
    "        stage='stage_07d_level1_knn',\n",
    "        required_paths=[\n",
    "            str((WORK_ROOT / 'features' / 'top_terms_13500.json').as_posix()),\n",
    "            str(knn_oof_path.as_posix()),\n",
    "            str(knn_test_path.as_posix()),\n",
    "        ],\n",
    "        note='Level-1 KNN (cosine) predictions using ESM2-3B embeddings (OOF + test).',\n",
    "    )\n",
    "    # Diagnostics: similarity distribution + IA-F1 vs threshold\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        if oof_max_sim is not None:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(oof_max_sim, bins=50)\n",
    "            plt.title('KNN OOF diagnostic: max cosine similarity to neighbours (per protein)')\n",
    "            plt.xlabel('max similarity')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(Y, oof_pred_knn, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('KNN OOF: IA-weighted F1 vs threshold')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('KNN diagnostics skipped:', repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f447b494",
   "metadata": {
    "id": "f447b494",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 14 - Solution: 5. PHASE 3: HIERARCHY-AWARE STACKING (GCN)\n",
    "# 5. PHASE 3: HIERARCHY-AWARE STACKING (GCN)\n",
    "# ==========================================\n",
    "# NOTE: This cell is kept as a lightweight alternative stacker.\n",
    "# The main stacker used by Phase 4 is implemented in the next cell.\n",
    "TRAIN_STACKER = False\n",
    "if TRAIN_STACKER:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from sklearn.metrics import f1_score\n",
    "    print(\"Loading OOF Predictions for stacking...\")\n",
    "    oof_logreg = np.load(WORK_ROOT / 'features' / 'oof_pred_logreg.npy')\n",
    "    try:\n",
    "        oof_gbdt = np.load(WORK_ROOT / 'features' / 'oof_pred_gbdt.npy')\n",
    "    except Exception:\n",
    "        oof_gbdt = np.zeros_like(oof_logreg)\n",
    "    try:\n",
    "        oof_dnn = np.load(WORK_ROOT / 'features' / 'oof_pred_dnn.npy')\n",
    "    except Exception:\n",
    "        oof_dnn = np.zeros_like(oof_logreg)\n",
    "    try:\n",
    "        oof_knn = np.load(WORK_ROOT / 'features' / 'oof_pred_knn.npy')\n",
    "    except Exception:\n",
    "        oof_knn = np.zeros_like(oof_logreg)\n",
    "    X_stack = np.hstack([oof_logreg, oof_gbdt, oof_dnn, oof_knn])\n",
    "    Y_stack = Y\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_stack_t = torch.tensor(X_stack, dtype=torch.float32).to(device)\n",
    "    Y_stack_t = torch.tensor(Y_stack, dtype=torch.float32).to(device)\n",
    "    class Stacker(nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(2048, output_dim),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    stacker = Stacker(X_stack.shape[1], Y_stack.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(stacker.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "    stacker.train()\n",
    "    for epoch in range(20):\n",
    "        optimizer.zero_grad()\n",
    "        out = stacker(X_stack_t)\n",
    "        loss = criterion(out, Y_stack_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                preds = (out > 0.3).float()\n",
    "                f1 = f1_score(Y_stack_t.cpu().numpy()[:1000], preds.cpu().numpy()[:1000], average='micro')\n",
    "            print(f\"Epoch {epoch}: Loss {loss.item():.4f}, Approx micro-F1@0.30 {f1:.4f}\")\n",
    "    torch.save(stacker.state_dict(), WORK_ROOT / 'features' / 'final_stacker.pth')\n",
    "    print(\"Saved: final_stacker.pth\")\n",
    "    stacker.eval()\n",
    "    with torch.no_grad():\n",
    "        final_preds = stacker(X_stack_t).cpu().numpy()\n",
    "        final_f1 = f1_score(Y_stack, (final_preds > 0.3).astype(int), average='micro')\n",
    "    print(f\"Final Stacker micro-F1@0.30: {final_f1:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping Phase 3 (lightweight stacker).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132ae0e",
   "metadata": {
    "id": "3132ae0e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 15 - Solution: 5b. PHASE 3: HIERARCHY-AWARE STACKING (GRAPH SMOOTHING GCN)\n",
    "# 5b. PHASE 3: HIERARCHY-AWARE STACKING (GRAPH SMOOTHING GCN)\n",
    "# =========================================================\n",
    "# Option B strictness: if PROCESS_EXTERNAL=True, we REQUIRE the propagated prior files to exist.\n",
    "PROCESS_EXTERNAL = globals().get('PROCESS_EXTERNAL', True)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "# Load the persisted term contract (must match Phase 2)\n",
    "terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "if not terms_path.exists():\n",
    "    raise FileNotFoundError(f'Missing {terms_path}. Run Phase 2 target construction first.')\n",
    "top_terms = json.loads(terms_path.read_text())\n",
    "TOP_K = int(len(top_terms))\n",
    "print(f'TOP_K (term contract) = {TOP_K}')\n",
    "\n",
    "def _load_level1_pred(fname: str):\n",
    "    \"\"\"Load Level-1 prediction arrays from either features/level1_preds or features/.\"\"\"\n",
    "    cand = [\n",
    "        WORK_ROOT / 'features' / 'level1_preds' / fname,\n",
    "        WORK_ROOT / 'features' / fname,\n",
    "    ]\n",
    "    for p in cand:\n",
    "        if p.exists():\n",
    "            return np.load(p).astype(np.float32)\n",
    "    return None\n",
    "# 1. Load Level-1 predictions\n",
    "train_feats = []\n",
    "for fname in ['oof_pred_logreg.npy', 'oof_pred_gbdt.npy', 'oof_pred_dnn.npy', 'oof_pred_knn.npy']:\n",
    "    arr = _load_level1_pred(fname)\n",
    "    if arr is not None:\n",
    "        train_feats.append(arr)\n",
    "if not train_feats:\n",
    "    raise FileNotFoundError('No Level-1 OOF predictions found. Run Phase 2 first.')\n",
    "X_stack = np.mean(train_feats, axis=0).astype(np.float32)\n",
    "# 2. Build Y label matrix for the persisted term contract\n",
    "train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "train_ids_raw = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "train_ids = train_ids_raw.str.extract(r'\\|(.*?)\\|', expand=False).fillna(train_ids_raw)\n",
    "train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "Y_df = Y_df.reindex(train_ids, fill_value=0)\n",
    "Y_df = Y_df.reindex(columns=top_terms, fill_value=0)\n",
    "Y = Y_df.values.astype(np.float32)\n",
    "print('Targets:', Y.shape)\n",
    "# 2b. External priors (Phase 1 Step 4 outputs) -> inject as *conservative* extra signal\n",
    "EXTERNAL_PRIOR_WEIGHT = 0.25\n",
    "ext_dir = WORK_ROOT / 'external'\n",
    "prior_train_path = ext_dir / 'prop_train_no_kaggle.tsv.gz'\n",
    "if PROCESS_EXTERNAL:\n",
    "    if not prior_train_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f'Option B requires external priors, but missing: {prior_train_path}. '\n",
    "            'Run Phase 1 Step 4 propagation or ensure your checkpoint dataset contains these files (run setup: STORE.pull()).'\n",
    "        )\n",
    "    prior_train = pd.read_csv(prior_train_path, sep='\\t')\n",
    "    prior_train = prior_train[prior_train['term'].isin(top_terms)]\n",
    "    prior_mat = prior_train.pivot_table(index='EntryID', columns='term', values='score', aggfunc='max', fill_value=0.0)\n",
    "    prior_mat = prior_mat.reindex(train_ids.tolist(), fill_value=0.0)\n",
    "    prior_mat = prior_mat.reindex(columns=top_terms, fill_value=0.0)\n",
    "    prior_np = prior_mat.values.astype(np.float32)\n",
    "    X_stack = np.maximum(X_stack, EXTERNAL_PRIOR_WEIGHT * prior_np)\n",
    "    print(f'Injected external IEA prior into train stack (weight={EXTERNAL_PRIOR_WEIGHT}).')\n",
    "(WORK_ROOT / 'features').mkdir(parents=True, exist_ok=True)\n",
    "with open(WORK_ROOT / 'features' / 'top_terms_13500.json', 'w') as f:\n",
    "    json.dump(top_terms, f)\n",
    "print('Saved: top_terms_13500.json')\n",
    "# 3. Graph adjacency from go-basic.obo (reload if needed)\n",
    "if 'go_parents' not in locals() or 'go_namespaces' not in locals():\n",
    "    print('Reloading GO graph (parse_obo)...')\n",
    "    def parse_obo(path: Path):\n",
    "        parents = {}\n",
    "        namespaces = {}\n",
    "        cur_id, cur_ns = None, None\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == '[Term]':\n",
    "                    if cur_id and cur_ns:\n",
    "                        namespaces[cur_id] = cur_ns\n",
    "                    cur_id, cur_ns = None, None\n",
    "                elif line.startswith('id: GO:'):\n",
    "                    cur_id = line.split('id: ', 1)[1]\n",
    "                elif line.startswith('namespace:'):\n",
    "                    cur_ns = line.split('namespace: ', 1)[1]\n",
    "                elif line.startswith('is_a:') and cur_id:\n",
    "                    parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                    parents.setdefault(cur_id, set()).add(parent)\n",
    "            if cur_id and cur_ns:\n",
    "                namespaces[cur_id] = cur_ns\n",
    "        return parents, namespaces\n",
    "    go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "\n",
    "def build_adjacency(terms_list, parents_dict):\n",
    "    term_to_idx = {t: i for i, t in enumerate(terms_list)}\n",
    "    n_terms = len(terms_list)\n",
    "    src, dst = [], []\n",
    "    for child in terms_list:\n",
    "        parents = parents_dict.get(child, set())\n",
    "        if not parents:\n",
    "            continue\n",
    "        child_idx = term_to_idx[child]\n",
    "        for parent in parents:\n",
    "            if parent in term_to_idx:\n",
    "                parent_idx = term_to_idx[parent]\n",
    "                src.append(child_idx)\n",
    "                dst.append(parent_idx)\n",
    "                src.append(parent_idx)\n",
    "                dst.append(child_idx)\n",
    "    src.extend(range(n_terms))\n",
    "    dst.extend(range(n_terms))\n",
    "    indices = torch.tensor([src, dst], dtype=torch.long)\n",
    "    values = torch.ones(len(src), dtype=torch.float32)\n",
    "    return torch.sparse_coo_tensor(indices, values, (n_terms, n_terms)).coalesce().to(device)\n",
    "\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n",
    "        super().__init__()\n",
    "        self.adj = adj_matrix\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sparse.mm(self.adj, x.t()).t()\n",
    "        return torch.sigmoid(x)\n",
    "# 4. Build test stack once (and inject external priors once), then split by ontology\n",
    "print('\\nPreparing test stack...')\n",
    "test_feats = []\n",
    "for fname in ['test_pred_logreg.npy', 'test_pred_gbdt.npy', 'test_pred_dnn.npy', 'test_pred_knn.npy']:\n",
    "    arr = _load_level1_pred(fname)\n",
    "    if arr is not None:\n",
    "        test_feats.append(arr)\n",
    "if not test_feats:\n",
    "    raise FileNotFoundError('No Level-1 test predictions found. Run Phase 2 first.')\n",
    "X_test_stack = np.mean(test_feats, axis=0).astype(np.float32)\n",
    "prior_test_path = ext_dir / 'prop_test_no_kaggle.tsv.gz'\n",
    "if PROCESS_EXTERNAL:\n",
    "    if not prior_test_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f'Option B requires external priors, but missing: {prior_test_path}. '\n",
    "            'Run Phase 1 Step 4 propagation or ensure your checkpoint dataset contains these files (run setup: STORE.pull()).'\n",
    "        )\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id'].astype(str)\n",
    "    prior_test = pd.read_csv(prior_test_path, sep='\\t')\n",
    "    prior_test = prior_test[prior_test['term'].isin(top_terms)]\n",
    "    prior_t = prior_test.pivot_table(index='EntryID', columns='term', values='score', aggfunc='max', fill_value=0.0)\n",
    "    prior_t = prior_t.reindex(test_ids.tolist(), fill_value=0.0)\n",
    "    prior_t = prior_t.reindex(columns=top_terms, fill_value=0.0)\n",
    "    prior_test_np = prior_t.values.astype(np.float32)\n",
    "    X_test_stack = np.maximum(X_test_stack, EXTERNAL_PRIOR_WEIGHT * prior_test_np)\n",
    "    print(f'Injected external IEA prior into test stack (weight={EXTERNAL_PRIOR_WEIGHT}).')\n",
    "# 5. Ontology split (BP/MF/CC)\n",
    "ns_to_aspect = {\n",
    "    'molecular_function': 'MF',\n",
    "    'biological_process': 'BP',\n",
    "    'cellular_component': 'CC',\n",
    "}\n",
    "aspects = []\n",
    "for t in top_terms:\n",
    "    ns = go_namespaces.get(t, None)\n",
    "    aspects.append(ns_to_aspect.get(ns, 'UNK'))\n",
    "bp_idx = [i for i, a in enumerate(aspects) if a == 'BP']\n",
    "mf_idx = [i for i, a in enumerate(aspects) if a == 'MF']\n",
    "cc_idx = [i for i, a in enumerate(aspects) if a == 'CC']\n",
    "print(f'Aspect split (in contract): BP={len(bp_idx)} MF={len(mf_idx)} CC={len(cc_idx)} UNK={aspects.count(\"UNK\")}')\n",
    "X_tensor_full = torch.tensor(X_stack, dtype=torch.float32, device=device)\n",
    "Y_tensor_full = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "X_test_full = torch.tensor(X_test_stack, dtype=torch.float32, device=device)\n",
    "\n",
    "def train_one(aspect_name: str, idx_cols: list[int]):\n",
    "    if not idx_cols:\n",
    "        print(f'[{aspect_name}] No terms; skipping.')\n",
    "        return None\n",
    "    terms_sub = [top_terms[i] for i in idx_cols]\n",
    "    adj = build_adjacency(terms_sub, go_parents)\n",
    "    model = SimpleGCN(input_dim=len(idx_cols), hidden_dim=1024, output_dim=len(idx_cols), adj_matrix=adj).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "    X_t = X_tensor_full[:, idx_cols]\n",
    "    Y_t = Y_tensor_full[:, idx_cols]\n",
    "    n_samples = X_t.shape[0]\n",
    "    BS = 256\n",
    "    EPOCHS = 5\n",
    "    model.train()\n",
    "    print(f'\\n=== Training GCN[{aspect_name}] terms={len(idx_cols)} ===')\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        perm = torch.randperm(n_samples, device=device)\n",
    "        for i in range(0, n_samples, BS):\n",
    "            b = perm[i:i + BS]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X_t[b])\n",
    "            loss = criterion(out, Y_t[b])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item())\n",
    "        with torch.no_grad():\n",
    "            pred = (model(X_t[:2000]) > 0.3).float().cpu().numpy()\n",
    "            f1 = f1_score(Y_t[:2000].cpu().numpy(), pred, average='micro')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} Loss={total_loss:.4f} micro-F1@0.30={f1:.4f}')\n",
    "    return model\n",
    "\n",
    "# 6. Train GCN per-aspect and write test_pred_gcn.npy (required by Phase 4)\n",
    "out_path = WORK_ROOT / 'features' / 'test_pred_gcn.npy'\n",
    "if out_path.exists():\n",
    "    print(f'GCN stacker output already exists; skipping: {out_path}')\n",
    "else:\n",
    "    # Baseline = mean ensemble (already includes external priors if enabled)\n",
    "    final_test = np.asarray(X_test_stack, dtype=np.float32).copy()\n",
    "    # Train and predict per aspect; overwrite only the aspect columns\n",
    "    def _predict_batched(model: nn.Module, x: torch.Tensor, bs: int = 4096) -> np.ndarray:\n",
    "        model.eval()\n",
    "        outs = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, int(x.shape[0]), int(bs)):\n",
    "                xb = x[i:i+bs]\n",
    "                out = model(xb).detach().float().cpu().numpy()\n",
    "                outs.append(out)\n",
    "        return np.vstack(outs).astype(np.float32)\n",
    "    for aspect_name, idx_cols in [('MF', mf_idx), ('BP', bp_idx), ('CC', cc_idx)]:\n",
    "        if not idx_cols:\n",
    "            continue\n",
    "        model = train_one(aspect_name, idx_cols)\n",
    "        if model is None:\n",
    "            continue\n",
    "        pred_te = _predict_batched(model, X_test_full[:, idx_cols])\n",
    "        if pred_te.shape[1] != len(idx_cols):\n",
    "            raise RuntimeError(f'GCN[{aspect_name}] shape mismatch: got {pred_te.shape} expected (_, {len(idx_cols)})')\n",
    "        final_test[:, idx_cols] = pred_te\n",
    "    np.save(out_path, final_test.astype(np.float32))\n",
    "    print('Saved:', out_path)\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        try:\n",
    "            STORE.maybe_push('stage_08_stacker_gcn', [out_path, terms_path], note='GCN graph-smoothing stacker (test preds)')\n",
    "        except Exception as e:\n",
    "            print('WARN: failed to push stage_08_stacker_gcn:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784143e",
   "metadata": {
    "id": "0784143e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 16 - Solution: 6. PHASE 4: POST-PROCESSING & SUBMISSION\n",
    "# 6. PHASE 4: POST-PROCESSING & SUBMISSION\n",
    "# ========================================\n",
    "# HARDWARE: CPU / GPU\n",
    "# ========================================\n",
    "# This phase applies the \"Strict Post-Processing\" rules (Max/Min Propagation)\n",
    "# and generates the final submission file.\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if submission already exists\n",
    "if (WORK_ROOT / 'submission.tsv').exists():\n",
    "    print(\"submission.tsv already exists. Skipping Phase 4.\")\n",
    "else:\n",
    "    print(\"Starting Phase 4: Post-processing & submission...\")\n",
    "    # Ensure go_parents is available (from Phase 1)\n",
    "    if 'go_parents' not in locals() or 'go_namespaces' not in locals():\n",
    "        print(\"Reloading GO graph (parse_obo)...\")\n",
    "        def parse_obo(path: Path):\n",
    "            parents = {}\n",
    "            namespaces = {}\n",
    "            cur_id, cur_ns = None, None\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line == '[Term]':\n",
    "                        if cur_id and cur_ns:\n",
    "                            namespaces[cur_id] = cur_ns\n",
    "                        cur_id, cur_ns = None, None\n",
    "                    elif line.startswith('id: GO:'):\n",
    "                        cur_id = line.split('id: ', 1)[1]\n",
    "                    elif line.startswith('namespace:'):\n",
    "                        cur_ns = line.split('namespace: ', 1)[1]\n",
    "                    elif line.startswith('is_a:') and cur_id:\n",
    "                        parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                        parents.setdefault(cur_id, set()).add(parent)\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "            return parents, namespaces\n",
    "        go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "    # Load test IDs\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id']\n",
    "    # Load stacker predictions\n",
    "    pred_path = WORK_ROOT / 'features' / 'test_pred_gcn.npy'\n",
    "    if not pred_path.exists():\n",
    "        raise FileNotFoundError(\"Missing `test_pred_gcn.npy`. Run Phase 3 (GCN stacker) first.\")\n",
    "    preds = np.load(pred_path)\n",
    "    # Load term list (must match Phase 2/3)\n",
    "    terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "    if not terms_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {terms_path}. Re-run Phase 2 target construction.\")\n",
    "    with open(terms_path, 'r') as f:\n",
    "        top_terms = json.load(f)\n",
    "    if preds.shape[1] != len(top_terms):\n",
    "        raise ValueError(f\"Shape mismatch: preds has {preds.shape[1]} terms, top_terms has {len(top_terms)}.\")\n",
    "    # ------------------------------------------\n",
    "    # Strict post-processing (Max/Min Propagation)\n",
    "    # ------------------------------------------\n",
    "    print(f\"Applying hierarchy rules on {len(top_terms)} terms...\")\n",
    "    df_pred = pd.DataFrame(preds, columns=top_terms)\n",
    "    term_set = set(top_terms)\n",
    "    term_to_parents = {}\n",
    "    term_to_children = {}\n",
    "    for term in top_terms:\n",
    "        parents = go_parents.get(term, set())\n",
    "        if not parents:\n",
    "            continue\n",
    "        parents = parents.intersection(term_set)\n",
    "        if not parents:\n",
    "            continue\n",
    "        term_to_parents[term] = list(parents)\n",
    "        for p in parents:\n",
    "            term_to_children.setdefault(p, []).append(term)\n",
    "    # Max Propagation (Child -> Parent)\n",
    "    for _ in range(2):\n",
    "        for child, parents in term_to_parents.items():\n",
    "            child_scores = df_pred[child].values\n",
    "            for parent in parents:\n",
    "                df_pred[parent] = np.maximum(df_pred[parent].values, child_scores)\n",
    "    # Min Propagation (Parent -> Child)\n",
    "    for _ in range(2):\n",
    "        for parent, children in term_to_children.items():\n",
    "            parent_scores = df_pred[parent].values\n",
    "            for child in children:\n",
    "                df_pred[child] = np.minimum(df_pred[child].values, parent_scores)\n",
    "    # ------------------------------------------\n",
    "    # Submission formatting (CAFA rules)\n",
    "    # - tab-separated, no header\n",
    "    # - score in (0, 1.000]\n",
    "    # - up to 3 significant figures\n",
    "    # - <= 1500 terms per target (MF/BP/CC combined)\n",
    "    # ------------------------------------------\n",
    "    df_pred['EntryID'] = test_ids.values\n",
    "    submission = df_pred.melt(id_vars='EntryID', var_name='term', value_name='score')\n",
    "    # Enforce score range + remove zeros\n",
    "    submission['score'] = submission['score'].clip(lower=0.0, upper=1.0)\n",
    "    submission = submission[submission['score'] > 0.0]\n",
    "    # Light pruning (keeps file size sane; still rule-compliant)\n",
    "    submission = submission[submission['score'] >= 0.001]\n",
    "    # Keep top 1500 per protein (rule)\n",
    "    submission = submission.sort_values(['EntryID', 'score'], ascending=[True, False])\n",
    "    submission = submission.groupby('EntryID', sort=False).head(1500)\n",
    "    # Write with <= 3 significant figures\n",
    "    submission.to_csv(\n",
    "        WORK_ROOT / 'submission.tsv',\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        header=False,\n",
    "        float_format='%.3g',\n",
    "    )\n",
    "    print(f\"Done! Submission saved to {WORK_ROOT / 'submission.tsv'}\")\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.maybe_push('stage_09_submission', [WORK_ROOT / 'submission.tsv'], note='final submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644cdb6",
   "metadata": {
    "id": "a644cdb6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 17 - Solution: 7. PHASE 5: FREE TEXT PREDICTION (EXTRA)\n",
    "# 7. PHASE 5: FREE TEXT PREDICTION (EXTRA)\n",
    "# ==========================================\n",
    "# HARDWARE: CPU\n",
    "# ==========================================\n",
    "# Official CAFA constraints (summary):\n",
    "# - Combined file (GO + Text) allowed\n",
    "# - Text: up to 5 lines per protein; ASCII printable; no tabs; <=3000 chars per protein total\n",
    "# - Scores should be in (0, 1.000] and up to 3 significant figures\n",
    "if (WORK_ROOT / 'submission_with_text.tsv').exists():\n",
    "    print(\"submission_with_text.tsv already exists. Skipping Phase 5.\")\n",
    "elif not (WORK_ROOT / 'submission.tsv').exists():\n",
    "    print(\"submission.tsv not found. Please run Phase 4 first.\")\n",
    "else:\n",
    "    print(\"Starting Phase 5: Text Generation...\")\n",
    "    # 1. Load Submission & GO Graph\n",
    "    print(\"Loading submission and GO data...\")\n",
    "    submission = pd.read_csv(\n",
    "        WORK_ROOT / 'submission.tsv',\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=['EntryID', 'term', 'score'],\n",
    "    )\n",
    "    if 'graph' not in locals():\n",
    "        import obonet\n",
    "        graph = obonet.read_obo(PATH_GO_OBO)\n",
    "    # 2. Generate Text Descriptions\n",
    "    print(\"Generating descriptions...\")\n",
    "    # Pre-fetch term names to avoid graph lookups in loop\n",
    "    term_names = {node: data.get('name', 'unknown function') for node, data in graph.nodes(data=True)}\n",
    "    text_rows = []\n",
    "    unique_ids = submission['EntryID'].unique()\n",
    "    for protein_id in tqdm(unique_ids, desc=\"Generating Text\"):\n",
    "        prot_preds = submission[submission['EntryID'] == protein_id]\n",
    "        top_go = prot_preds.sort_values('score', ascending=False).head(3)\n",
    "        if top_go.empty:\n",
    "            # If we have no GO lines for this protein, skip text (keeps score>0 rule clean)\n",
    "            continue\n",
    "        term_descs = []\n",
    "        for _, row in top_go.iterrows():\n",
    "            term_id = row['term']\n",
    "            term_descs.append(term_names.get(term_id, term_id))\n",
    "        joined_terms = \", \".join(term_descs)\n",
    "        description = f\"{protein_id} is predicted to be involved in: {joined_terms}.\"\n",
    "        # Ensure no tabs in description\n",
    "        description = description.replace('\\t', ' ')\n",
    "        # Score: strictly > 0 and <= 1\n",
    "        score = float(top_go.iloc[0]['score'])\n",
    "        score = min(max(score, 0.001), 1.0)\n",
    "        # One line per protein (<=5 allowed)\n",
    "        text_rows.append({\n",
    "            'EntryID': protein_id,\n",
    "            'term': 'Text',\n",
    "            'score': score,\n",
    "            'description': description,\n",
    "        })\n",
    "    df_text = pd.DataFrame(text_rows)\n",
    "    print(\"Saving combined submission...\")\n",
    "    with open(WORK_ROOT / 'submission_with_text.tsv', 'w', encoding='utf-8') as f:\n",
    "        # 1) GO preds (3 cols) already CAFA-formatted in Phase 4\n",
    "        submission.to_csv(f, sep='\\t', index=False, header=False, float_format='%.3g')\n",
    "        # 2) Text preds (4 cols)\n",
    "        for _, row in df_text.iterrows():\n",
    "            # Up to 3 significant figures\n",
    "            score_str = format(float(row['score']), '.3g')\n",
    "            f.write(f\"{row['EntryID']}\\tText\\t{score_str}\\t{row['description']}\\n\")\n",
    "    print(f\"Done! Combined submission saved to {WORK_ROOT / 'submission_with_text.tsv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 18 - Diagnostics: what actually contributes (OOF ablations; read-only)\n",
    "# Goal: identify which Level-1 predictors help/hurt an OOF mean-ensemble under IA-F1.\n",
    "# This cell ONLY reads artefacts and plots; it does not affect training or saved outputs.\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "except Exception as e:\n",
    "    plt = None\n",
    "    sns = None\n",
    "    print('Plotting libs not available; will print tables only:', repr(e))\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "DIAG_N = int(os.getenv('CAFA_DIAG_N', '20000'))\n",
    "THRS = np.linspace(0.05, 0.60, 23)\n",
    "\n",
    "def _load_pred(stem: str) -> np.ndarray | None:\n",
    "    # Prefer level1_preds folder, fall back to legacy root folder\n",
    "    cands = [\n",
    "        WORK_ROOT / 'features' / 'level1_preds' / f'{stem}.npy',\n",
    "        WORK_ROOT / 'features' / f'{stem}.npy',\n",
    "    ]\n",
    "    for p in cands:\n",
    "        if p.exists():\n",
    "            return np.load(p)\n",
    "    return None\n",
    "\n",
    "def _load_targets_topk() -> tuple[np.ndarray, list[str]]:\n",
    "    train_terms_path = WORK_ROOT / 'parsed' / 'train_terms.parquet'\n",
    "    train_seq_path = WORK_ROOT / 'parsed' / 'train_seq.feather'\n",
    "    top_terms_path = WORK_ROOT / 'features' / 'top_terms_13500.json'\n",
    "    if not train_terms_path.exists() or not train_seq_path.exists():\n",
    "        raise FileNotFoundError('Missing parsed targets; expected parsed/train_terms.parquet and parsed/train_seq.feather')\n",
    "    train_terms = pd.read_parquet(train_terms_path)\n",
    "    train_ids_raw = pd.read_feather(train_seq_path)['id'].astype(str)\n",
    "    train_ids = train_ids_raw.str.extract(r'\\|(.*?)\\|', expand=False).fillna(train_ids_raw).tolist()\n",
    "    if top_terms_path.exists():\n",
    "        import json\n",
    "        top_terms = json.loads(top_terms_path.read_text())\n",
    "    else:\n",
    "        # Fallback: recompute top terms from training labels\n",
    "        top_terms = train_terms['term'].value_counts().head(13500).index.tolist()\n",
    "    train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "    y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "    y_df = y_df.reindex(train_ids, fill_value=0)\n",
    "    y_df = y_df.reindex(columns=top_terms, fill_value=0)\n",
    "    y = y_df.values.astype(np.float32)\n",
    "    return y, top_terms\n",
    "\n",
    "def _load_ia_weights(top_terms: list[str]) -> np.ndarray:\n",
    "    # IA.tsv is required under DATASET_ROOT; still guard for robustness\n",
    "    ia_path = WORK_ROOT / 'IA.tsv'\n",
    "    if not ia_path.exists():\n",
    "        return np.ones((len(top_terms),), dtype=np.float32)\n",
    "    ia_df = pd.read_csv(ia_path, sep='\\t', names=['term', 'ia'])\n",
    "    ia_map = dict(zip(ia_df['term'], ia_df['ia']))\n",
    "    w = np.array([float(ia_map.get(t, 0.0) or 0.0) for t in top_terms], dtype=np.float32)\n",
    "    # Avoid all-zeros (would collapse the metric)\n",
    "    if float(w.sum()) <= 0:\n",
    "        w = np.ones_like(w)\n",
    "    return w\n",
    "\n",
    "def _subsample_rows(y_true: np.ndarray, *ys: np.ndarray) -> tuple[np.ndarray, list[np.ndarray]]:\n",
    "    n = int(y_true.shape[0])\n",
    "    m = min(n, int(DIAG_N))\n",
    "    if m <= 0:\n",
    "        return y_true[:0], [a[:0] for a in ys]\n",
    "    idx = np.linspace(0, n - 1, num=m, dtype=np.int64)\n",
    "    out = [np.asarray(a[idx], dtype=np.float32) for a in ys]\n",
    "    return np.asarray(y_true[idx], dtype=np.float32), out\n",
    "\n",
    "def _ia_f1(y_true: np.ndarray, y_score: np.ndarray, weights: np.ndarray, thr: float) -> float:\n",
    "    # IA-weighted F1 over ALL terms (no aspect split here; goal is model contribution ranking)\n",
    "    y_true = (y_true > 0).astype(np.int8)\n",
    "    y_pred = (y_score >= float(thr)).astype(np.int8)\n",
    "    tp = (y_pred & y_true).sum(axis=0).astype(np.float64)\n",
    "    pred = y_pred.sum(axis=0).astype(np.float64)\n",
    "    true = y_true.sum(axis=0).astype(np.float64)\n",
    "    w = weights.astype(np.float64)\n",
    "    w_tp = float((w * tp).sum())\n",
    "    w_pred = float((w * pred).sum())\n",
    "    w_true = float((w * true).sum())\n",
    "    p = (w_tp / w_pred) if w_pred > 0 else 0.0\n",
    "    r = (w_tp / w_true) if w_true > 0 else 0.0\n",
    "    return (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "\n",
    "def _best_over_thrs(y_true: np.ndarray, y_score: np.ndarray, weights: np.ndarray) -> tuple[float, float, np.ndarray]:\n",
    "    scores = np.array([_ia_f1(y_true, y_score, weights, thr=float(t)) for t in THRS], dtype=np.float32)\n",
    "    best_i = int(np.argmax(scores))\n",
    "    return float(THRS[best_i]), float(scores[best_i]), scores\n",
    "# ----------------------------\n",
    "# Load Y + predictions\n",
    "# ----------------------------\n",
    "try:\n",
    "    Y_full, top_terms = _load_targets_topk()\n",
    "    w = _load_ia_weights(top_terms)\n",
    "except Exception as e:\n",
    "    print('Cannot run contributor analysis (missing targets/IA):', repr(e))\n",
    "    raise\n",
    "models = {\n",
    "    'logreg': _load_pred('oof_pred_logreg'),\n",
    "    'gbdt': _load_pred('oof_pred_gbdt'),\n",
    "    'dnn': _load_pred('oof_pred_dnn'),\n",
    "    'knn': _load_pred('oof_pred_knn'),\n",
    "}\n",
    "models = {k: v for k, v in models.items() if v is not None}\n",
    "if not models:\n",
    "    print('No OOF prediction matrices found. Expected files like features/level1_preds/oof_pred_logreg.npy')\n",
    "    raise SystemExit(0)\n",
    "# Sanity: shape alignment\n",
    "for k, v in models.items():\n",
    "    if v.shape != Y_full.shape:\n",
    "        raise ValueError(f'Model {k} shape mismatch: got {v.shape}, expected {Y_full.shape}')\n",
    "# Baseline = mean ensemble\n",
    "stack = np.mean(np.stack(list(models.values()), axis=0), axis=0).astype(np.float32)\n",
    "Y_sub, [stack_sub] = _subsample_rows(Y_full, stack)\n",
    "thr0, s0, curve0 = _best_over_thrs(Y_sub, stack_sub, w)\n",
    "print(f'Baseline mean-ensemble best IA-F1={s0:.4f} @ thr={thr0:.3f} (sampled N={len(Y_sub)})')\n",
    "# Ablations\n",
    "rows = []\n",
    "keys = list(models.keys())\n",
    "for drop in keys:\n",
    "    keep = [k for k in keys if k != drop]\n",
    "    st = np.mean(np.stack([models[k] for k in keep], axis=0), axis=0).astype(np.float32)\n",
    "    Y_sub, [st_sub] = _subsample_rows(Y_full, st)\n",
    "    thr, s, _ = _best_over_thrs(Y_sub, st_sub, w)\n",
    "    rows.append({'drop': drop, 'best_ia_f1': s, 'best_thr': thr, 'delta_vs_base': s - s0})\n",
    "ab = pd.DataFrame(rows).sort_values('delta_vs_base', ascending=True)\n",
    "print('\\nAblation deltas (negative = dropping hurts):')\n",
    "print(ab)\n",
    "if plt is not None:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.barh(ab['drop'], ab['delta_vs_base'])\n",
    "    plt.axvline(0.0, color='k', linewidth=1)\n",
    "    plt.title('OOF mean-ensemble: IA-F1 change when dropping one model (sampled)')\n",
    "    plt.xlabel('Δ IA-F1 vs baseline')\n",
    "    plt.ylabel('dropped model')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
