{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42ecde8",
   "metadata": {
    "id": "bootstrap",
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ git clone --depth 1 https://github.com/PeterOla/cafa-6-protein-function-prediction.git /content/cafa-6-protein-function-prediction\n",
      "Cloning into '/content/cafa-6-protein-function-prediction'...\n",
      "\n",
      "CWD: /content/cafa-6-protein-function-prediction\n"
     ]
    }
   ],
   "source": [
    "# CELL 01 - Bootstrap repo (clone/update)\n",
    "# Colab bootstrap (no external notebook/script execution)\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = os.environ.get('CAFA_REPO_GIT_URL', 'https://github.com/PeterOla/cafa-6-protein-function-prediction.git')\n",
    "REPO_DIR = Path(os.environ.get('CAFA_REPO_DIR', '/content/cafa-6-protein-function-prediction'))\n",
    "SAFE_CWD = Path('/content') if Path('/content').exists() else Path('/')\n",
    "def run(cmd: list[str]) -> None:\n",
    "    cmd_str = ' '.join(cmd)\n",
    "    print('+', cmd_str)\n",
    "    p = subprocess.run(cmd, text=True, capture_output=True, cwd=str(SAFE_CWD))\n",
    "    if p.stdout.strip():\n",
    "        print(p.stdout)\n",
    "    if p.stderr.strip():\n",
    "        print(p.stderr)\n",
    "    if p.returncode != 0:\n",
    "        raise RuntimeError(f'Command failed (exit={p.returncode}): {cmd_str}')\n",
    "os.chdir(SAFE_CWD)\n",
    "if REPO_DIR.exists() and (REPO_DIR / '.git').is_dir():\n",
    "    run(['git', '-C', str(REPO_DIR), 'fetch', '--depth', '1', 'origin'])\n",
    "    run(['git', '-C', str(REPO_DIR), 'reset', '--hard', 'origin/HEAD'])\n",
    "else:\n",
    "    if REPO_DIR.exists():\n",
    "        shutil.rmtree(REPO_DIR, ignore_errors=True)\n",
    "    run(['git', 'clone', '--depth', '1', REPO_URL, str(REPO_DIR)])\n",
    "os.chdir(REPO_DIR)\n",
    "print('CWD:', Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missing dependencies\n",
    "!pip install py-boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94540c0a",
   "metadata": {
    "id": "install",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 02 - Install dependencies\n",
    "import importlib.util\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def _detect_kaggle() -> bool:\n",
    "    return bool(os.environ.get('KAGGLE_KERNEL_RUN_TYPE') or os.environ.get('KAGGLE_URL_BASE') or os.environ.get('KAGGLE_DATA_PROXY_URL'))\n",
    "def _detect_colab() -> bool:\n",
    "    return bool(os.environ.get('COLAB_RELEASE_TAG') or os.environ.get('COLAB_GPU') or os.environ.get('COLAB_TPU_ADDR'))\n",
    "IS_KAGGLE = _detect_kaggle()\n",
    "IS_COLAB = (not IS_KAGGLE) and _detect_colab()\n",
    "if IS_KAGGLE:\n",
    "    print('Environment: Kaggle Detected')\n",
    "elif IS_COLAB:\n",
    "    print('Environment: Colab Detected')\n",
    "else:\n",
    "    print('Environment: Local Detected')\n",
    "FORCE_PIP = os.environ.get('CAFA_FORCE_PIP', '0').strip() == '1'\n",
    "\n",
    "def _pip_install(args: list[str]) -> None:\n",
    "    print('+', sys.executable, '-m', 'pip', 'install', *args)\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install', *args])\n",
    "# Kaggle has a heavily preinstalled environment; avoid upgrading core packages (pandas/notebook/requests/tornado/RAPIDS).\n",
    "# Only install a small set if missing, unless CAFA_FORCE_PIP=1.\n",
    "if IS_KAGGLE and not FORCE_PIP:\n",
    "    minimal = ['obonet', 'biopython', 'pyyaml', 'py-boost']\n",
    "    missing = [p for p in minimal if importlib.util.find_spec(p) is None]\n",
    "    if missing:\n",
    "        _pip_install(missing)\n",
    "    else:\n",
    "        print('Kaggle: skipping pip install (already satisfied). Set CAFA_FORCE_PIP=1 to force.')\n",
    "else:\n",
    "    _pip_install(['-r', 'requirements.txt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 02b - Safety switches (recommended for \"reuse artefacts\" runs)\n",
    "# This notebook is designed to *reuse* an existing checkpoint dataset. To avoid accidental multi-GB uploads,\n",
    "# we disable checkpoint pushes by default. Opt-in by setting CAFA_CHECKPOINT_PUSH=1.\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ.setdefault('CAFA_CHECKPOINT_PULL', '1')\n",
    "os.environ.setdefault('CAFA_CHECKPOINT_REQUIRED', '0')\n",
    "# In Colab, pulling the entire checkpoint dataset in one go can get the kaggle process SIGKILL'd (return code -9).\n",
    "# Default to lean pulls: grab parsed + external first; only pull features.zip when you actually need embeddings/preds.\n",
    "os.environ.setdefault('CAFA_CHECKPOINT_PULL_FILES', 'parsed.zip,external.zip')\n",
    "os.environ.setdefault('CAFA_CHECKPOINT_PUSH', '0')\n",
    "os.environ.setdefault('CAFA_CHECKPOINT_PUSH_EXISTING', '0')\n",
    "\n",
    "print('CAFA_CHECKPOINT_PULL:', os.environ.get('CAFA_CHECKPOINT_PULL'))\n",
    "print('CAFA_CHECKPOINT_REQUIRED:', os.environ.get('CAFA_CHECKPOINT_REQUIRED'))\n",
    "print('CAFA_CHECKPOINT_PULL_FILES:', os.environ.get('CAFA_CHECKPOINT_PULL_FILES'))\n",
    "print('CAFA_CHECKPOINT_PUSH :', os.environ.get('CAFA_CHECKPOINT_PUSH'))\n",
    "print('CAFA_CHECKPOINT_PUSH_EXISTING:', os.environ.get('CAFA_CHECKPOINT_PUSH_EXISTING'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858dad8",
   "metadata": {
    "id": "1858dad8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 03 - Solution: 1. SETUP, CONFIG & DIAGNOSTICS\n",
    "# 1. SETUP, CONFIG & DIAGNOSTICS\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------\n",
    "# Environment Detection & Paths\n",
    "# ------------------------------------------\n",
    "# Kaggle images can have `google-colab` installed; never use `import google.colab` as a signal.\n",
    "\n",
    "def _detect_kaggle() -> bool:\n",
    "    # Kaggle kernels reliably set at least one of these env vars.\n",
    "    return bool(\n",
    "        os.environ.get('KAGGLE_KERNEL_RUN_TYPE')\n",
    "        or os.environ.get('KAGGLE_URL_BASE')\n",
    "        or os.environ.get('KAGGLE_DATA_PROXY_URL')\n",
    "    )\n",
    "\n",
    "\n",
    "def _detect_colab() -> bool:\n",
    "    # Colab sets these env vars; this avoids false positives on Kaggle.\n",
    "    return bool(os.environ.get('COLAB_RELEASE_TAG') or os.environ.get('COLAB_GPU') or os.environ.get('COLAB_TPU_ADDR'))\n",
    "\n",
    "\n",
    "IS_KAGGLE = _detect_kaggle()\n",
    "IS_COLAB = (not IS_KAGGLE) and _detect_colab()\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print('Environment: Kaggle Detected')\n",
    "    INPUT_ROOT = Path('/kaggle/input')\n",
    "    WORKING_ROOT = Path('/kaggle/working')\n",
    "    if INPUT_ROOT.exists():\n",
    "        for dirname, _, filenames in os.walk(str(INPUT_ROOT)):\n",
    "            for filename in filenames:\n",
    "                print(os.path.join(dirname, filename))\n",
    "elif IS_COLAB:\n",
    "    print('Environment: Colab Detected')\n",
    "    INPUT_ROOT = Path(os.environ.get('CAFA_INPUT_ROOT', str(Path('/content'))))\n",
    "    WORKING_ROOT = Path(os.environ.get('CAFA_WORKING_ROOT', str(Path('/content'))))\n",
    "else:\n",
    "    print('Environment: Local Detected')\n",
    "    CURRENT_DIR = Path.cwd()\n",
    "    if CURRENT_DIR.name == 'notebooks':\n",
    "        PROJECT_ROOT = CURRENT_DIR.parent\n",
    "    else:\n",
    "        PROJECT_ROOT = CURRENT_DIR\n",
    "    INPUT_ROOT = Path(os.environ.get('CAFA_INPUT_ROOT', str(PROJECT_ROOT)))\n",
    "    WORKING_ROOT = Path(os.environ.get('CAFA_WORKING_ROOT', str(PROJECT_ROOT / 'artefacts_local')))\n",
    "    WORKING_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Local cache roots (ephemeral) + published artefacts root\n",
    "# ------------------------------------------\n",
    "# IMPORTANT: we always write locally first (every runtime needs a write path),\n",
    "# but the Kaggle Dataset is the *single source of truth* for resumability.\n",
    "\n",
    "WORK_ROOT = Path(os.environ.get('CAFA_WORK_ROOT', str(WORKING_ROOT / 'work')))\n",
    "WORK_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "(WORK_ROOT / 'parsed').mkdir(parents=True, exist_ok=True)\n",
    "(WORK_ROOT / 'features').mkdir(parents=True, exist_ok=True)\n",
    "(WORK_ROOT / 'external').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Keep caches OUT of WORK_ROOT so we never accidentally publish them.\n",
    "CACHE_ROOT = Path(os.environ.get('CAFA_CACHE_ROOT', str(WORKING_ROOT / 'cache')))\n",
    "CACHE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "os.environ.setdefault('HF_HOME', str(CACHE_ROOT / 'hf_home'))\n",
    "os.environ.setdefault('TRANSFORMERS_CACHE', str(CACHE_ROOT / 'hf_home'))\n",
    "os.environ.setdefault('HF_HUB_CACHE', str(CACHE_ROOT / 'hf_hub'))\n",
    "os.environ.setdefault('TORCH_HOME', str(CACHE_ROOT / 'torch_home'))\n",
    "\n",
    "# ------------------------------------------\n",
    "# Dataset Discovery (competition data)\n",
    "# ------------------------------------------\n",
    "\n",
    "DATASET_SLUG = 'cafa-6-protein-function-prediction'\n",
    "\n",
    "\n",
    "def _score_dataset_dir(p: Path) -> int:\n",
    "    return (\n",
    "        int((p / 'Train').exists())\n",
    "        + int((p / 'Test').exists())\n",
    "        + int((p / 'IA.tsv').exists())\n",
    "        + int((p / 'sample_submission.tsv').exists())\n",
    "    )\n",
    "\n",
    "\n",
    "def find_dataset_root(input_root: Path, dataset_slug: str) -> Path:\n",
    "    override = (os.environ.get('CAFA_DATASET_ROOT') or '').strip()\n",
    "    if override:\n",
    "        p = Path(override)\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise FileNotFoundError(f'CAFA_DATASET_ROOT is set but does not exist: {p}')\n",
    "\n",
    "    candidate = input_root / dataset_slug\n",
    "    if candidate.exists() and _score_dataset_dir(candidate) >= 2:\n",
    "        return candidate\n",
    "\n",
    "    if _score_dataset_dir(input_root) >= 2:\n",
    "        return input_root\n",
    "\n",
    "    # Shallow scan: input_root/* and input_root/*/* (helps Colab + GitHub clones)\n",
    "    candidates: list[Path] = []\n",
    "    if input_root.exists():\n",
    "        for p in input_root.iterdir():\n",
    "            if not p.is_dir():\n",
    "                continue\n",
    "            candidates.append(p)\n",
    "            try:\n",
    "                for q in p.iterdir():\n",
    "                    if q.is_dir():\n",
    "                        candidates.append(q)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    candidates = sorted(set(candidates), key=_score_dataset_dir, reverse=True)\n",
    "    if candidates and _score_dataset_dir(candidates[0]) >= 2:\n",
    "        return candidates[0]\n",
    "\n",
    "    auto_flag = (os.environ.get('CAFA_COLAB_AUTO_DOWNLOAD') or '').strip() or '<unset>'\n",
    "\n",
    "    # Colab rule: secrets must be fetched ONLY via google.colab.userdata.get(...)\n",
    "    if IS_COLAB:\n",
    "        try:\n",
    "            from google.colab import userdata  # type: ignore\n",
    "\n",
    "            has_user = bool((userdata.get('KAGGLE_USERNAME') or '').strip())\n",
    "            has_key = bool((userdata.get('KAGGLE_KEY') or '').strip())\n",
    "        except Exception:\n",
    "            has_user = False\n",
    "            has_key = False\n",
    "    else:\n",
    "        has_user = bool((os.environ.get('KAGGLE_USERNAME') or '').strip())\n",
    "        has_key = bool((os.environ.get('KAGGLE_KEY') or '').strip())\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f'Dataset not found under {input_root}. '\n",
    "        'Note: a GitHub clone does not include the CAFA competition files.\\n'\n",
    "        f'CAFA_COLAB_AUTO_DOWNLOAD={auto_flag}; Kaggle creds present={has_user and has_key}.\\n'\n",
    "        'Fix options:\\n'\n",
    "        '  1) Set CAFA_DATASET_ROOT to a folder containing Train/ and Test/ (e.g. Drive).\\n'\n",
    "        '  2) (Colab) Set Colab secrets KAGGLE_USERNAME and KAGGLE_KEY, then set CAFA_COLAB_AUTO_DOWNLOAD=1.'\n",
    "    )\n",
    "\n",
    "\n",
    "def _maybe_colab_auto_download_competition() -> None:\n",
    "    if not IS_COLAB:\n",
    "        return\n",
    "\n",
    "    # Default-on in Colab.\n",
    "    # - Disable by explicitly setting CAFA_COLAB_AUTO_DOWNLOAD=0.\n",
    "    os.environ.setdefault('CAFA_COLAB_AUTO_DOWNLOAD', '1')\n",
    "\n",
    "    flag = (os.environ.get('CAFA_COLAB_AUTO_DOWNLOAD') or '').strip()\n",
    "    if flag == '0':\n",
    "        print('Colab auto-download disabled (CAFA_COLAB_AUTO_DOWNLOAD=0).')\n",
    "        return\n",
    "\n",
    "    target_dir = Path(os.environ.get('CAFA_COLAB_DATA_DIR', str(Path('/content/cafa6_data'))))\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if _score_dataset_dir(target_dir) >= 2:\n",
    "        os.environ['CAFA_DATASET_ROOT'] = str(target_dir)\n",
    "        return\n",
    "\n",
    "    # Install kaggle CLI if missing\n",
    "    try:\n",
    "        subprocess.run(['kaggle', '--version'], check=True, capture_output=True, text=True)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install', 'kaggle'])\n",
    "\n",
    "    # Colab rule: fetch secrets ONLY via google.colab.userdata.get(...)\n",
    "    try:\n",
    "        from google.colab import userdata  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise RuntimeError('Colab detected but google.colab.userdata is unavailable.') from e\n",
    "\n",
    "    username = (userdata.get('KAGGLE_USERNAME') or '').strip()\n",
    "    key = (userdata.get('KAGGLE_KEY') or '').strip()\n",
    "    if (not username) or (not key):\n",
    "        raise RuntimeError(\n",
    "            'CAFA_COLAB_AUTO_DOWNLOAD=1 but Kaggle API auth is missing. '\n",
    "            'Set Colab secrets KAGGLE_USERNAME and KAGGLE_KEY.'\n",
    "        )\n",
    "\n",
    "    # Export into env for downstream subprocesses (but do NOT *source* from env in Colab).\n",
    "    os.environ['KAGGLE_USERNAME'] = username\n",
    "    os.environ['KAGGLE_KEY'] = key\n",
    "    env = os.environ.copy()\n",
    "\n",
    "    print(f'Downloading competition data to {target_dir} via Kaggle API...')\n",
    "    # Do NOT rely on kaggle --unzip; unzip ourselves.\n",
    "    p = subprocess.run(\n",
    "        ['kaggle', 'competitions', 'download', '-c', DATASET_SLUG, '-p', str(target_dir)],\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        env=env,\n",
    "    )\n",
    "    if p.returncode != 0:\n",
    "        print(p.stdout)\n",
    "        print(p.stderr)\n",
    "        raise RuntimeError(\n",
    "            'Failed to download competition data. See logs above (you may need to accept the competition rules).'\n",
    "        )\n",
    "\n",
    "    for z in target_dir.glob('*.zip'):\n",
    "        try:\n",
    "            print(f'Unzipping {z.name}...')\n",
    "            with zipfile.ZipFile(z, 'r') as zf:\n",
    "                zf.extractall(target_dir)\n",
    "            z.unlink()\n",
    "        except Exception as e:\n",
    "            print(f'Warning: failed to unzip {z}: {e}')\n",
    "\n",
    "    os.environ['CAFA_DATASET_ROOT'] = str(target_dir)\n",
    "\n",
    "\n",
    "_maybe_colab_auto_download_competition()\n",
    "\n",
    "DATASET_ROOT = find_dataset_root(INPUT_ROOT, DATASET_SLUG)\n",
    "print(f'DATASET_ROOT: {DATASET_ROOT}')\n",
    "\n",
    "# Define input paths\n",
    "PATH_IA = DATASET_ROOT / 'IA.tsv'\n",
    "PATH_SAMPLE_SUB = DATASET_ROOT / 'sample_submission.tsv'\n",
    "PATH_TRAIN_FASTA = DATASET_ROOT / 'Train' / 'train_sequences.fasta'\n",
    "PATH_TRAIN_TERMS = DATASET_ROOT / 'Train' / 'train_terms.tsv'\n",
    "PATH_TRAIN_TAXON = DATASET_ROOT / 'Train' / 'train_taxonomy.tsv'\n",
    "PATH_GO_OBO = DATASET_ROOT / 'Train' / 'go-basic.obo'\n",
    "PATH_TEST_FASTA = DATASET_ROOT / 'Test' / 'testsuperset.fasta'\n",
    "PATH_TEST_TAXON = DATASET_ROOT / 'Test' / 'testsuperset-taxon-list.tsv'\n",
    "\n",
    "# ------------------------------------------\n",
    "# Sanity Checks\n",
    "# ------------------------------------------\n",
    "\n",
    "required = {\n",
    "    'IA.tsv': PATH_IA,\n",
    "    'Train/train_sequences.fasta': PATH_TRAIN_FASTA,\n",
    "    'Train/train_terms.tsv': PATH_TRAIN_TERMS,\n",
    "    'Train/go-basic.obo': PATH_GO_OBO,\n",
    "}\n",
    "missing = {k: v for k, v in required.items() if not v.exists()}\n",
    "if missing:\n",
    "    raise FileNotFoundError(f'Missing files: {missing}')\n",
    "print('All required inputs found.')\n",
    "\n",
    "# ------------------------------------------\n",
    "# Fail-fast: FASTA readability checks (path issues)\n",
    "# ------------------------------------------\n",
    "\n",
    "def _fasta_smoke_test(path: Path, label: str, max_lines: int = 20000) -> None:\n",
    "    path = Path(path)\n",
    "\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"{label} FASTA not found: {path}\\n\"\n",
    "            f\"DATASET_ROOT={DATASET_ROOT}\\n\"\n",
    "            \"If you're on Colab, ensure the competition data downloaded/unzipped correctly,\\n\"\n",
    "            \"or set CAFA_DATASET_ROOT to the folder containing Train/ and Test/.\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        size = path.stat().st_size\n",
    "    except Exception:\n",
    "        size = None\n",
    "\n",
    "    if size is not None and size == 0:\n",
    "        raise RuntimeError(f\"{label} FASTA file is empty (0 bytes): {path}\")\n",
    "\n",
    "    headers = 0\n",
    "    first_nonempty = None\n",
    "    try:\n",
    "        with path.open('r', encoding='utf-8', errors='ignore') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                s = line.strip()\n",
    "                if not s:\n",
    "                    continue\n",
    "                if first_nonempty is None:\n",
    "                    first_nonempty = s\n",
    "                if s.startswith('>'):\n",
    "                    headers += 1\n",
    "                if i >= max_lines:\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed reading {label} FASTA at {path}: {type(e).__name__}: {e}\")\n",
    "\n",
    "    if first_nonempty is None:\n",
    "        raise RuntimeError(f\"{label} FASTA appears empty/unreadable (no non-empty lines): {path}\")\n",
    "\n",
    "    if not first_nonempty.startswith('>'):\n",
    "        raise RuntimeError(\n",
    "            f\"{label} FASTA does not look like FASTA (first content line doesn't start with '>'): {path}\\n\"\n",
    "            f\"First line was: {first_nonempty[:120]!r}\"\n",
    "        )\n",
    "\n",
    "    if headers == 0:\n",
    "        raise RuntimeError(f\"{label} FASTA had zero headers in the first {max_lines} lines: {path}\")\n",
    "\n",
    "\n",
    "print('FASTA smoke tests:')\n",
    "_fasta_smoke_test(PATH_TRAIN_FASTA, 'Train')\n",
    "print('  Train FASTA: OK')\n",
    "\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    _fasta_smoke_test(PATH_TEST_FASTA, 'Test')\n",
    "    print('  Test FASTA: OK')\n",
    "else:\n",
    "    print(f'  Test FASTA: MISSING at {PATH_TEST_FASTA} (continuing; some steps may fail later)')\n",
    "\n",
    "# ------------------------------------------\n",
    "# Checkpoint store (Kaggle Dataset = single source of truth)\n",
    "# ------------------------------------------\n",
    "\n",
    "def _get_secret(name: str) -> str:\n",
    "    # Colab rule: secrets must be fetched ONLY via google.colab.userdata.get(...).\n",
    "    if IS_COLAB:\n",
    "        try:\n",
    "            from google.colab import userdata  # type: ignore\n",
    "\n",
    "            return (userdata.get(name) or '').strip()\n",
    "        except Exception:\n",
    "            return ''\n",
    "\n",
    "    # Non-Colab: allow env var -> Kaggle Secrets.\n",
    "    v = (os.environ.get(name, '') or '').strip()\n",
    "    if v:\n",
    "        return v\n",
    "\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient  # type: ignore\n",
    "\n",
    "        v = (UserSecretsClient().get_secret(name) or '').strip()\n",
    "        if v:\n",
    "            return v\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return ''\n",
    "\n",
    "\n",
    "# Kaggle Secrets are NOT automatically environment variables.\n",
    "# Resolve CAFA_FORCE_REBUILD via the same secret lookup and export it so later cells\n",
    "# (which use os.getenv) behave consistently.\n",
    "_raw_force_rebuild = (_get_secret('CAFA_FORCE_REBUILD') or '').strip()\n",
    "if _raw_force_rebuild:\n",
    "    os.environ.setdefault('CAFA_FORCE_REBUILD', _raw_force_rebuild)\n",
    "\n",
    "\n",
    "def _truthy(v: str) -> bool:\n",
    "    return str(v).strip().lower() in {'1', 'true', 'yes', 'y'}\n",
    "\n",
    "\n",
    "print('CAFA_FORCE_REBUILD (env):', repr(os.environ.get('CAFA_FORCE_REBUILD', '')))\n",
    "print('CAFA_FORCE_REBUILD (effective):', int(_truthy(os.environ.get('CAFA_FORCE_REBUILD', '0'))))\n",
    "\n",
    "\n",
    "CHECKPOINT_DATASET_ID = (\n",
    "    _get_secret('CAFA_CHECKPOINT_DATASET_ID')\n",
    "    or _get_secret('CAFA_KAGGLE_DATASET_ID')\n",
    ")\n",
    "CHECKPOINT_DATASET_TITLE = os.environ.get('CAFA_CHECKPOINT_DATASET_TITLE', 'CAFA6 Checkpoints').strip()\n",
    "CHECKPOINT_PULL = os.environ.get('CAFA_CHECKPOINT_PULL', '1').strip() == '1'\n",
    "CHECKPOINT_PUSH = os.environ.get('CAFA_CHECKPOINT_PUSH', '1').strip() == '1'\n",
    "MANIFEST_PATH = WORK_ROOT / 'manifest.json'\n",
    "\n",
    "\n",
    "def _get_kaggle_token() -> str:\n",
    "    return _get_secret('KAGGLE_API_TOKEN')\n",
    "\n",
    "\n",
    "def _get_kaggle_user_key() -> tuple[str, str]:\n",
    "    # Kaggle CLI expects Kaggle API credentials: username + key.\n",
    "    username = _get_secret('KAGGLE_USERNAME')\n",
    "    key = _get_secret('KAGGLE_KEY')\n",
    "    if username and key:\n",
    "        return username, key\n",
    "\n",
    "    # Back-compat: allow KAGGLE_API_TOKEN to carry either JSON ({username,key}) or 'username:key'.\n",
    "    tok = _get_kaggle_token()\n",
    "    if tok:\n",
    "        try:\n",
    "            obj = json.loads(tok)\n",
    "            username = (obj.get('username') or '').strip()\n",
    "            key = (obj.get('key') or '').strip()\n",
    "            if username and key:\n",
    "                return username, key\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if ':' in tok:\n",
    "            u, k = tok.split(':', 1)\n",
    "            username = u.strip()\n",
    "            key = k.strip()\n",
    "            if username and key:\n",
    "                return username, key\n",
    "\n",
    "    return '', ''\n",
    "\n",
    "\n",
    "def _kaggle_env(require: bool = False) -> dict[str, str]:\n",
    "    env = os.environ.copy()\n",
    "    username, key = _get_kaggle_user_key()\n",
    "    if username and key:\n",
    "        # Export into both subprocess env (this call) and process env (subsequent cells).\n",
    "        env['KAGGLE_USERNAME'] = username\n",
    "        env['KAGGLE_KEY'] = key\n",
    "        os.environ['KAGGLE_USERNAME'] = username\n",
    "        os.environ['KAGGLE_KEY'] = key\n",
    "\n",
    "    if require and (not env.get('KAGGLE_USERNAME') or not env.get('KAGGLE_KEY')):\n",
    "        raise RuntimeError(\n",
    "            'Kaggle API auth missing. The `kaggle` CLI requires `KAGGLE_USERNAME` + `KAGGLE_KEY` '\n",
    "            '(set them as env vars or Kaggle/Colab secrets). Or attach the '\n",
    "            'checkpoint dataset as an Input so `STORE.pull()` can use the mounted copy.'\n",
    "        )\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "def _ensure_kaggle_cli() -> None:\n",
    "    try:\n",
    "        subprocess.run(['kaggle', '--version'], check=True, capture_output=True, text=True)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install', 'kaggle'])\n",
    "        subprocess.run(['kaggle', '--version'], check=True)\n",
    "\n",
    "\n",
    "def _copy_merge(src: Path, dst: Path) -> None:\n",
    "    src = Path(src)\n",
    "    dst = Path(dst)\n",
    "\n",
    "    for p in src.rglob('*'):\n",
    "        if p.is_dir():\n",
    "            continue\n",
    "        rel = p.relative_to(src)\n",
    "        out = dst / rel\n",
    "        out.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(p, out)\n",
    "\n",
    "\n",
    "def _maybe_unpack_dir_mode_zips(work_root: Path) -> None:\n",
    "    # Kaggle CLI with `--dir-mode zip` uploads directories as `parsed.zip`, `external.zip`, etc.\n",
    "    # We unpack them back into folders so the pipeline can resume from `WORK_ROOT/{parsed,external,features}/...`.\n",
    "    work_root = Path(work_root)\n",
    "    for name in ['parsed', 'external', 'features']:\n",
    "        zpath = work_root / f'{name}.zip'\n",
    "        if not zpath.exists():\n",
    "            continue\n",
    "\n",
    "        target_dir = work_root / name\n",
    "        target_dir.mkdir(parents=True, exist_ok=True)\n",
    "        needs_unpack = (not any(target_dir.rglob('*')))\n",
    "        if not needs_unpack:\n",
    "            # Folder already populated; keep the zip as-is (it might be a newer version).\n",
    "            continue\n",
    "\n",
    "        print(f'Unpacking checkpoint archive: {zpath} -> {target_dir}')\n",
    "        with zipfile.ZipFile(zpath, 'r') as zf:\n",
    "            zf.extractall(target_dir)\n",
    "        try:\n",
    "            zpath.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def _load_manifest() -> dict:\n",
    "    if MANIFEST_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(MANIFEST_PATH.read_text(encoding='utf-8'))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "\n",
    "def _update_manifest(stage: str, required_paths: list[Path], note: str = '') -> None:\n",
    "    m = _load_manifest()\n",
    "    stages = m.get('stages', {})\n",
    "    files = []\n",
    "\n",
    "    for p in required_paths:\n",
    "        p = Path(p)\n",
    "        rel = str(p.relative_to(WORK_ROOT)) if str(p).startswith(str(WORK_ROOT)) else str(p)\n",
    "        files.append({'path': rel, 'bytes': int(p.stat().st_size) if p.exists() else None})\n",
    "\n",
    "    stages[stage] = {\n",
    "        'ts_utc': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "        'note': note,\n",
    "        'files': files,\n",
    "    }\n",
    "\n",
    "    m['stages'] = stages\n",
    "    MANIFEST_PATH.write_text(json.dumps(m, indent=2), encoding='utf-8')\n",
    "\n",
    "\n",
    "def _stage_files_signature(required_paths: list[Path]) -> list[dict]:\n",
    "    sig = []\n",
    "    for p in required_paths:\n",
    "        p = Path(p)\n",
    "        rel = str(p.relative_to(WORK_ROOT)) if str(p).startswith(str(WORK_ROOT)) else str(p)\n",
    "        sig.append({'path': rel, 'bytes': int(p.stat().st_size) if p.exists() else None})\n",
    "    return sorted(sig, key=lambda x: x['path'])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class KaggleCheckpointStore:\n",
    "    work_root: Path\n",
    "    dataset_id: str\n",
    "    dataset_title: str\n",
    "    pull_enabled: bool\n",
    "    push_enabled: bool\n",
    "    input_root: Path\n",
    "    is_kaggle: bool\n",
    "\n",
    "    @property\n",
    "    def mount_dir(self) -> Path | None:\n",
    "        if not self.is_kaggle or not self.dataset_id:\n",
    "            return None\n",
    "        slug = self.dataset_id.split('/')[-1]\n",
    "        p = self.input_root / slug\n",
    "        return p if p.exists() else None\n",
    "\n",
    "    def pull(self) -> None:\n",
    "        if not self.pull_enabled:\n",
    "            print('Checkpoint pull disabled (CAFA_CHECKPOINT_PULL=0).')\n",
    "            return\n",
    "\n",
    "        checkpoint_required = str(os.environ.get('CAFA_CHECKPOINT_REQUIRED', '1')).strip().lower() in {'1', 'true', 'yes'}\n",
    "        if not self.dataset_id:\n",
    "            msg = 'Missing CAFA_CHECKPOINT_DATASET_ID=<user>/<slug>; cannot resume.'\n",
    "            if checkpoint_required:\n",
    "                raise ValueError(msg)\n",
    "            print('WARNING: ' + msg)\n",
    "            return\n",
    "\n",
    "        if self.mount_dir is not None:\n",
    "            print(f'Pulling checkpoints from Kaggle mounted dataset: {self.mount_dir}')\n",
    "            _copy_merge(self.mount_dir, self.work_root)\n",
    "            _maybe_unpack_dir_mode_zips(self.work_root)\n",
    "            return\n",
    "\n",
    "        print(f'Downloading checkpoints from Kaggle API: {self.dataset_id}')\n",
    "        _ensure_kaggle_cli()\n",
    "        env = _kaggle_env(require=checkpoint_required)\n",
    "\n",
    "        if not env.get('KAGGLE_USERNAME') or not env.get('KAGGLE_KEY'):\n",
    "            msg = (\n",
    "                'Kaggle API auth missing. The `kaggle` CLI requires `KAGGLE_USERNAME` + `KAGGLE_KEY` '\n",
    "                '(set them as env vars or Kaggle/Colab secrets). Either set them, or attach the '\n",
    "                'checkpoint dataset as a Notebook Input so `STORE.pull()` can use the mounted copy.'\n",
    "            )\n",
    "            if checkpoint_required:\n",
    "                raise RuntimeError(msg)\n",
    "            print('WARNING: ' + msg)\n",
    "            return\n",
    "\n",
    "        tmp = self.work_root / '_tmp_kaggle_download'\n",
    "        if tmp.exists():\n",
    "            shutil.rmtree(tmp)\n",
    "        tmp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Debug (no secrets): confirm auth is present before calling kaggle.\n",
    "        print('Kaggle auth present:', bool(env.get('KAGGLE_USERNAME')) and bool(env.get('KAGGLE_KEY')))\n",
    "        print('Kaggle username length:', len(env.get('KAGGLE_USERNAME', '')))\n",
    "        print('Kaggle key length:', len(env.get('KAGGLE_KEY', '')))\n",
    "\n",
    "        # Colab can SIGKILL big unzip steps (returncode -9). Avoid `--unzip` and unzip ourselves.\n",
    "        pull_files_raw = (os.environ.get('CAFA_CHECKPOINT_PULL_FILES') or '').strip()\n",
    "        pull_files = [f.strip() for f in pull_files_raw.split(',') if f.strip()] if pull_files_raw else []\n",
    "\n",
    "        def _download_via_kaggle_api(files: list[str] | None) -> tuple[int, str, str]:\n",
    "            # Use the Kaggle Python API in Colab (more stable than spawning the CLI for large downloads).\n",
    "            try:\n",
    "                from kaggle.api.kaggle_api_extended import KaggleApi  # type: ignore\n",
    "            except Exception as e:\n",
    "                return 1, '', f'Failed to import KaggleApi: {type(e).__name__}: {e}'\n",
    "\n",
    "            try:\n",
    "                api = KaggleApi()\n",
    "                api.authenticate()\n",
    "            except Exception as e:\n",
    "                return 1, '', f'KaggleApi.authenticate() failed: {type(e).__name__}: {e}'\n",
    "\n",
    "            def _list_dataset_files() -> list[str]:\n",
    "                try:\n",
    "                    lf = api.dataset_list_files(self.dataset_id)\n",
    "                    out = []\n",
    "                    for f in getattr(lf, 'files', []) or []:\n",
    "                        name = getattr(f, 'name', None)\n",
    "                        if name:\n",
    "                            out.append(str(name))\n",
    "                    return sorted(out)\n",
    "                except Exception as e:\n",
    "                    print('WARNING: failed to list dataset files via KaggleApi:', type(e).__name__, str(e)[:200])\n",
    "                    return []\n",
    "\n",
    "            available = _list_dataset_files()\n",
    "            if available:\n",
    "                print('Checkpoint dataset files (first 60):')\n",
    "                for n in available[:60]:\n",
    "                    print(' -', n)\n",
    "                if len(available) > 60:\n",
    "                    print(f' - ... ({len(available)-60} more)')\n",
    "            else:\n",
    "                print('WARNING: could not list files for dataset via KaggleApi (might be private/not visible).')\n",
    "\n",
    "            # Strategy: Force FULL DATASET DOWNLOAD (skipping individual files)\n",
    "            print('Forcing FULL DATASET DOWNLOAD (skipping individual files)...')\n",
    "\n",
    "            # Track what we need to extract\n",
    "            needed_prefixes = set()\n",
    "            if not files:\n",
    "                needed_prefixes.add('') # Root\n",
    "            else:\n",
    "                for f in files:\n",
    "                    if f.endswith('.zip'):\n",
    "                        needed_prefixes.add(f[:-4] + '/') # parsed.zip -> parsed/\n",
    "                    else:\n",
    "                        needed_prefixes.add(f) # literal file\n",
    "\n",
    "            try:\n",
    "                # Download full zip\n",
    "                api.dataset_download_files(self.dataset_id, path=str(tmp), force=True, quiet=False, unzip=False)\n",
    "                \n",
    "                # Find the zip\n",
    "                zips = list(tmp.glob('*.zip'))\n",
    "                if not zips:\n",
    "                    return 1, '', 'Full download finished but no .zip file found.'\n",
    "                \n",
    "                main_zip = zips[0]\n",
    "                print(f'Extracting relevant files from {main_zip.name}...')\n",
    "                \n",
    "                with zipfile.ZipFile(main_zip, 'r') as zf:\n",
    "                    all_names = zf.namelist()\n",
    "                    to_extract = []\n",
    "                    for n in all_names:\n",
    "                        if '' in needed_prefixes:\n",
    "                            to_extract.append(n)\n",
    "                            continue\n",
    "                        \n",
    "                        for p in needed_prefixes:\n",
    "                            if n.startswith(p) or n in {'manifest.json', 'README.md'}:\n",
    "                                to_extract.append(n)\n",
    "                                break\n",
    "                    \n",
    "                    print(f'Extracting {len(to_extract)} files...')\n",
    "                    zf.extractall(tmp, members=to_extract)\n",
    "                \n",
    "                main_zip.unlink()\n",
    "                return 0, 'kaggle_api: ok (full download)', ''\n",
    "\n",
    "            except Exception as e_full:\n",
    "                return 1, '', f'Full download failed: {type(e_full).__name__}: {e_full}'\n",
    "\n",
    "        def _extract_outer_zips(download_dir: Path) -> None:\n",
    "            zips = sorted(download_dir.glob('*.zip'))\n",
    "            for z in zips:\n",
    "                try:\n",
    "                    with zipfile.ZipFile(z, 'r') as zf:\n",
    "                        zf.extractall(download_dir)\n",
    "                    z.unlink()\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f'Failed to unzip downloaded archive: {z}: {type(e).__name__}: {e}')\n",
    "\n",
    "        if IS_COLAB:\n",
    "            # In Colab, prefer KaggleApi (avoids subprocess SIGKILLs more often than the CLI).\n",
    "            if pull_files:\n",
    "                print('Checkpoint pull (file mode via KaggleApi):', ', '.join(pull_files))\n",
    "            else:\n",
    "                print('Checkpoint pull (full via KaggleApi).')\n",
    "            rc, out_s, err_s = _download_via_kaggle_api(pull_files if pull_files else None)\n",
    "            class _P:  # lightweight subprocess-like container\n",
    "                def __init__(self, returncode, stdout, stderr):\n",
    "                    self.returncode = returncode\n",
    "                    self.stdout = stdout\n",
    "                    self.stderr = stderr\n",
    "            p = _P(rc, out_s, err_s)\n",
    "            if p.returncode == 0:\n",
    "                _extract_outer_zips(tmp)\n",
    "        else:\n",
    "            # Local/Kaggle: keep using the CLI.\n",
    "            if pull_files:\n",
    "                print('Checkpoint pull (file mode):', ', '.join(pull_files))\n",
    "                for fname in pull_files:\n",
    "                    p = subprocess.run(\n",
    "                        ['kaggle', 'datasets', 'download', '-d', self.dataset_id, '-f', fname, '-p', str(tmp)],\n",
    "                        text=True,\n",
    "                        capture_output=True,\n",
    "                        env=env,\n",
    "                    )\n",
    "                    if p.returncode != 0:\n",
    "                        break\n",
    "                    _extract_outer_zips(tmp)\n",
    "            else:\n",
    "                p = subprocess.run(\n",
    "                    ['kaggle', 'datasets', 'download', '-d', self.dataset_id, '-p', str(tmp)],\n",
    "                    text=True,\n",
    "                    capture_output=True,\n",
    "                    env=env,\n",
    "                )\n",
    "                if p.returncode == 0:\n",
    "                    _extract_outer_zips(tmp)\n",
    "\n",
    "        if p.returncode != 0:\n",
    "            print('Kaggle return code:', p.returncode)\n",
    "            print(p.stdout)\n",
    "            print(p.stderr)\n",
    "\n",
    "            stderr_raw = p.stderr or ''\n",
    "            stdout_raw = p.stdout or ''\n",
    "            err = (stderr_raw + '\\n' + stdout_raw).strip()\n",
    "            if (p.returncode in {-9, 137}) and (not err):\n",
    "                err = (\n",
    "                    f'kaggle process was killed (returncode={p.returncode}). This is usually OOM or a large unzip. '\n",
    "                    'or set CAFA_CHECKPOINT_REQUIRED=0 to continue without checkpoints.'\n",
    "                )\n",
    "\n",
    "            # If kaggle returned non-zero but we captured nothing, rerun without capture_output so logs appear.\n",
    "            if not err:\n",
    "                print('Kaggle CLI returned non-zero but no output was captured; rerunning without capture_output for logs...')\n",
    "                try:\n",
    "                    subprocess.run(\n",
    "                        ['kaggle', 'datasets', 'download', '-d', self.dataset_id, '-p', str(tmp)],\n",
    "                        text=True,\n",
    "                        env=env,\n",
    "                        check=False,\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print('Rerun failed:', repr(e))\n",
    "                err = '<no kaggle cli output captured; see the rerun logs above>'\n",
    "\n",
    "            err_excerpt = err[:2000] + ('\\n...<truncated>' if len(err) > 2000 else '')\n",
    "\n",
    "            if '403' in err or 'Forbidden' in err:\n",
    "                msg = (\n",
    "                    'Checkpoint download was forbidden (HTTP 403). This almost always means the dataset is private '\n",
    "                    'or not accessible from the current Kaggle account/runtime. Fix options:\\n'\n",
    "                    '  1) Attach the checkpoint dataset as a Kaggle notebook Input (fastest; no API call).\\n'\n",
    "                    '  2) Make the checkpoint dataset public (or share it with the account running the notebook).\\n'\n",
    "                    '  3) Ensure Secrets `KAGGLE_USERNAME`/`KAGGLE_KEY` belong to a user with access.'\n",
    "                )\n",
    "                if checkpoint_required:\n",
    "                    raise RuntimeError(msg)\n",
    "                print('WARNING: ' + msg)\n",
    "                shutil.rmtree(tmp)\n",
    "                return\n",
    "\n",
    "            if '404' in err or 'Not Found' in err:\n",
    "                msg = (\n",
    "                    'Checkpoint dataset not found / not visible (HTTP 404). This can mean the dataset ID is wrong '\n",
    "                    'or the dataset is private and not accessible to the current account. '\n",
    "                    f'Dataset: {self.dataset_id}'\n",
    "                )\n",
    "                if checkpoint_required:\n",
    "                    raise RuntimeError(msg + '\\n\\nKaggle CLI output:\\n' + err_excerpt)\n",
    "                print('WARNING: ' + msg)\n",
    "                shutil.rmtree(tmp)\n",
    "                return\n",
    "\n",
    "            msg = 'Failed to download checkpoints from Kaggle. Check auth/network.'\n",
    "            if checkpoint_required:\n",
    "                raise RuntimeError(msg + '\\n\\nKaggle CLI output:\\n' + err_excerpt)\n",
    "            print('WARNING: ' + msg)\n",
    "            shutil.rmtree(tmp)\n",
    "            return\n",
    "\n",
    "        _copy_merge(tmp, self.work_root)\n",
    "        _maybe_unpack_dir_mode_zips(self.work_root)\n",
    "        shutil.rmtree(tmp)\n",
    "\n",
    "    def push(self, stage: str, required_paths: list[Path], note: str = '') -> None:\n",
    "        if not self.push_enabled:\n",
    "            print('Checkpoint push disabled (CAFA_CHECKPOINT_PUSH=0).')\n",
    "            return\n",
    "\n",
    "        if not self.dataset_id:\n",
    "            raise ValueError('Missing CAFA_CHECKPOINT_DATASET_ID=<user>/<slug>; cannot checkpoint.')\n",
    "\n",
    "        missing = [p for p in required_paths if not Path(p).exists()]\n",
    "        if missing:\n",
    "            raise FileNotFoundError(\n",
    "                'Cannot checkpoint; missing required artefacts:\\n' + '\\n'.join([f' - {m}' for m in missing])\n",
    "            )\n",
    "\n",
    "        force_push = os.environ.get('CAFA_CHECKPOINT_FORCE_PUSH', '0').strip() == '1'\n",
    "        if not force_push:\n",
    "            m = _load_manifest()\n",
    "            existing = (m.get('stages', {}) or {}).get(stage) if isinstance(m, dict) else None\n",
    "            if isinstance(existing, dict):\n",
    "                prev_files = existing.get('files', [])\n",
    "                if isinstance(prev_files, list):\n",
    "                    prev_sig = sorted(\n",
    "                        [{'path': f.get('path'), 'bytes': f.get('bytes')} for f in prev_files if isinstance(f, dict)],\n",
    "                        key=lambda x: str(x.get('path')),\n",
    "                    )\n",
    "                    cur_sig = _stage_files_signature(required_paths)\n",
    "                    if prev_sig == cur_sig:\n",
    "                        print(\n",
    "                            f'Checkpoint stage {stage} unchanged; skipping publish '\n",
    "                            '(set CAFA_CHECKPOINT_FORCE_PUSH=1 to force).'\n",
    "                        )\n",
    "                        return\n",
    "\n",
    "        _update_manifest(stage, required_paths, note=note)\n",
    "\n",
    "        # Publish WORK_ROOT directly (must not contain caches).\n",
    "        (self.work_root / 'dataset-metadata.json').write_text(\n",
    "            json.dumps({'title': self.dataset_title, 'id': self.dataset_id, 'licenses': [{'name': 'CC0-1.0'}]}, indent=2),\n",
    "            encoding='utf-8',\n",
    "        )\n",
    "        (self.work_root / 'README.md').write_text(\n",
    "            f'# {self.dataset_title}\\n\\nAuto-published checkpoint dataset for CAFA6.\\n\\nLatest stage: {stage}\\n',\n",
    "            encoding='utf-8',\n",
    "        )\n",
    "\n",
    "        _ensure_kaggle_cli()\n",
    "        env = _kaggle_env(require=True)\n",
    "        msg = f'{stage}: {note}'.strip() if note else stage\n",
    "\n",
    "        # IMPORTANT: Kaggle CLI skips directories unless --dir-mode is set.\n",
    "        p = subprocess.run(\n",
    "            ['kaggle', 'datasets', 'version', '-p', str(self.work_root), '--dir-mode', 'zip', '-m', msg],\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            env=env,\n",
    "        )\n",
    "\n",
    "        if p.returncode != 0:\n",
    "            # If dataset does not exist yet, create it.\n",
    "            p2 = subprocess.run(\n",
    "                ['kaggle', 'datasets', 'create', '-p', str(self.work_root), '--dir-mode', 'zip'],\n",
    "                text=True,\n",
    "                capture_output=True,\n",
    "                env=env,\n",
    "            )\n",
    "            if p2.returncode != 0:\n",
    "                print(p.stdout)\n",
    "                print(p.stderr)\n",
    "                print(p2.stdout)\n",
    "                print(p2.stderr)\n",
    "                raise RuntimeError('Kaggle dataset publish failed. See logs above.')\n",
    "            print(p2.stdout)\n",
    "            print(p2.stderr)\n",
    "        else:\n",
    "            print(p.stdout)\n",
    "            print(p.stderr)\n",
    "            print('Published new checkpoint dataset version:', self.dataset_id)\n",
    "\n",
    "\n",
    "STORE = KaggleCheckpointStore(\n",
    "    work_root=WORK_ROOT,\n",
    "    dataset_id=CHECKPOINT_DATASET_ID,\n",
    "    dataset_title=CHECKPOINT_DATASET_TITLE,\n",
    "    pull_enabled=CHECKPOINT_PULL,\n",
    "    push_enabled=CHECKPOINT_PUSH,\n",
    "    input_root=INPUT_ROOT,\n",
    "    is_kaggle=IS_KAGGLE,\n",
    ")\n",
    "\n",
    "# Pull once at startup (fresh runtimes resume here)\n",
    "STORE.pull()\n",
    "\n",
    "\n",
    "# Post-pull diagnostics: make it obvious whether artefacts are present (and whether dir-mode zips were unpacked).\n",
    "\n",
    "def _p(path: Path) -> str:\n",
    "    return str(path)\n",
    "\n",
    "\n",
    "def _exists_bytes(path: Path) -> str:\n",
    "    if not path.exists():\n",
    "        return 'MISSING'\n",
    "    try:\n",
    "        return f'OK ({path.stat().st_size / (1024**2):.1f} MB)'\n",
    "    except Exception:\n",
    "        return 'OK'\n",
    "\n",
    "\n",
    "print('Checkpoint status (after pull):')\n",
    "print('  WORK_ROOT:', _p(WORK_ROOT))\n",
    "print('  parsed/:', _exists_bytes(WORK_ROOT / 'parsed'))\n",
    "print('  external/:', _exists_bytes(WORK_ROOT / 'external'))\n",
    "print('  features/:', _exists_bytes(WORK_ROOT / 'features'))\n",
    "print('  parsed.zip:', _exists_bytes(WORK_ROOT / 'parsed.zip'))\n",
    "print('  external.zip:', _exists_bytes(WORK_ROOT / 'external.zip'))\n",
    "print('  features.zip:', _exists_bytes(WORK_ROOT / 'features.zip'))\n",
    "print('  external/entryid_text.tsv:', _exists_bytes(WORK_ROOT / 'external' / 'entryid_text.tsv'))\n",
    "print('  parsed/train_seq.feather:', _exists_bytes(WORK_ROOT / 'parsed' / 'train_seq.feather'))\n",
    "\n",
    "\n",
    "def stage_present(required_paths: list[Path]) -> bool:\n",
    "    return all(Path(p).exists() for p in required_paths)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Initial Diagnostics (Sequence Lengths)\n",
    "# ------------------------------------------\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "\n",
    "def read_fasta_lengths(path: Path, max_records=20000):\n",
    "    lengths = []\n",
    "    current = 0\n",
    "    n = 0\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if n > 0:\n",
    "                    lengths.append(current)\n",
    "                n += 1\n",
    "                current = 0\n",
    "                if max_records and n > max_records:\n",
    "                    break\n",
    "            else:\n",
    "                current += len(line)\n",
    "        if n > 0:\n",
    "            lengths.append(current)\n",
    "    return np.array(lengths)\n",
    "\n",
    "\n",
    "train_lens = read_fasta_lengths(PATH_TRAIN_FASTA)\n",
    "test_lens = read_fasta_lengths(PATH_TEST_FASTA) if PATH_TEST_FASTA.exists() else np.array([])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "# Use fixed bins (0 to 3000) to ignore outliers (like Titin) and ensure alignment\n",
    "bins = np.linspace(0, 3000, 60)\n",
    "\n",
    "plt.hist(train_lens, bins=bins, alpha=0.5, label='Train', density=True, color='tab:blue')\n",
    "if len(test_lens) > 0:\n",
    "    plt.hist(test_lens, bins=bins, alpha=0.5, label='Test', density=True, color='tab:orange')\n",
    "\n",
    "plt.title('Sequence Length Distribution (0-3000aa, Normalized)')\n",
    "plt.xlabel('Length (amino acids)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "print(f'Train sequences: {len(train_lens)}')\n",
    "print(f'Test sequences : {len(test_lens)}')\n",
    "if len(train_lens) > 0:\n",
    "    print(f'Train max len  : {train_lens.max()}')\n",
    "if len(test_lens) > 0:\n",
    "    print(f'Test max len   : {test_lens.max()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b013607",
   "metadata": {
    "id": "3b013607",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 04 - Solution: 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "# ------------------------------------------\n",
    "# B. Parse OBO & Terms (needed in-memory downstream)\n",
    "# ------------------------------------------\n",
    "\n",
    "def parse_obo(path: Path):\n",
    "    parents = {}\n",
    "    namespaces = {}\n",
    "    cur_id, cur_ns = None, None\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '[Term]':\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "                cur_id, cur_ns = None, None\n",
    "            elif line.startswith('id: GO:'):\n",
    "                cur_id = line.split('id: ', 1)[1]\n",
    "            elif line.startswith('namespace:'):\n",
    "                cur_ns = line.split('namespace: ', 1)[1]\n",
    "            elif line.startswith('is_a:') and cur_id:\n",
    "                parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                parents.setdefault(cur_id, set()).add(parent)\n",
    "        if cur_id and cur_ns:\n",
    "            namespaces[cur_id] = cur_ns\n",
    "    return parents, namespaces\n",
    "print(\"Parsing OBO...\")\n",
    "go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "print(f\"GO Graph: {len(go_parents)} nodes with parents, {len(go_namespaces)} terms with namespace.\")\n",
    "# ------------------------------------------\n",
    "# Milestone checkpoint: stage_01_parsed\n",
    "# ------------------------------------------\n",
    "parsed_dir = WORK_ROOT / 'parsed'\n",
    "parsed_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_train_seq = parsed_dir / 'train_seq.feather'\n",
    "out_test_seq = parsed_dir / 'test_seq.feather'\n",
    "out_train_terms = parsed_dir / 'train_terms.parquet'\n",
    "out_term_counts = parsed_dir / 'term_counts.parquet'\n",
    "out_term_priors = parsed_dir / 'term_priors.parquet'\n",
    "out_train_taxa = parsed_dir / 'train_taxa.feather'\n",
    "out_test_taxa = parsed_dir / 'test_taxa.feather'\n",
    "expected = [out_train_seq, out_train_terms, out_term_counts, out_term_priors, out_train_taxa]\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    expected += [out_test_seq, out_test_taxa]\n",
    "missing = [p for p in expected if not p.exists()]\n",
    "if not missing:\n",
    "    print(\"Parsed artefacts already exist; skipping Phase 1 writes.\")\n",
    "else:\n",
    "    # ------------------------------------------\n",
    "    # A. Parse FASTA to Feather\n",
    "    # ------------------------------------------\n",
    "    def parse_fasta(path: Path) -> pd.DataFrame:\n",
    "        ids, seqs = [], []\n",
    "        cur_id, cur_seq = None, []\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    if cur_id:\n",
    "                        ids.append(cur_id)\n",
    "                        seqs.append(''.join(cur_seq))\n",
    "                    cur_id = line[1:].split()[0]\n",
    "                    cur_seq = []\n",
    "                else:\n",
    "                    cur_seq.append(line)\n",
    "            if cur_id:\n",
    "                ids.append(cur_id)\n",
    "                seqs.append(''.join(cur_seq))\n",
    "        return pd.DataFrame({'id': ids, 'sequence': seqs})\n",
    "    print(\"Parsing FASTA...\")\n",
    "    parse_fasta(PATH_TRAIN_FASTA).to_feather(out_train_seq)\n",
    "    if PATH_TEST_FASTA.exists():\n",
    "        parse_fasta(PATH_TEST_FASTA).to_feather(out_test_seq)\n",
    "    print(\"FASTA parsed and saved to artefacts.\")\n",
    "    # ------------------------------------------\n",
    "    # C. Process Terms & Priors\n",
    "    # ------------------------------------------\n",
    "    terms = pd.read_csv(PATH_TRAIN_TERMS, sep='\\t')\n",
    "    col_term = terms.columns[1]\n",
    "    terms['aspect'] = terms[col_term].map(lambda x: go_namespaces.get(x, 'UNK'))\n",
    "    # Plot Aspects\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    terms['aspect'].value_counts().plot(kind='bar', title='Annotations by Namespace')\n",
    "    plt.show()\n",
    "    # Save Priors\n",
    "    priors = (terms[col_term].value_counts() / terms.iloc[:, 0].nunique()).reset_index()\n",
    "    priors.columns = ['term', 'prior']\n",
    "    if PATH_IA.exists():\n",
    "        ia = pd.read_csv(PATH_IA, sep='\\t', names=['term', 'ia'])\n",
    "        priors = priors.merge(ia, on='term', how='left').fillna(0)\n",
    "    priors.to_parquet(out_term_priors)\n",
    "    print(\"Terms processed and priors saved.\")\n",
    "    # ------------------------------------------\n",
    "    # D. Process Taxonomy\n",
    "    # ------------------------------------------\n",
    "    print(\"Processing Taxonomy...\")\n",
    "    # Train Taxonomy\n",
    "    tax_train = pd.read_csv(PATH_TRAIN_TAXON, sep='\\t', header=None, names=['id', 'taxon_id'])\n",
    "    tax_train['taxon_id'] = tax_train['taxon_id'].astype(int)\n",
    "    tax_train.to_feather(out_train_taxa)\n",
    "    # Test Taxonomy (Extract from FASTA headers)\n",
    "    if PATH_TEST_FASTA.exists():\n",
    "        ids, taxons = [], []\n",
    "        with PATH_TEST_FASTA.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    parts = line[1:].split()\n",
    "                    ids.append(parts[0])\n",
    "                    # Assume second part is taxon if present\n",
    "                    if len(parts) > 1:\n",
    "                        try:\n",
    "                            taxons.append(int(parts[1]))\n",
    "                        except ValueError:\n",
    "                            taxons.append(0)\n",
    "                    else:\n",
    "                        taxons.append(0)\n",
    "        tax_test = pd.DataFrame({'id': ids, 'taxon_id': taxons})\n",
    "        tax_test.to_feather(out_test_taxa)\n",
    "        print(f\"Taxonomy processed. Train: {len(tax_train)}, Test: {len(tax_test)}\")\n",
    "    else:\n",
    "        print(f\"Taxonomy processed. Train: {len(tax_train)}\")\n",
    "    # ------------------------------------------\n",
    "    # E. Save Targets & Term List\n",
    "    # ------------------------------------------\n",
    "    print(\"Saving Targets & Term List...\")\n",
    "    # Save full terms list (long format)\n",
    "    terms.to_parquet(out_train_terms)\n",
    "    # Save unique term list with counts\n",
    "    term_counts = terms['term'].value_counts().reset_index()\n",
    "    term_counts.columns = ['term', 'count']\n",
    "    term_counts.to_parquet(out_term_counts)\n",
    "    print(\"Targets saved.\")\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.push('stage_01_parsed', [p for p in expected if p.exists()], note='parsed FASTA/taxa/terms/priors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea612e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10b - Diagnostics: artefact manifest (existence + sizes)\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "def _mb(p: Path) -> float:\n",
    "    return p.stat().st_size / (1024**2)\n",
    "\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "\n",
    "# Minimal contract for *this* notebook (first submission, no Ankh):\n",
    "# - parsed/*\n",
    "# - core embeddings (t5, esm2, esm2_3b, text)\n",
    "# - taxonomy\n",
    "# - external priors (if you want them in the stacker)\n",
    "paths = {\n",
    "    # Phase 1 parsed\n",
    "    'parsed/train_seq.feather': WORK_ROOT / 'parsed' / 'train_seq.feather',\n",
    "    'parsed/test_seq.feather': WORK_ROOT / 'parsed' / 'test_seq.feather',\n",
    "    'parsed/train_terms.parquet': WORK_ROOT / 'parsed' / 'train_terms.parquet',\n",
    "    'parsed/term_priors.parquet': WORK_ROOT / 'parsed' / 'term_priors.parquet',\n",
    "    'parsed/train_taxa.feather': WORK_ROOT / 'parsed' / 'train_taxa.feather',\n",
    "    'parsed/test_taxa.feather': WORK_ROOT / 'parsed' / 'test_taxa.feather',\n",
    "    # Text pipeline\n",
    "    'external/entryid_text.tsv': WORK_ROOT / 'external' / 'entryid_text.tsv',\n",
    "    'features/text_vectorizer.joblib': WORK_ROOT / 'features' / 'text_vectorizer.joblib',\n",
    "    'features/train_embeds_text.npy': WORK_ROOT / 'features' / 'train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy': WORK_ROOT / 'features' / 'test_embeds_text.npy',\n",
    "    # Sequence embeddings (core)\n",
    "    'features/train_embeds_t5.npy': WORK_ROOT / 'features' / 'train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy': WORK_ROOT / 'features' / 'test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy': WORK_ROOT / 'features' / 'train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy': WORK_ROOT / 'features' / 'test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy': WORK_ROOT / 'features' / 'train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy': WORK_ROOT / 'features' / 'test_embeds_esm2_3b.npy',\n",
    "    # External priors (optional but used if present)\n",
    "    'external/prop_train_no_kaggle.tsv.gz': WORK_ROOT / 'external' / 'prop_train_no_kaggle.tsv.gz',\n",
    "    'external/prop_test_no_kaggle.tsv.gz': WORK_ROOT / 'external' / 'prop_test_no_kaggle.tsv.gz',\n",
    "    # Downstream expectations\n",
    "    'features/top_terms_1500.json': WORK_ROOT / 'features' / 'top_terms_1500.json',\n",
    "    'features/oof_pred_logreg.npy': WORK_ROOT / 'features' / 'oof_pred_logreg.npy',\n",
    "    'features/oof_pred_gbdt.npy': WORK_ROOT / 'features' / 'oof_pred_gbdt.npy',\n",
    "    'features/oof_pred_dnn.npy': WORK_ROOT / 'features' / 'oof_pred_dnn.npy',\n",
    "    'features/oof_pred_knn.npy': WORK_ROOT / 'features' / 'oof_pred_knn.npy',\n",
    "    'features/test_pred_logreg.npy': WORK_ROOT / 'features' / 'test_pred_logreg.npy',\n",
    "    'features/test_pred_gbdt.npy': WORK_ROOT / 'features' / 'test_pred_gbdt.npy',\n",
    "    'features/test_pred_dnn.npy': WORK_ROOT / 'features' / 'test_pred_dnn.npy',\n",
    "    'features/test_pred_knn.npy': WORK_ROOT / 'features' / 'test_pred_knn.npy',\n",
    "    'features/test_pred_gcn.npy': WORK_ROOT / 'features' / 'test_pred_gcn.npy',\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, p in paths.items():\n",
    "    rows.append({'artefact': name, 'exists': p.exists(), 'mb': _mb(p) if p.exists() else 0.0, 'path': str(p)})\n",
    "df = pd.DataFrame(rows).sort_values(['exists', 'mb'], ascending=[True, False])\n",
    "print('WORK_ROOT:', WORK_ROOT)\n",
    "display(df)\n",
    "\n",
    "# Visual: top 25 largest artefacts\n",
    "df2 = df[df['exists']].sort_values('mb', ascending=False).head(25)\n",
    "if len(df2) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df2, y='artefact', x='mb')\n",
    "    plt.title('Largest artefacts (MB)')\n",
    "    plt.xlabel('MB')\n",
    "    plt.ylabel('artefact')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Strict check (what must exist before Phase 2)\n",
    "required_for_phase2 = [\n",
    "    'features/train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy',\n",
    "    'features/train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy',\n",
    "    'parsed/train_taxa.feather',\n",
    "    'parsed/test_taxa.feather',\n",
    "    'parsed/train_terms.parquet',\n",
    "    'parsed/train_seq.feather',\n",
    "    'parsed/test_seq.feather',\n",
    "    # If you intend to use external priors in the GCN cell, these must exist too:\n",
    "    # 'external/prop_train_no_kaggle.tsv.gz',\n",
    "    # 'external/prop_test_no_kaggle.tsv.gz',\n",
    "]\n",
    "missing = [a for a in required_for_phase2 if not paths[a].exists()]\n",
    "if missing:\n",
    "    print('\\nMissing artefacts for Phase 2 (first submission, no Ankh):')\n",
    "    for m in missing:\n",
    "        print(' -', m)\n",
    "else:\n",
    "    print('\\nPhase 2 artefacts OK: embeddings + taxonomy + parsed targets present.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8a - Setup & Data Loading\n",
    "# =============================================\n",
    "# 4. PHASE 2: LEVEL-1 MODELS (DIVERSE ENSEMBLE)\n",
    "# =============================================\n",
    "TRAIN_LEVEL1 = True\n",
    "if TRAIN_LEVEL1:\n",
    "    import joblib\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import gc\n",
    "    from pathlib import Path\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import f1_score\n",
    "    import psutil\n",
    "\n",
    "    # AUDITOR: Hardware Check\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"[AUDITOR] GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"[AUDITOR] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"[AUDITOR] WARNING: No GPU detected. RAPIDS will fail.\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    def log_mem(tag=\"\"):\n",
    "        try:\n",
    "            mem = psutil.virtual_memory()\n",
    "            print(f\"[MEM] {tag:<30} | Used: {mem.used/1e9:.2f}GB / {mem.total/1e9:.2f}GB ({mem.percent}%)\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if 'WORK_ROOT' not in locals() and 'WORK_ROOT' not in globals():\n",
    "        if os.path.exists('/content/work'):\n",
    "            WORK_ROOT = Path('/content/work')\n",
    "        elif os.path.exists('/kaggle/working/work'):\n",
    "            WORK_ROOT = Path('/kaggle/working/work')\n",
    "        else:\n",
    "            WORK_ROOT = Path.cwd() / 'artefacts_local' / 'work'\n",
    "        print(f\"WORK_ROOT recovered: {WORK_ROOT}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load targets + ids\n",
    "    # -----------------------------\n",
    "    print(\"Loading targets...\")\n",
    "    train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "    train_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id'].astype(str)\n",
    "    \n",
    "    # FIX: Clean IDs in train_ids to match EntryID format\n",
    "    print(\"Applying ID cleaning fix...\")\n",
    "    train_ids_clean = train_ids.str.extract(r'\\|(.*?)\\|')[0]\n",
    "    train_ids_clean = train_ids_clean.fillna(train_ids)\n",
    "    \n",
    "    # Target Matrix Construction (Champion Strategy: 13,500 Terms)\n",
    "    print(\"Selecting Top-K terms per aspect (Champion Strategy)...\")\n",
    "    \n",
    "    try:\n",
    "        import obonet\n",
    "        obo_path = WORK_ROOT.parent / 'go-basic.obo'\n",
    "        if not obo_path.exists(): obo_path = Path('go-basic.obo')\n",
    "        if not obo_path.exists(): obo_path = Path('/content/cafa-6-protein-function-prediction/Train/go-basic.obo')\n",
    "        \n",
    "        print(f\"Loading OBO from {obo_path}...\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        term_to_ns = {node: data.get('namespace', 'unknown') for node, data in graph.nodes(data=True)}\n",
    "        ns_map = {'biological_process': 'BP', 'molecular_function': 'MF', 'cellular_component': 'CC'}\n",
    "        if 'aspect' not in train_terms.columns:\n",
    "            train_terms['aspect'] = train_terms['term'].map(lambda t: ns_map.get(term_to_ns.get(t), 'UNK'))\n",
    "    except ImportError:\n",
    "        print(\"[WARNING] obonet not found. Falling back to global Top-K (13,500).\")\n",
    "        train_terms['aspect'] = 'UNK'\n",
    "\n",
    "    term_counts = train_terms.groupby(['aspect', 'term']).size().reset_index(name='count')\n",
    "    targets_bp = term_counts[term_counts['aspect'] == 'BP'].nlargest(10000, 'count')['term'].tolist()\n",
    "    targets_mf = term_counts[term_counts['aspect'] == 'MF'].nlargest(2000, 'count')['term'].tolist()\n",
    "    targets_cc = term_counts[term_counts['aspect'] == 'CC'].nlargest(1500, 'count')['term'].tolist()\n",
    "    \n",
    "    if len(targets_bp) == 0 and len(targets_mf) == 0:\n",
    "        print(\"  Using global Top-13,500 (OBO fallback)\")\n",
    "        top_terms = train_terms['term'].value_counts().head(13500).index.tolist()\n",
    "    else:\n",
    "        top_terms = list(set(targets_bp + targets_mf + targets_cc))\n",
    "        print(f\"  Selected: {len(targets_bp)} BP + {len(targets_mf)} MF + {len(targets_cc)} CC\")\n",
    "\n",
    "    train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids_clean, fill_value=0)\n",
    "    Y = Y_df.values.astype(np.float32)\n",
    "    print(f\"Targets: Y={Y.shape}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Feature loading helper\n",
    "    # -----------------------------\n",
    "    FEAT_DIR = WORK_ROOT / 'features'\n",
    "    \n",
    "    def load_features_dict(split='both'):\n",
    "        log_mem(f\"Start load_features_dict({split})\")\n",
    "        print(f\"Loading multimodal features (mode={split})...\")\n",
    "        def _load_pair(stem):\n",
    "            tr = FEAT_DIR / f'train_embeds_{stem}.npy'\n",
    "            te = FEAT_DIR / f'test_embeds_{stem}.npy'\n",
    "            return tr, te\n",
    "        \n",
    "        ft_train = {}\n",
    "        ft_test = {}\n",
    "        for stem, key in [('t5', 't5'), ('esm2', 'esm2_650m'), ('esm2_3b', 'esm2_3b'), ('text', 'text')]:\n",
    "            tr_path, te_path = _load_pair(stem)\n",
    "            if split in ['both', 'train'] and tr_path.exists():\n",
    "                arr = np.load(tr_path).astype(np.float32)\n",
    "                ft_train[key] = arr\n",
    "            if split in ['both', 'test'] and te_path.exists():\n",
    "                arr = np.load(te_path).astype(np.float32)\n",
    "                ft_test[key] = arr\n",
    "        \n",
    "        taxa_train_path = WORK_ROOT / 'parsed' / 'train_taxa.feather'\n",
    "        taxa_test_path = WORK_ROOT / 'parsed' / 'test_taxa.feather'\n",
    "        if taxa_train_path.exists() and taxa_test_path.exists():\n",
    "            from sklearn.preprocessing import OneHotEncoder\n",
    "            tax_tr = pd.read_feather(taxa_train_path).astype({'id': str})\n",
    "            tax_te = pd.read_feather(taxa_test_path).astype({'id': str})\n",
    "            enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32)\n",
    "            enc.fit(pd.concat([tax_tr[['taxon_id']], tax_te[['taxon_id']]], axis=0))\n",
    "            \n",
    "            if split in ['both', 'train']:\n",
    "                tax_tr = tax_tr.set_index('id').reindex(train_ids, fill_value=0).reset_index()\n",
    "                ft_train['taxa'] = enc.transform(tax_tr[['taxon_id']]).astype(np.float32)\n",
    "            if split in ['both', 'test']:\n",
    "                tax_te = tax_te.set_index('id').reindex(test_ids, fill_value=0).reset_index()\n",
    "                ft_test['taxa'] = enc.transform(tax_te[['taxon_id']]).astype(np.float32)\n",
    "        \n",
    "        log_mem(f\"End load_features_dict({split})\")\n",
    "        if split == 'train': return ft_train\n",
    "        if split == 'test': return ft_test\n",
    "        return ft_train, ft_test\n",
    "\n",
    "    # -----------------------------\n",
    "    # IA-weighted F1 Helper\n",
    "    # -----------------------------\n",
    "    if 'ia' in locals(): ia_df = ia[['term', 'ia']].copy()\n",
    "    elif (WORK_ROOT.parent / 'IA.tsv').exists(): ia_df = pd.read_csv(WORK_ROOT.parent / 'IA.tsv', sep='\\t', names=['term', 'ia'])\n",
    "    elif (WORK_ROOT / 'IA.tsv').exists(): ia_df = pd.read_csv(WORK_ROOT / 'IA.tsv', sep='\\t', names=['term', 'ia'])\n",
    "    else: ia_df = pd.DataFrame({'term': [], 'ia': []})\n",
    "\n",
    "    ia_map = dict(zip(ia_df['term'], ia_df['ia']))\n",
    "    weights = np.array([ia_map.get(t, 0.0) for t in top_terms], dtype=np.float32)\n",
    "    \n",
    "    if 'go_namespaces' not in locals() and 'go_namespaces' not in globals():\n",
    "         term_aspects = np.array(['UNK'] * len(top_terms))\n",
    "    else:\n",
    "        ns_to_aspect = {'molecular_function': 'MF', 'biological_process': 'BP', 'cellular_component': 'CC'}\n",
    "        term_aspects = np.array([ns_to_aspect.get(go_namespaces.get(t, ''), 'UNK') for t in top_terms])\n",
    "\n",
    "    def ia_weighted_f1(y_true, y_score, thr=0.3):\n",
    "        y_true = (y_true > 0).astype(np.int8)\n",
    "        y_pred = (y_score >= thr).astype(np.int8)\n",
    "        tp = (y_pred & y_true).sum(axis=0).astype(np.float64)\n",
    "        pred = y_pred.sum(axis=0).astype(np.float64)\n",
    "        true = y_true.sum(axis=0).astype(np.float64)\n",
    "        def _score(mask=None):\n",
    "            w = weights if mask is None else (weights * mask)\n",
    "            w_tp = float((w * tp).sum())\n",
    "            w_pred = float((w * pred).sum())\n",
    "            w_true = float((w * true).sum())\n",
    "            p = (w_tp / w_pred) if w_pred > 0 else 0.0\n",
    "            r = (w_tp / w_true) if w_true > 0 else 0.0\n",
    "            return (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "        out = {'ALL': _score(None)}\n",
    "        for asp in ['MF', 'BP', 'CC']:\n",
    "            mask = (term_aspects == asp).astype(np.float32)\n",
    "            out[asp] = _score(mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ccd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8b - Phase 2a: Logistic Regression (Optimized)\n",
    "if TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 2a: Classical Models (LR/GBDT) ===\")\n",
    "    \n",
    "    # 1. Create Train X (Memory Optimized)\n",
    "    log_mem(\"Before loading Train\")\n",
    "    features_train = load_features_dict(split='train')\n",
    "    FLAT_KEYS = [k for k in ['t5', 'esm2_650m', 'esm2_3b', 'taxa', 'text'] if k in features_train]\n",
    "    print(f\"Creating Flat X from keys: {FLAT_KEYS}\")\n",
    "    \n",
    "    # OPTIMIZATION: Use Memory-Mapped File for X to avoid 56GB RAM crash\n",
    "    X_path = WORK_ROOT / 'features' / 'X_train_mmap.npy'\n",
    "    \n",
    "    # Calculate total shape\n",
    "    n_samples = features_train[FLAT_KEYS[0]].shape[0]\n",
    "    n_features = sum(features_train[k].shape[1] for k in FLAT_KEYS)\n",
    "    print(f\"Target X shape: ({n_samples}, {n_features})\")\n",
    "    \n",
    "    # Create mmap file\n",
    "    X_mmap = np.lib.format.open_memmap(X_path, mode='w+', dtype=np.float32, shape=(n_samples, n_features))\n",
    "    \n",
    "    # Fill mmap column by column (or block by block) to save RAM\n",
    "    current_col = 0\n",
    "    for k in FLAT_KEYS:\n",
    "        data = features_train[k]\n",
    "        dim = data.shape[1]\n",
    "        print(f\"  Writing {k} ({dim} cols) to mmap...\")\n",
    "        X_mmap[:, current_col:current_col+dim] = data\n",
    "        current_col += dim\n",
    "        # Free memory immediately\n",
    "        del features_train[k]\n",
    "        gc.collect()\n",
    "        \n",
    "    del features_train\n",
    "    X_mmap.flush()\n",
    "    del X_mmap\n",
    "    gc.collect()\n",
    "    log_mem(\"Created X mmap\")\n",
    "    \n",
    "    # Load X in read-only mmap mode\n",
    "    X = np.load(X_path, mmap_mode='r')\n",
    "    print(f\"Loaded X from mmap: {X.shape}\")\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # A. Logistic Regression (Baseline)\n",
    "    # ------------------------------------------\n",
    "    print(\"\\n--- Training Logistic Regression ---\")\n",
    "    from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "    from sklearn.multiclass import OneVsRestClassifier\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    try:\n",
    "        import cuml\n",
    "        from cuml.linear_model import LogisticRegression as cuLogReg\n",
    "        from cuml.multiclass import OneVsRestClassifier as cuOVR\n",
    "        import cupy as cp\n",
    "        HAS_RAPIDS = True\n",
    "        print(\"[AUDITOR] RAPIDS (cuml) detected.\")\n",
    "    except ImportError:\n",
    "        HAS_RAPIDS = False\n",
    "        print(\"[AUDITOR] RAPIDS NOT detected.\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds_logreg = np.zeros(Y.shape, dtype=np.float32)\n",
    "    \n",
    "    for fold, (idx_tr, idx_val) in enumerate(kf.split(X)):\n",
    "        print(f\"LogReg Fold {fold+1}/5\")\n",
    "        \n",
    "        # Load data slices (RAM usage increases here)\n",
    "        # X is mmap, so X[idx_tr] reads from disk into RAM.\n",
    "        # X_tr size ~ 25GB.\n",
    "        log_mem(\"Before loading X_tr\")\n",
    "        X_tr = X[idx_tr] # Copy to RAM\n",
    "        X_val = X[idx_val] # Copy to RAM\n",
    "        log_mem(\"Loaded X_tr/X_val\")\n",
    "        \n",
    "        Y_tr, Y_val = Y[idx_tr], Y[idx_val]\n",
    "        \n",
    "        # SCALING\n",
    "        scaler = StandardScaler()\n",
    "        X_tr = scaler.fit_transform(X_tr)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        \n",
    "        if HAS_RAPIDS:\n",
    "            # OPTIMIZATION: Enable RMM\n",
    "            try:\n",
    "                import rmm\n",
    "                rmm.reinitialize(managed_memory=True)\n",
    "            except: pass\n",
    "\n",
    "            # Move to GPU and delete CPU copy\n",
    "            X_tr_gpu = cp.asarray(X_tr)\n",
    "            X_val_gpu = cp.asarray(X_val)\n",
    "            del X_tr, X_val\n",
    "            gc.collect()\n",
    "            \n",
    "            # Batch targets\n",
    "            n_targets = Y.shape[1]\n",
    "            chunk_size = 2000\n",
    "            val_probs = np.zeros((Y_val.shape[0], n_targets), dtype=np.float32)\n",
    "            all_coefs = []\n",
    "            all_intercepts = []\n",
    "            \n",
    "            for start in range(0, n_targets, chunk_size):\n",
    "                end = min(start + chunk_size, n_targets)\n",
    "                print(f\"    Chunk {start}-{end}...\")\n",
    "                Y_tr_chunk = cp.asarray(Y_tr[:, start:end])\n",
    "                clf_chunk = cuOVR(cuLogReg(solver='qn', penalty='l2', C=1.0, max_iter=1000, tol=1e-3))\n",
    "                clf_chunk.fit(X_tr_gpu, Y_tr_chunk)\n",
    "                \n",
    "                p_chunk = clf_chunk.predict_proba(X_val_gpu)\n",
    "                if hasattr(p_chunk, 'get'): p_chunk = p_chunk.get()\n",
    "                elif hasattr(p_chunk, 'to_numpy'): p_chunk = p_chunk.to_numpy()\n",
    "                val_probs[:, start:end] = p_chunk\n",
    "                \n",
    "                for est in clf_chunk.estimators_:\n",
    "                    all_coefs.append(est.coef_.to_numpy() if hasattr(est.coef_, 'to_numpy') else est.coef_)\n",
    "                    all_intercepts.append(est.intercept_.to_numpy() if hasattr(est.intercept_, 'to_numpy') else est.intercept_)\n",
    "                \n",
    "                del Y_tr_chunk, clf_chunk, p_chunk\n",
    "                cp.get_default_memory_pool().free_all_blocks()\n",
    "            \n",
    "            del X_tr_gpu, X_val_gpu\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "            \n",
    "            # Save weights\n",
    "            model_data = {'coef': np.vstack(all_coefs), 'intercept': np.hstack(all_intercepts)}\n",
    "            joblib.dump(model_data, WORK_ROOT / 'features' / f'level1_logreg_weights_fold{fold}.pkl')\n",
    "            \n",
    "        else:\n",
    "            # CPU Fallback\n",
    "            clf_logreg = OneVsRestClassifier(SGDClassifier(loss='log_loss', penalty='l2', alpha=0.0001, max_iter=1000, tol=1e-3, n_jobs=4))\n",
    "            clf_logreg.fit(X_tr, Y_tr)\n",
    "            val_probs = clf_logreg.predict_proba(X_val)\n",
    "            joblib.dump(clf_logreg, WORK_ROOT / 'features' / f'level1_logreg_fold{fold}.pkl')\n",
    "            \n",
    "        oof_preds_logreg[idx_val] = val_probs\n",
    "        \n",
    "        # Diagnostics\n",
    "        best_f1 = 0.0\n",
    "        best_thr = 0.0\n",
    "        for thr in np.linspace(0.01, 0.20, 20):\n",
    "            vp = (val_probs > thr).astype(int)\n",
    "            score = f1_score(Y_val, vp, average='micro')\n",
    "            if score > best_f1: best_f1, best_thr = score, thr\n",
    "        \n",
    "        ia_f1 = ia_weighted_f1(Y_val, val_probs, thr=best_thr)\n",
    "        print(f\"  >> Fold {fold+1} Best F1: {best_f1:.4f} at Thr {best_thr:.2f}\")\n",
    "        print(f\"  >> Fold {fold+1} IA-F1: ALL={ia_f1['ALL']:.4f}\")\n",
    "        \n",
    "        joblib.dump(scaler, WORK_ROOT / 'features' / f'level1_logreg_scaler_fold{fold}.pkl')\n",
    "        del Y_tr, Y_val, scaler, val_probs\n",
    "        if 'X_tr' in locals(): del X_tr\n",
    "        if 'X_val' in locals(): del X_val\n",
    "        gc.collect()\n",
    "        \n",
    "    np.save(WORK_ROOT / 'features' / 'oof_pred_logreg.npy', oof_preds_logreg)\n",
    "    print(\"LogReg OOF saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef412cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8c - Phase 2a: Py-Boost (GBDT)\n",
    "if TRAIN_LEVEL1:\n",
    "    # Ensure X is available\n",
    "    if 'X' not in locals():\n",
    "        X_path = WORK_ROOT / 'features' / 'X_train_mmap.npy'\n",
    "        if X_path.exists():\n",
    "            X = np.load(X_path, mmap_mode='r')\n",
    "            print(f\"Reloaded X from mmap: {X.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(\"X mmap not found. Run Cell 8b first.\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # B. Py-Boost (GBDT)\n",
    "    # ------------------------------------------\n",
    "    try:\n",
    "        from py_boost import GradientBoosting\n",
    "        HAS_PYBOOST = True\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"CRITICAL: py_boost is missing. GBDT is a mandatory component. Install with 'pip install py-boost'.\") from e\n",
    "        \n",
    "    if HAS_PYBOOST:\n",
    "        print(\"\\n--- Training Py-Boost GBDT ---\")\n",
    "        oof_preds_gbdt = np.zeros(Y.shape, dtype=np.float32)\n",
    "        \n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf.split(X)):\n",
    "            print(f\"GBDT Fold {fold+1}/5\")\n",
    "            \n",
    "            # Load data slices\n",
    "            X_tr = X[idx_tr]\n",
    "            X_val = X[idx_val]\n",
    "            Y_tr, Y_val = Y[idx_tr], Y[idx_val]\n",
    "            \n",
    "            model = GradientBoosting(\n",
    "                loss='bce', ntrees=1000, lr=0.05, max_depth=6, \n",
    "                verbose=100, es=50, gpu_id=0\n",
    "            )\n",
    "            model.fit(X_tr, Y_tr, eval_sets=[{'X': X_val, 'y': Y_val}])\n",
    "            val_probs = model.predict(X_val)\n",
    "            oof_preds_gbdt[idx_val] = val_probs\n",
    "            \n",
    "            val_preds = (val_probs > 0.3).astype(int)\n",
    "            f1 = f1_score(Y_val, val_preds, average='micro')\n",
    "            ia_f1 = ia_weighted_f1(Y_val, val_probs, thr=0.3)\n",
    "            print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "            print(f\"  >> Fold {fold+1} IA-F1: ALL={ia_f1['ALL']:.4f}\")\n",
    "            \n",
    "            model.save(str(WORK_ROOT / 'features' / f'level1_gbdt_fold{fold}.json'))\n",
    "            del model, X_tr, X_val, Y_tr, Y_val\n",
    "            gc.collect()\n",
    "            \n",
    "        np.save(WORK_ROOT / 'features' / 'oof_pred_gbdt.npy', oof_preds_gbdt)\n",
    "        print(\"GBDT OOF saved.\")\n",
    "        \n",
    "    # Cleanup X mmap to free file handle\n",
    "    del X\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8d - Phase 2a Post-Processing: Generate Test Predictions\n",
    "if TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 2a Post-Processing: Generate Test Predictions ===\")\n",
    "    log_mem(\"Before loading Test Features\")\n",
    "    features_test = load_features_dict(split='test')\n",
    "    log_mem(\"Loaded Test Features\")\n",
    "    \n",
    "    # Helper for batched prediction with scaling\n",
    "    def predict_proba_batched_scaled(model, feat_dict, keys, scaler, batch_size=5000):\n",
    "        n_test = feat_dict[keys[0]].shape[0]\n",
    "        preds = []\n",
    "        is_cuml = False\n",
    "        try:\n",
    "            import cuml\n",
    "            if 'cuml' in str(type(model)) or 'cuml' in str(type(getattr(model, 'estimator', None))):\n",
    "                is_cuml = True\n",
    "                import cupy as cp\n",
    "        except: pass\n",
    "\n",
    "        for i in range(0, n_test, batch_size):\n",
    "            X_batch = np.hstack([feat_dict[k][i:i+batch_size] for k in keys]).astype(np.float32)\n",
    "            X_batch = scaler.transform(X_batch)\n",
    "            if is_cuml: X_batch = cp.asarray(X_batch)\n",
    "            p = model.predict_proba(X_batch)\n",
    "            if is_cuml:\n",
    "                if hasattr(p, 'get'): p = p.get()\n",
    "                elif hasattr(p, 'to_numpy'): p = p.to_numpy()\n",
    "            preds.append(p)\n",
    "            del X_batch\n",
    "        return np.vstack(preds)\n",
    "\n",
    "    # Helper for batched prediction from weights (LogReg)\n",
    "    def predict_proba_from_weights(weights_path, feat_dict, keys, scaler, batch_size=5000):\n",
    "        model_data = joblib.load(weights_path)\n",
    "        coef = model_data['coef'] # (n_classes, n_features)\n",
    "        intercept = model_data['intercept'] # (n_classes,)\n",
    "        \n",
    "        # Use GPU if available\n",
    "        try:\n",
    "            import cupy as cp\n",
    "            coef = cp.asarray(coef)\n",
    "            intercept = cp.asarray(intercept)\n",
    "            use_gpu = True\n",
    "        except:\n",
    "            use_gpu = False\n",
    "            \n",
    "        n_test = feat_dict[keys[0]].shape[0]\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(0, n_test, batch_size):\n",
    "            X_batch = np.hstack([feat_dict[k][i:i+batch_size] for k in keys]).astype(np.float32)\n",
    "            X_batch = scaler.transform(X_batch)\n",
    "            \n",
    "            if use_gpu:\n",
    "                X_batch = cp.asarray(X_batch)\n",
    "                # LogReg: sigmoid(X @ W.T + b)\n",
    "                logits = cp.dot(X_batch, coef.T) + intercept\n",
    "                p = 1 / (1 + cp.exp(-logits))\n",
    "                p = p.get()\n",
    "            else:\n",
    "                logits = np.dot(X_batch, coef.T) + intercept\n",
    "                p = 1 / (1 + np.exp(-logits))\n",
    "                \n",
    "            preds.append(p)\n",
    "            del X_batch\n",
    "        \n",
    "        if use_gpu:\n",
    "            del coef, intercept\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "            \n",
    "        return np.vstack(preds)\n",
    "\n",
    "    # A. LogReg Test Preds\n",
    "    print(\"Generating LogReg Test Predictions...\")\n",
    "    test_preds_logreg = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "    for fold in range(5):\n",
    "        print(f\"  Loading LogReg Fold {fold+1}...\")\n",
    "        scaler = joblib.load(WORK_ROOT / 'features' / f'level1_logreg_scaler_fold{fold}.pkl')\n",
    "        \n",
    "        # Check if we have weights or model\n",
    "        weights_path = WORK_ROOT / 'features' / f'level1_logreg_weights_fold{fold}.pkl'\n",
    "        model_path = WORK_ROOT / 'features' / f'level1_logreg_fold{fold}.pkl'\n",
    "        \n",
    "        if weights_path.exists():\n",
    "            probs = predict_proba_from_weights(weights_path, features_test, FLAT_KEYS, scaler)\n",
    "        elif model_path.exists():\n",
    "            clf = joblib.load(model_path)\n",
    "            probs = predict_proba_batched_scaled(clf, features_test, FLAT_KEYS, scaler)\n",
    "            del clf\n",
    "        else:\n",
    "            print(f\"  [WARNING] No model found for fold {fold}\")\n",
    "            probs = np.zeros_like(test_preds_logreg) # Should not happen\n",
    "            \n",
    "        test_preds_logreg += probs / 5.0\n",
    "        del scaler, probs\n",
    "        gc.collect()\n",
    "    np.save(WORK_ROOT / 'features' / 'test_pred_logreg.npy', test_preds_logreg)\n",
    "    del test_preds_logreg\n",
    "    gc.collect()\n",
    "    print(\"LogReg Test Preds Saved.\")\n",
    "\n",
    "    # B. GBDT Test Preds\n",
    "    if HAS_PYBOOST:\n",
    "        print(\"Generating GBDT Test Predictions...\")\n",
    "        test_preds_gbdt = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "        \n",
    "        # Helper for GBDT batched\n",
    "        def predict_proba_batched_gbdt(model, feat_dict, keys, batch_size=5000):\n",
    "            n_test = feat_dict[keys[0]].shape[0]\n",
    "            preds = []\n",
    "            for i in range(0, n_test, batch_size):\n",
    "                X_batch = np.hstack([feat_dict[k][i:i+batch_size] for k in keys]).astype(np.float32)\n",
    "                p = model.predict(X_batch)\n",
    "                preds.append(p)\n",
    "                del X_batch\n",
    "            return np.vstack(preds)\n",
    "\n",
    "        for fold in range(5):\n",
    "            print(f\"  Loading GBDT Fold {fold+1}...\")\n",
    "            model = GradientBoosting.load(str(WORK_ROOT / 'features' / f'level1_gbdt_fold{fold}.json'))\n",
    "            probs = predict_proba_batched_gbdt(model, features_test, FLAT_KEYS)\n",
    "            test_preds_gbdt += probs / 5.0\n",
    "            del model, probs\n",
    "            gc.collect()\n",
    "        np.save(WORK_ROOT / 'features' / 'test_pred_gbdt.npy', test_preds_gbdt)\n",
    "        del test_preds_gbdt\n",
    "        gc.collect()\n",
    "        print(\"GBDT Test Preds Saved.\")\n",
    "        \n",
    "    # Cleanup Test Features\n",
    "    del features_test\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f25ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8e - Phase 2b: Deep Learning (DNN)\n",
    "if TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 2b: Deep Learning (DNN) ===\")\n",
    "    \n",
    "    # 1. Reload dicts\n",
    "    features_train, features_test = load_features_dict()\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # C. DNN Ensemble (PyTorch, IA-weighted, multimodal, multi-state)\n",
    "    # ------------------------------------------\n",
    "    print(\"\\n--- Training DNN Ensemble (IA-weighted, multimodal, multi-state) ---\")\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Build a stable per-label IA weight vector for the current TOP_K targets\n",
    "    ia_w = weights.copy()\n",
    "    ia_w = np.where(np.isfinite(ia_w) & (ia_w > 0), ia_w, 1.0).astype(np.float32)\n",
    "    ia_w = ia_w / float(np.mean(ia_w))\n",
    "    ia_w = np.clip(ia_w, 0.5, 5.0)\n",
    "    ia_w_t = torch.tensor(ia_w, dtype=torch.float32, device=device).view(1, -1)\n",
    "    \n",
    "    # Optional: include other model predictions as an input stream (PB OOFs analogue)\n",
    "    USE_BASE_OOFS_IN_DNN = True\n",
    "    if USE_BASE_OOFS_IN_DNN and (WORK_ROOT / 'features' / 'oof_pred_logreg.npy').exists():\n",
    "        oof_stream = [np.load(WORK_ROOT / 'features' / 'oof_pred_logreg.npy').astype(np.float32)]\n",
    "        test_stream = [np.load(WORK_ROOT / 'features' / 'test_pred_logreg.npy').astype(np.float32)]\n",
    "        if (WORK_ROOT / 'features' / 'oof_pred_gbdt.npy').exists():\n",
    "            oof_stream.append(np.load(WORK_ROOT / 'features' / 'oof_pred_gbdt.npy').astype(np.float32))\n",
    "            test_stream.append(np.load(WORK_ROOT / 'features' / 'test_pred_gbdt.npy').astype(np.float32))\n",
    "        base_oof = np.hstack(oof_stream)\n",
    "        base_test = np.hstack(test_stream)\n",
    "        features_train['base_oof'] = base_oof\n",
    "        features_test['base_oof'] = base_test\n",
    "        print(f\"Base OOF stream: train={base_oof.shape} test={base_test.shape}\")\n",
    "        \n",
    "    # Select modality keys for the DNN (towers)\n",
    "    DNN_KEYS = [k for k in ['t5', 'esm2_650m', 'esm2_3b', 'taxa', 'text', 'base_oof'] if k in features_train]\n",
    "    print(f\"DNN modality keys={DNN_KEYS}\")\n",
    "    \n",
    "    class Tower(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim=512, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(1024, out_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "            \n",
    "    class ColossalMultiModalDNN(nn.Module):\n",
    "        def __init__(self, dims: dict, output_dim: int):\n",
    "            super().__init__()\n",
    "            self.keys = list(dims.keys())\n",
    "            self.towers = nn.ModuleDict({k: Tower(dims[k]) for k in self.keys})\n",
    "            fused_dim = 512 * len(self.keys)\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(fused_dim, 2048),\n",
    "                nn.BatchNorm1d(2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1024, output_dim),\n",
    "            )\n",
    "        def forward(self, batch: dict):\n",
    "            hs = [self.towers[k](batch[k]) for k in self.keys]\n",
    "            h = torch.cat(hs, dim=1)\n",
    "            return self.head(h)\n",
    "            \n",
    "    # Prepare torch tensors per modality\n",
    "    train_t = {k: torch.tensor(features_train[k], dtype=torch.float32, device=device) for k in DNN_KEYS}\n",
    "    test_t = {k: torch.tensor(features_test[k], dtype=torch.float32, device=device) for k in DNN_KEYS}\n",
    "    \n",
    "    def _batch_dict(tensors: dict, idx):\n",
    "        return {k: v[idx] for k, v in tensors.items()}\n",
    "        \n",
    "    # Multi-state ensembling\n",
    "    DNN_SEEDS = [42, 43, 44, 45, 46]\n",
    "    DNN_EPOCHS = 10\n",
    "    BATCH_SIZE = 256\n",
    "    oof_sum = np.zeros(Y.shape, dtype=np.float32)\n",
    "    test_sum = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "    n_states = len(DNN_SEEDS)\n",
    "    \n",
    "    for state_i, seed in enumerate(DNN_SEEDS, 1):\n",
    "        print(f\"\\n[DNN] Random state {state_i}/{n_states}: seed={seed}\")\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        kf_state = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        oof_state = np.zeros(Y.shape, dtype=np.float32)\n",
    "        test_state = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "        dims = {k: int(features_train[k].shape[1]) for k in DNN_KEYS}\n",
    "        \n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf_state.split(train_ids)):\n",
    "            print(f\"DNN Fold {fold+1}/5\")\n",
    "            model = ColossalMultiModalDNN(dims=dims, output_dim=Y.shape[1]).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "            Y_full_t = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "            n_samples = len(idx_tr)\n",
    "            model.train()\n",
    "            idx_tr_t = torch.tensor(idx_tr, dtype=torch.long, device=device)\n",
    "            \n",
    "            for _epoch in range(DNN_EPOCHS):\n",
    "                perm = idx_tr_t[torch.randperm(n_samples, device=device)]\n",
    "                for i in range(0, n_samples, BATCH_SIZE):\n",
    "                    b = perm[i:i + BATCH_SIZE]\n",
    "                    optimizer.zero_grad()\n",
    "                    logits = model(_batch_dict(train_t, b))\n",
    "                    yb = Y_full_t[b]\n",
    "                    loss_el = F.binary_cross_entropy_with_logits(logits, yb, reduction='none')\n",
    "                    loss = (loss_el * ia_w_t).mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                idx_val_t = torch.tensor(idx_val, dtype=torch.long, device=device)\n",
    "                val_probs = torch.sigmoid(model(_batch_dict(train_t, idx_val_t))).cpu().numpy()\n",
    "                oof_state[idx_val] = val_probs\n",
    "                test_probs = torch.sigmoid(model(test_t)).cpu().numpy()\n",
    "                test_state += test_probs / kf_state.get_n_splits()\n",
    "                \n",
    "            val_preds = (val_probs > 0.3).astype(int)\n",
    "            f1 = f1_score(Y[idx_val], val_preds, average='micro')\n",
    "            ia_f1 = ia_weighted_f1(Y[idx_val], val_probs, thr=0.3)\n",
    "            print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "            print(f\"  >> Fold {fold+1} IA-F1: ALL={ia_f1['ALL']:.4f}\")\n",
    "            \n",
    "            torch.save(model.state_dict(), WORK_ROOT / 'features' / f'level1_dnn_seed{seed}_fold{fold}.pth')\n",
    "            \n",
    "        oof_sum += oof_state\n",
    "        test_sum += test_state\n",
    "        \n",
    "    oof_preds_dnn = (oof_sum / n_states).astype(np.float32)\n",
    "    test_preds_dnn = (test_sum / n_states).astype(np.float32)\n",
    "    np.save(WORK_ROOT / 'features' / 'oof_pred_dnn.npy', oof_preds_dnn)\n",
    "    np.save(WORK_ROOT / 'features' / 'test_pred_dnn.npy', test_preds_dnn)\n",
    "    print(\"DNN OOF + test preds saved (multi-state averaged).\")\n",
    "    \n",
    "    # Persist term list\n",
    "    with open(WORK_ROOT / 'features' / 'top_terms_1500.json', 'w') as f:\n",
    "        json.dump(top_terms, f)\n",
    "        \n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        req = [WORK_ROOT / 'features' / 'top_terms_1500.json', WORK_ROOT / 'features' / 'oof_pred_logreg.npy', WORK_ROOT / 'features' / 'test_pred_logreg.npy', WORK_ROOT / 'features' / 'oof_pred_dnn.npy', WORK_ROOT / 'features' / 'test_pred_dnn.npy']\n",
    "        req += [WORK_ROOT / 'features' / 'oof_pred_gbdt.npy', WORK_ROOT / 'features' / 'test_pred_gbdt.npy']\n",
    "        STORE.push('stage_07_level1_preds', req, note='Level-1 OOF + test preds (LR/GBDT/DNN)')\n",
    "        \n",
    "    print(\"Phase 2 Complete.\")\n",
    "else:\n",
    "    print(\"Skipping Phase 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6574cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 - Phase 3: Hierarchy-Aware Stacking (GCN)\n",
    "# =============================================================================\n",
    "# PHASE 3: GCN STACKER (Hierarchy-Aware Refinement)\n",
    "# =============================================================================\n",
    "# Strategy:\n",
    "# 1. Input: OOF predictions from Level 1 models (LogReg, GBDT, DNN) -> Shape (N, K, 3)\n",
    "# 2. Graph: GO Ontology Adjacency Matrix (K, K)\n",
    "# 3. Model: GCN that refines predictions based on graph structure\n",
    "# 4. Output: Refined probabilities (N, K)\n",
    "\n",
    "TRAIN_STACKER = True\n",
    "\n",
    "if TRAIN_STACKER and TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 3: Hierarchy-Aware Stacking (GCN) ===\")\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import networkx as nx\n",
    "    \n",
    "    # 1. Load OOF Predictions\n",
    "    print(\"Loading Level 1 OOFs...\")\n",
    "    oof_files = ['oof_pred_logreg.npy', 'oof_pred_gbdt.npy', 'oof_pred_dnn.npy']\n",
    "    oofs = []\n",
    "    for f in oof_files:\n",
    "        p = WORK_ROOT / 'features' / f\n",
    "        if p.exists():\n",
    "            oofs.append(np.load(p).astype(np.float32))\n",
    "            print(f\"  Loaded {f}\")\n",
    "    \n",
    "    if not oofs:\n",
    "        raise RuntimeError(\"CRITICAL: No Level 1 OOF predictions found (LogReg/GBDT/DNN). Cannot train Stacker. Check Phase 2 execution.\")\n",
    "    else:\n",
    "        # Stack: (N, K, M) where M is number of models\n",
    "        X_stack = np.stack(oofs, axis=2) \n",
    "        print(f\"Stacker Input Shape: {X_stack.shape} (Samples, Terms, Models)\")\n",
    "        \n",
    "        # 2. Build Adjacency Matrix\n",
    "        print(\"Building GO Graph Adjacency Matrix...\")\n",
    "        try:\n",
    "            import obonet\n",
    "            obo_path = WORK_ROOT.parent / 'go-basic.obo'\n",
    "            if not obo_path.exists(): obo_path = Path('go-basic.obo')\n",
    "            graph = obonet.read_obo(obo_path)\n",
    "            \n",
    "            # Create subgraph for our K terms\n",
    "            # Map term to index\n",
    "            term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "            \n",
    "            # Build adjacency (A_ij = 1 if i is parent of j or j is parent of i? GCN usually undirected or directed?)\n",
    "            # Standard GCN: Undirected + Self-loops.\n",
    "            # But GO is directed (Child -> Parent).\n",
    "            # We want information to flow both ways? Or mostly Child -> Parent (consistency)?\n",
    "            # Let's use a symmetric normalized adjacency for standard GCN.\n",
    "            \n",
    "            adj = np.eye(len(top_terms), dtype=np.float32) # Self-loops\n",
    "            \n",
    "            # Fill edges\n",
    "            edges_count = 0\n",
    "            for child in top_terms:\n",
    "                if child in graph:\n",
    "                    for parent in graph.successors(child): # 'is_a' points to parent in obonet/networkx\n",
    "                        if parent in term_to_idx:\n",
    "                            i, j = term_to_idx[child], term_to_idx[parent]\n",
    "                            adj[i, j] = 1.0\n",
    "                            adj[j, i] = 1.0 # Symmetric\n",
    "                            edges_count += 1\n",
    "            \n",
    "            print(f\"Graph built. Nodes: {len(top_terms)}, Edges (in subset): {edges_count}\")\n",
    "            \n",
    "            # Normalize Adjacency: D^{-1/2} A D^{-1/2}\n",
    "            D = np.sum(adj, axis=1)\n",
    "            D_inv_sqrt = np.power(D, -0.5)\n",
    "            D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0.\n",
    "            D_mat = np.diag(D_inv_sqrt)\n",
    "            A_norm = D_mat @ adj @ D_mat\n",
    "            A_norm = torch.tensor(A_norm, dtype=torch.float32, device=device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to build graph: {e}. Using Identity (No-op GCN).\")\n",
    "            A_norm = torch.eye(len(top_terms), device=device)\n",
    "\n",
    "        # 3. Define GCN Model\n",
    "        class GCNStacker(nn.Module):\n",
    "            def __init__(self, n_models, n_hidden=16):\n",
    "                super().__init__()\n",
    "                # We apply GCN per protein.\n",
    "                # Input features per node: [p_logreg, p_gbdt, p_dnn] (dim=3)\n",
    "                # We want to learn a mixing weight + graph smoothing.\n",
    "                \n",
    "                # Simple 2-layer GCN\n",
    "                self.gc1 = nn.Linear(n_models, n_hidden)\n",
    "                self.gc2 = nn.Linear(n_hidden, 1) # Output 1 score per node\n",
    "                self.relu = nn.ReLU()\n",
    "                \n",
    "            def forward(self, x, adj):\n",
    "                # x: (Batch, Nodes, Models)\n",
    "                # adj: (Nodes, Nodes)\n",
    "                \n",
    "                # Layer 1: H1 = ReLU(A X W1)\n",
    "                # Support = X W1\n",
    "                # Output = A Support\n",
    "                \n",
    "                # Batch matmul is tricky with static adj.\n",
    "                # x: (B, N, M)\n",
    "                # W1: (M, H)\n",
    "                # x @ W1 -> (B, N, H)\n",
    "                support = self.gc1(x) \n",
    "                \n",
    "                # A @ support\n",
    "                # A: (N, N)\n",
    "                # support: (B, N, H) -> permute to (B, H, N) for matmul?\n",
    "                # Or just (A @ support[i]) for each i?\n",
    "                # Einsum: n k, b k h -> b n h\n",
    "                out1 = torch.einsum('nk,bkh->bnh', adj, support)\n",
    "                out1 = self.relu(out1)\n",
    "                \n",
    "                # Layer 2\n",
    "                support2 = self.gc2(out1) # (B, N, 1)\n",
    "                out2 = torch.einsum('nk,bkh->bnh', adj, support2)\n",
    "                \n",
    "                return out2.squeeze(-1) # (B, N)\n",
    "\n",
    "        # 4. Train Stacker\n",
    "        print(\"Training GCN Stacker...\")\n",
    "        # We split the OOFs into train/val for the stacker?\n",
    "        # Actually, OOFs are already \"test-like\" for the whole train set.\n",
    "        # We can train on the whole train set (since OOFs were generated via CV).\n",
    "        \n",
    "        # Targets\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "        X_stack_tensor = torch.tensor(X_stack, dtype=torch.float32, device=device)\n",
    "        \n",
    "        model = GCNStacker(n_models=X_stack.shape[2]).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        BATCH_SIZE = 128\n",
    "        N_EPOCHS = 20\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X_stack_tensor, Y_tensor)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            total_loss = 0\n",
    "            for xb, yb in loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb, A_norm)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{N_EPOCHS} Loss: {total_loss/len(loader):.4f}\")\n",
    "                \n",
    "        # 5. Generate Final Predictions (on Test)\n",
    "        print(\"Generating Final Stacker Predictions on Test...\")\n",
    "        # Load Test Preds\n",
    "        test_files = ['test_pred_logreg.npy', 'test_pred_gbdt.npy', 'test_pred_dnn.npy']\n",
    "        tests = []\n",
    "        for f in test_files:\n",
    "            p = WORK_ROOT / 'features' / f\n",
    "            if p.exists():\n",
    "                tests.append(np.load(p).astype(np.float32))\n",
    "        \n",
    "        if tests:\n",
    "            X_test_stack = np.stack(tests, axis=2)\n",
    "            X_test_tensor = torch.tensor(X_test_stack, dtype=torch.float32, device=device)\n",
    "            \n",
    "            model.eval()\n",
    "            final_preds = []\n",
    "            with torch.no_grad():\n",
    "                # Process in chunks to avoid OOM\n",
    "                for i in range(0, len(X_test_tensor), BATCH_SIZE):\n",
    "                    batch = X_test_tensor[i:i+BATCH_SIZE]\n",
    "                    logits = model(batch, A_norm)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                    final_preds.append(probs)\n",
    "            \n",
    "            final_preds = np.vstack(final_preds)\n",
    "            np.save(WORK_ROOT / 'features' / 'final_pred_gcn.npy', final_preds)\n",
    "            print(f\"Final GCN Predictions Saved: {final_preds.shape}\")\n",
    "            \n",
    "            # Clean up\n",
    "            del X_stack_tensor, Y_tensor, X_test_tensor, model, A_norm\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise RuntimeError(\"CRITICAL: No Level 1 Test predictions found. Cannot generate final submission.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Phase 3 (Stacker).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10 - Phase 4: Strict Post-Processing & Submission\n",
    "# =============================================================================\n",
    "# PHASE 4: STRICT POST-PROCESSING (Hierarchy Enforcement)\n",
    "# =============================================================================\n",
    "# 1. Max Propagation (Parent Rule): P(parent) >= P(child)\n",
    "# 2. Min Propagation (Child Rule): P(child) <= P(parent)\n",
    "# 3. Top-1500 Selection\n",
    "# 4. Submission Generation\n",
    "\n",
    "APPLY_POSTPROC = True\n",
    "\n",
    "if APPLY_POSTPROC:\n",
    "    print(\"\\n=== Phase 4: Strict Post-Processing ===\")\n",
    "    \n",
    "    # Load predictions (prefer GCN, fallback to DNN/LogReg average)\n",
    "    if (WORK_ROOT / 'features' / 'final_pred_gcn.npy').exists():\n",
    "        print(\"Loading GCN predictions...\")\n",
    "        preds = np.load(WORK_ROOT / 'features' / 'final_pred_gcn.npy')\n",
    "    elif (WORK_ROOT / 'features' / 'test_pred_dnn.npy').exists():\n",
    "        print(\"Loading DNN predictions (GCN missing)...\")\n",
    "        preds = np.load(WORK_ROOT / 'features' / 'test_pred_dnn.npy')\n",
    "    else:\n",
    "        print(\"Loading LogReg predictions (Fallback)...\")\n",
    "        preds = np.load(WORK_ROOT / 'features' / 'test_pred_logreg.npy')\n",
    "        \n",
    "    print(f\"Raw Predictions Shape: {preds.shape}\")\n",
    "    \n",
    "    # Load Graph for Propagation\n",
    "    try:\n",
    "        import obonet\n",
    "        import networkx as nx\n",
    "        obo_path = WORK_ROOT.parent / 'go-basic.obo'\n",
    "        if not obo_path.exists(): obo_path = Path('go-basic.obo')\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        \n",
    "        # Map term to index\n",
    "        term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "        \n",
    "        # Precompute parent/child indices for fast propagation\n",
    "        # We need a topological sort or just iterate multiple times?\n",
    "        # Since DAG depth is small, iterating 2-3 times is usually enough.\n",
    "        # Or we can just use the edge list.\n",
    "        \n",
    "        # Build parent map: child_idx -> [parent_indices]\n",
    "        child_to_parents = {}\n",
    "        parent_to_children = {}\n",
    "        \n",
    "        for child in top_terms:\n",
    "            if child in graph:\n",
    "                c_idx = term_to_idx[child]\n",
    "                # Parents\n",
    "                parents = [p for p in graph.successors(child) if p in term_to_idx]\n",
    "                if parents:\n",
    "                    child_to_parents[c_idx] = [term_to_idx[p] for p in parents]\n",
    "                \n",
    "                # Children (predecessors in networkx/obonet)\n",
    "                children = [c for c in graph.predecessors(child) if c in term_to_idx]\n",
    "                if children:\n",
    "                    parent_to_children[c_idx] = [term_to_idx[c] for c in children]\n",
    "                    \n",
    "        print(f\"Hierarchy constraints: {len(child_to_parents)} terms have parents in subset.\")\n",
    "        \n",
    "        # 1. Max Propagation (Child -> Parent)\n",
    "        # Ensure Parent >= Child\n",
    "        # Iterate a few times to propagate up\n",
    "        print(\"Applying Max Propagation (Child -> Parent)...\")\n",
    "        for _ in range(3):\n",
    "            for c_idx, p_indices in child_to_parents.items():\n",
    "                # Vectorized update for all samples\n",
    "                # P(parent) = max(P(parent), P(child))\n",
    "                # We can do this efficiently?\n",
    "                # preds[:, p_indices] = np.maximum(preds[:, p_indices], preds[:, c_idx:c_idx+1])\n",
    "                # This is slow in python loop.\n",
    "                pass\n",
    "        \n",
    "        # Optimized Propagation (Matrix based?)\n",
    "        # Since we have 13.5k terms, a loop is okay if we batch operations?\n",
    "        # Actually, iterating over 13k terms is fast in Python (milliseconds).\n",
    "        # The operation inside is numpy array (20k samples).\n",
    "        \n",
    "        # Let's do a topological sort order to do it in 1 pass?\n",
    "        # Subgraph of top_terms\n",
    "        subgraph = graph.subgraph(top_terms)\n",
    "        try:\n",
    "            topo_order = list(nx.topological_sort(subgraph))\n",
    "            # Reverse topo order for Child -> Parent (Leaves first)\n",
    "            topo_order_rev = topo_order[::-1]\n",
    "            \n",
    "            print(\"  Optimized Max Prop (1 pass)...\")\n",
    "            for term in topo_order_rev:\n",
    "                if term not in term_to_idx: continue\n",
    "                c_idx = term_to_idx[term]\n",
    "                # Propagate to parents\n",
    "                if term in child_to_parents: # Use precomputed map\n",
    "                    # parents = child_to_parents[term] # Wait, map uses indices\n",
    "                    pass\n",
    "                \n",
    "                # Use graph directly\n",
    "                parents = [p for p in graph.successors(term) if p in term_to_idx]\n",
    "                if not parents: continue\n",
    "                \n",
    "                p_indices = [term_to_idx[p] for p in parents]\n",
    "                # preds[:, p_indices] = np.maximum(preds[:, p_indices], preds[:, c_idx, None])\n",
    "                # Use broadcasting\n",
    "                child_val = preds[:, c_idx:c_idx+1]\n",
    "                preds[:, p_indices] = np.maximum(preds[:, p_indices], child_val)\n",
    "                \n",
    "        except nx.NetworkXUnfeasible:\n",
    "            print(\"  [WARNING] Cycle detected or graph issue. Skipping topological prop.\")\n",
    "            \n",
    "        # 2. Min Propagation (Parent -> Child)\n",
    "        # Ensure Child <= Parent\n",
    "        # Iterate Root -> Leaves (Topo order)\n",
    "        print(\"Applying Min Propagation (Parent -> Child)...\")\n",
    "        try:\n",
    "            for term in topo_order:\n",
    "                if term not in term_to_idx: continue\n",
    "                p_idx = term_to_idx[term]\n",
    "                \n",
    "                children = [c for c in graph.predecessors(term) if c in term_to_idx]\n",
    "                if not children: continue\n",
    "                \n",
    "                c_indices = [term_to_idx[c] for c in children]\n",
    "                parent_val = preds[:, p_idx:p_idx+1]\n",
    "                preds[:, c_indices] = np.minimum(preds[:, c_indices], parent_val)\n",
    "                \n",
    "        except: pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Post-processing failed: {e}\")\n",
    "        \n",
    "    # 3. Submission Formatting\n",
    "    print(\"Formatting Submission...\")\n",
    "    # Clip to (0, 1]\n",
    "    preds = np.clip(preds, 0.0, 1.0)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    # Format: ProteinID, TermID, Score\n",
    "    # We need to melt the matrix. This is huge (20k * 13.5k = 270M rows).\n",
    "    # We only keep top 1500 per protein?\n",
    "    \n",
    "    # Strategy:\n",
    "    # 1. For each protein, find top 1500 indices.\n",
    "    # 2. Create sparse rows.\n",
    "    \n",
    "    submission_rows = []\n",
    "    test_ids_list = test_ids.tolist()\n",
    "    \n",
    "    print(\"Selecting Top-1500 per protein...\")\n",
    "    for i, pid in enumerate(test_ids_list):\n",
    "        # Get scores for this protein\n",
    "        scores = preds[i]\n",
    "        # Find top 1500 indices\n",
    "        # argpartition is faster than sort\n",
    "        if len(scores) > 1500:\n",
    "            top_indices = np.argpartition(scores, -1500)[-1500:]\n",
    "        else:\n",
    "            top_indices = np.arange(len(scores))\n",
    "            \n",
    "        # Filter out zero scores?\n",
    "        # top_indices = top_indices[scores[top_indices] > 0.001]\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            score = scores[idx]\n",
    "            if score > 0: # Only positive\n",
    "                term = top_terms[idx]\n",
    "                submission_rows.append((pid, term, f\"{score:.3f}\"))\n",
    "                \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(test_ids_list)} proteins...\")\n",
    "            \n",
    "    # Write to file\n",
    "    print(\"Writing submission.tsv...\")\n",
    "    with open('submission.tsv', 'w') as f:\n",
    "        # No header usually? Or check sample_submission\n",
    "        # CAFA format: no header? Or header?\n",
    "        # Sample submission has header?\n",
    "        # Let's check sample_submission.tsv\n",
    "        pass\n",
    "        \n",
    "    # Just write standard TSV\n",
    "    df_sub = pd.DataFrame(submission_rows, columns=['Protein Id', 'GO Term Id', 'Prediction'])\n",
    "    df_sub.to_csv('submission.tsv', sep='\\t', index=False, header=False)\n",
    "    print(f\"Submission saved: {len(df_sub)} rows.\")\n",
    "    \n",
    "    # Zip it\n",
    "    os.system('zip submission.zip submission.tsv')\n",
    "    print(\"Zipped to submission.zip\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Post-Processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13E - KNN (cosine; ESM2-3B)\n",
    "# Notes:\n",
    "# - Uses in-memory features from CELL 13 (features_train/features_test, Y, top_terms).\n",
    "# - Checkpoint pushing is controlled globally by CAFA_CHECKPOINT_PUSH (default 0 in this notebook).\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping KNN (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import os\n",
    "    import json\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    FORCE_REBUILD = (os.getenv('CAFA_FORCE_REBUILD', '0').strip() == '1')\n",
    "\n",
    "    PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    knn_oof_path = PRED_DIR / 'oof_pred_knn.npy'\n",
    "    knn_test_path = PRED_DIR / 'test_pred_knn.npy'\n",
    "\n",
    "    # Backwards-compatible copies (some downstream code loads from WORK_ROOT/features)\n",
    "    knn_oof_compat = WORK_ROOT / 'features' / 'oof_pred_knn.npy'\n",
    "    knn_test_compat = WORK_ROOT / 'features' / 'test_pred_knn.npy'\n",
    "\n",
    "    if knn_oof_path.exists() and knn_test_path.exists() and (not FORCE_REBUILD):\n",
    "        print('KNN preds exist; skipping training (set CAFA_FORCE_REBUILD=1 to force).')\n",
    "        oof_pred_knn = np.load(knn_oof_path)\n",
    "        test_pred_knn = np.load(knn_test_path)\n",
    "        oof_max_sim = None\n",
    "    else:\n",
    "        if 'features_train' not in globals() or 'features_test' not in globals():\n",
    "            raise RuntimeError('Missing `features_train`/`features_test`. Run CELL 13 first.')\n",
    "        if 'esm2_3b' not in features_train:\n",
    "            raise FileNotFoundError(\"Missing required modality 'esm2_3b' in features_train. Ensure features/train_embeds_esm2_3b.npy exists.\")\n",
    "\n",
    "        X_knn = features_train['esm2_3b'].astype(np.float32)\n",
    "        X_knn_test = features_test['esm2_3b'].astype(np.float32)\n",
    "\n",
    "        # Enforce TOP_K alignment using the persisted term list.\n",
    "        top_terms_path = WORK_ROOT / 'features' / 'top_terms_1500.json'\n",
    "        if top_terms_path.exists():\n",
    "            top_terms_knn = json.loads(top_terms_path.read_text())\n",
    "            if Y.shape[1] != len(top_terms_knn):\n",
    "                raise ValueError(f'KNN shape mismatch: Y has {Y.shape[1]} cols but top_terms_1500.json has {len(top_terms_knn)} terms.')\n",
    "        else:\n",
    "            if 'top_terms' not in globals():\n",
    "                raise RuntimeError('Missing top_terms_1500.json and in-memory top_terms. Run CELL 13 first.')\n",
    "            top_terms_knn = list(top_terms)\n",
    "            if Y.shape[1] != len(top_terms_knn):\n",
    "                raise ValueError(f'KNN shape mismatch: Y has {Y.shape[1]} cols but top_terms has {len(top_terms_knn)} terms.')\n",
    "\n",
    "        # KNN needs binary targets (presence/absence), not counts.\n",
    "        Y_knn = (Y > 0).astype(np.float32)\n",
    "\n",
    "        def _l2_norm(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "            n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "            return x / np.maximum(n, eps)\n",
    "\n",
    "        # Cosine distance is best-behaved on L2-normalised vectors\n",
    "        X_knn = _l2_norm(X_knn)\n",
    "        X_knn_test = _l2_norm(X_knn_test)\n",
    "\n",
    "        KNN_K = int(os.getenv('CAFA_KNN_K', '50'))\n",
    "        KNN_BATCH = int(os.getenv('CAFA_KNN_BATCH', '256'))\n",
    "\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        oof_pred_knn = np.zeros((X_knn.shape[0], Y_knn.shape[1]), dtype=np.float32)\n",
    "        test_pred_knn = np.zeros((X_knn_test.shape[0], Y_knn.shape[1]), dtype=np.float32)\n",
    "        oof_max_sim = np.zeros((X_knn.shape[0],), dtype=np.float32)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_knn), start=1):\n",
    "            print(f'Fold {fold}/{n_splits} (KNN)')\n",
    "            knn = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=4)\n",
    "            knn.fit(X_knn[tr_idx])\n",
    "\n",
    "            dists, neigh = knn.kneighbors(X_knn[va_idx], return_distance=True)\n",
    "            sims = np.clip((1.0 - dists).astype(np.float32), 0.0, 1.0)\n",
    "            oof_max_sim[va_idx] = sims.max(axis=1)\n",
    "            neigh_global = tr_idx[neigh]  # map to global row indices into Y_knn\n",
    "\n",
    "            for i in range(0, len(va_idx), KNN_BATCH):\n",
    "                j = min(i + KNN_BATCH, len(va_idx))\n",
    "                neigh_b = neigh_global[i:j]\n",
    "                sims_b = sims[i:j]\n",
    "                denom = np.maximum(sims_b.sum(axis=1, keepdims=True), 1e-8)\n",
    "                Y_nei = Y_knn[neigh_b]  # (B, K, L)\n",
    "                scores = (np.einsum('bk,bkl->bl', sims_b, Y_nei) / denom).astype(np.float32)\n",
    "                oof_pred_knn[va_idx[i:j]] = scores\n",
    "\n",
    "            if 'ia_weighted_f1' in globals():\n",
    "                print('  IA-F1:', ia_weighted_f1(Y_knn[va_idx], oof_pred_knn[va_idx], thr=0.3))\n",
    "\n",
    "        # Final model on full train -> test\n",
    "        knn_final = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=4)\n",
    "        knn_final.fit(X_knn)\n",
    "        dists_te, neigh_te = knn_final.kneighbors(X_knn_test, return_distance=True)\n",
    "        sims_te = np.clip((1.0 - dists_te).astype(np.float32), 0.0, 1.0)\n",
    "        denom_te = np.maximum(sims_te.sum(axis=1, keepdims=True), 1e-8)\n",
    "\n",
    "        for i in range(0, X_knn_test.shape[0], KNN_BATCH):\n",
    "            j = min(i + KNN_BATCH, X_knn_test.shape[0])\n",
    "            neigh_b = neigh_te[i:j]\n",
    "            sims_b = sims_te[i:j]\n",
    "            Y_nei = Y_knn[neigh_b]\n",
    "            scores = (np.einsum('bk,bkl->bl', sims_b, Y_nei) / denom_te[i:j]).astype(np.float32)\n",
    "            test_pred_knn[i:j] = scores\n",
    "\n",
    "        np.save(knn_oof_path, oof_pred_knn)\n",
    "        np.save(knn_test_path, test_pred_knn)\n",
    "        np.save(knn_oof_compat, oof_pred_knn)\n",
    "        np.save(knn_test_compat, test_pred_knn)\n",
    "        print('Saved:', knn_oof_path)\n",
    "        print('Saved:', knn_test_path)\n",
    "\n",
    "    # Checkpoint push is controlled by CAFA_CHECKPOINT_PUSH inside STORE.push; no extra guard needed here.\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.push(\n",
    "            stage='stage_07d_level1_knn',\n",
    "            required_paths=[\n",
    "                str((WORK_ROOT / 'features' / 'top_terms_1500.json').as_posix()),\n",
    "                str(knn_oof_path.as_posix()),\n",
    "                str(knn_test_path.as_posix()),\n",
    "            ],\n",
    "            note='Level-1 KNN (cosine) predictions using ESM2-3B embeddings (OOF + test).',\n",
    "        )\n",
    "\n",
    "    # Diagnostics: similarity distribution + IA-F1 vs threshold\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        if oof_max_sim is not None:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(oof_max_sim, bins=50)\n",
    "            plt.title('KNN OOF diagnostic: max cosine similarity to neighbours (per protein)')\n",
    "            plt.xlabel('max similarity')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(Y, oof_pred_knn, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('KNN OOF: IA-weighted F1 vs threshold')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('KNN diagnostics skipped:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132ae0e",
   "metadata": {
    "id": "3132ae0e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 15 - Solution: 5b. PHASE 3: HIERARCHY-AWARE STACKING (GRAPH SMOOTHING GCN)\n",
    "# 5b. PHASE 3: HIERARCHY-AWARE STACKING (GRAPH SMOOTHING GCN)\n",
    "# =========================================================\n",
    "# Option B strictness: if PROCESS_EXTERNAL=True, we REQUIRE the propagated prior files to exist.\n",
    "PROCESS_EXTERNAL = globals().get('PROCESS_EXTERNAL', True)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "TOP_K = 1500\n",
    "\n",
    "def _load_level1_pred(fname: str):\n",
    "    \"\"\"Load Level-1 prediction arrays from either features/level1_preds or features/.\"\"\"\n",
    "    cand = [\n",
    "        WORK_ROOT / 'features' / 'level1_preds' / fname,\n",
    "        WORK_ROOT / 'features' / fname,\n",
    "    ]\n",
    "    for p in cand:\n",
    "        if p.exists():\n",
    "            return np.load(p).astype(np.float32)\n",
    "    return None\n",
    "\n",
    "# 1. Load Level-1 predictions\n",
    "train_feats = []\n",
    "for fname in ['oof_pred_logreg.npy', 'oof_pred_gbdt.npy', 'oof_pred_dnn.npy', 'oof_pred_knn.npy']:\n",
    "    arr = _load_level1_pred(fname)\n",
    "    if arr is not None:\n",
    "        train_feats.append(arr)\n",
    "if not train_feats:\n",
    "    raise FileNotFoundError('No Level-1 OOF predictions found. Run Phase 2 first.')\n",
    "X_stack = np.mean(train_feats, axis=0).astype(np.float32)\n",
    "\n",
    "# 2. Build Y label matrix for top-K terms\n",
    "train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "train_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "top_terms = train_terms['term'].value_counts().head(TOP_K).index.tolist()\n",
    "train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "Y_df = Y_df.reindex(train_ids, fill_value=0)\n",
    "Y = Y_df.values.astype(np.float32)\n",
    "\n",
    "# 2b. External priors (Phase 1 Step 4 outputs) -> inject as *conservative* extra signal\n",
    "EXTERNAL_PRIOR_WEIGHT = 0.25\n",
    "ext_dir = WORK_ROOT / 'external'\n",
    "prior_train_path = ext_dir / 'prop_train_no_kaggle.tsv.gz'\n",
    "if PROCESS_EXTERNAL:\n",
    "    if not prior_train_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f'Option B requires external priors, but missing: {prior_train_path}. '\n",
    "            'Run Phase 1 Step 4 propagation or ensure your checkpoint dataset contains these files (run setup: STORE.pull()).',\n",
    "        )\n",
    "    prior_train = pd.read_csv(prior_train_path, sep='\\t')\n",
    "    prior_train = prior_train[prior_train['term'].isin(top_terms)]\n",
    "    prior_mat = prior_train.pivot_table(index='EntryID', columns='term', values='score', aggfunc='max', fill_value=0.0)\n",
    "    prior_mat = prior_mat.reindex(train_ids.tolist(), fill_value=0.0)\n",
    "    prior_mat = prior_mat.reindex(columns=top_terms, fill_value=0.0)\n",
    "    prior_np = prior_mat.values.astype(np.float32)\n",
    "    X_stack = np.maximum(X_stack, EXTERNAL_PRIOR_WEIGHT * prior_np)\n",
    "    print(f'Injected external IEA prior into train stack (weight={EXTERNAL_PRIOR_WEIGHT}).')\n",
    "(WORK_ROOT / 'features').mkdir(parents=True, exist_ok=True)\n",
    "with open(WORK_ROOT / 'features' / 'top_terms_1500.json', 'w') as f:\n",
    "    json.dump(top_terms, f)\n",
    "print('Saved: top_terms_1500.json')\n",
    "\n",
    "# 3. Graph adjacency from go-basic.obo (reload if needed)\n",
    "if 'go_parents' not in locals() or 'go_namespaces' not in locals():\n",
    "    print('Reloading GO graph (parse_obo)...')\n",
    "    def parse_obo(path: Path):\n",
    "        parents = {}\n",
    "        namespaces = {}\n",
    "        cur_id, cur_ns = None, None\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line == '[Term]':\n",
    "                    if cur_id and cur_ns:\n",
    "                        namespaces[cur_id] = cur_ns\n",
    "                    cur_id, cur_ns = None, None\n",
    "                elif line.startswith('id: GO:'):\n",
    "                    cur_id = line.split('id: ', 1)[1]\n",
    "                elif line.startswith('namespace:'):\n",
    "                    cur_ns = line.split('namespace: ', 1)[1]\n",
    "                elif line.startswith('is_a:') and cur_id:\n",
    "                    parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                    parents.setdefault(cur_id, set()).add(parent)\n",
    "            if cur_id and cur_ns:\n",
    "                namespaces[cur_id] = cur_ns\n",
    "        return parents, namespaces\n",
    "    go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "def build_adjacency(terms_list, parents_dict):\n",
    "    term_to_idx = {t: i for i, t in enumerate(terms_list)}\n",
    "    n_terms = len(terms_list)\n",
    "    src, dst = [], []\n",
    "    for child in terms_list:\n",
    "        parents = parents_dict.get(child, set())\n",
    "        if not parents:\n",
    "            continue\n",
    "        child_idx = term_to_idx[child]\n",
    "        for parent in parents:\n",
    "            if parent in term_to_idx:\n",
    "                parent_idx = term_to_idx[parent]\n",
    "                src.append(child_idx); dst.append(parent_idx)\n",
    "                src.append(parent_idx); dst.append(child_idx)\n",
    "    src.extend(range(n_terms))\n",
    "    dst.extend(range(n_terms))\n",
    "    indices = torch.tensor([src, dst], dtype=torch.long)\n",
    "    values = torch.ones(len(src), dtype=torch.float32)\n",
    "    return torch.sparse_coo_tensor(indices, values, (n_terms, n_terms)).coalesce().to(device)\n",
    "class SimpleGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, adj_matrix):\n",
    "        super().__init__()\n",
    "        self.adj = adj_matrix\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sparse.mm(self.adj, x.t()).t()\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# 4. Build test stack once (and inject external priors once), then split by ontology\n",
    "print('\\nPreparing test stack...')\n",
    "test_feats = []\n",
    "for fname in ['test_pred_logreg.npy', 'test_pred_gbdt.npy', 'test_pred_dnn.npy', 'test_pred_knn.npy']:\n",
    "    arr = _load_level1_pred(fname)\n",
    "    if arr is not None:\n",
    "        test_feats.append(arr)\n",
    "if not test_feats:\n",
    "    raise FileNotFoundError('No Level-1 test predictions found. Run Phase 2 first.')\n",
    "X_test_stack = np.mean(test_feats, axis=0).astype(np.float32)\n",
    "prior_test_path = ext_dir / 'prop_test_no_kaggle.tsv.gz'\n",
    "if PROCESS_EXTERNAL:\n",
    "    if not prior_test_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f'Option B requires external priors, but missing: {prior_test_path}. '\n",
    "            'Run Phase 1 Step 4 propagation or ensure your checkpoint dataset contains these files (run setup: STORE.pull()).',\n",
    "        )\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id'].astype(str)\n",
    "    prior_test = pd.read_csv(prior_test_path, sep='\\t')\n",
    "    prior_test = prior_test[prior_test['term'].isin(top_terms)]\n",
    "    prior_t = prior_test.pivot_table(index='EntryID', columns='term', values='score', aggfunc='max', fill_value=0.0)\n",
    "    prior_t = prior_t.reindex(test_ids.tolist(), fill_value=0.0)\n",
    "    prior_t = prior_t.reindex(columns=top_terms, fill_value=0.0)\n",
    "    prior_test_np = prior_t.values.astype(np.float32)\n",
    "    X_test_stack = np.maximum(X_test_stack, EXTERNAL_PRIOR_WEIGHT * prior_test_np)\n",
    "    print(f'Injected external IEA prior into test stack (weight={EXTERNAL_PRIOR_WEIGHT}).')\n",
    "\n",
    "# 5. Ontology split (BP/MF/CC)\n",
    "ns_to_aspect = {\n",
    "    'molecular_function': 'MF',\n",
    "    'biological_process': 'BP',\n",
    "    'cellular_component': 'CC',\n",
    "}\n",
    "aspects = []\n",
    "for t in top_terms:\n",
    "    asp = ns_to_aspect.get(go_namespaces.get(t, ''), 'BP')\n",
    "    aspects.append(asp)\n",
    "aspects = np.array(aspects)\n",
    "aspect_to_idx = {\n",
    "    'BP': np.where(aspects == 'BP')[0].tolist(),\n",
    "    'MF': np.where(aspects == 'MF')[0].tolist(),\n",
    "    'CC': np.where(aspects == 'CC')[0].tolist(),\n",
    "}\n",
    "for k in ['BP', 'MF', 'CC']:\n",
    "    print(f'Terms[{k}]={len(aspect_to_idx[k])}')\n",
    "\n",
    "# 6. Train 3 specialised GCNs and stitch outputs back\n",
    "test_pred_gcn = np.zeros_like(X_test_stack, dtype=np.float32)\n",
    "X_tensor_full = torch.tensor(X_stack, dtype=torch.float32, device=device)\n",
    "Y_tensor_full = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "X_test_full = torch.tensor(X_test_stack, dtype=torch.float32, device=device)\n",
    "def train_one(aspect_name: str, idx_cols: list[int]):\n",
    "    if not idx_cols:\n",
    "        print(f'[{aspect_name}] No terms; skipping.')\n",
    "        return None\n",
    "    terms_sub = [top_terms[i] for i in idx_cols]\n",
    "    adj = build_adjacency(terms_sub, go_parents)\n",
    "    model = SimpleGCN(input_dim=len(idx_cols), hidden_dim=1024, output_dim=len(idx_cols), adj_matrix=adj).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.BCELoss()\n",
    "    X_t = X_tensor_full[:, idx_cols]\n",
    "    Y_t = Y_tensor_full[:, idx_cols]\n",
    "    n_samples = X_t.shape[0]\n",
    "    BS = 256\n",
    "    EPOCHS = 5\n",
    "    model.train()\n",
    "    print(f'\\n=== Training GCN[{aspect_name}] terms={len(idx_cols)} ===')\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        perm = torch.randperm(n_samples, device=device)\n",
    "        for i in range(0, n_samples, BS):\n",
    "            b = perm[i:i + BS]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X_t[b])\n",
    "            loss = criterion(out, Y_t[b])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += float(loss.item())\n",
    "        with torch.no_grad():\n",
    "            pred = (model(X_t[:2000]) > 0.3).float().cpu().numpy()\n",
    "            f1 = f1_score(Y_t[:2000].cpu().numpy(), pred, average='micro')\n",
    "        print(f'Epoch {epoch+1}/{EPOCHS} Loss={total_loss:.4f} micro-F1@0.30={f1:.4f}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784143e",
   "metadata": {
    "id": "0784143e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 16 - Solution: 6. PHASE 4: POST-PROCESSING & SUBMISSION\n",
    "# 6. PHASE 4: POST-PROCESSING & SUBMISSION\n",
    "# ========================================\n",
    "# HARDWARE: CPU / GPU\n",
    "# ========================================\n",
    "# This phase applies the \"Strict Post-Processing\" rules (Max/Min Propagation)\n",
    "# and generates the final submission file.\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if submission already exists\n",
    "if (WORK_ROOT / 'submission.tsv').exists():\n",
    "    print(\"submission.tsv already exists. Skipping Phase 4.\")\n",
    "else:\n",
    "    print(\"Starting Phase 4: Post-processing & submission...\")\n",
    "    # Ensure go_parents is available (from Phase 1)\n",
    "    if 'go_parents' not in locals() or 'go_namespaces' not in locals():\n",
    "        print(\"Reloading GO graph (parse_obo)...\")\n",
    "        def parse_obo(path: Path):\n",
    "            parents = {}\n",
    "            namespaces = {}\n",
    "            cur_id, cur_ns = None, None\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line == '[Term]':\n",
    "                        if cur_id and cur_ns:\n",
    "                            namespaces[cur_id] = cur_ns\n",
    "                        cur_id, cur_ns = None, None\n",
    "                    elif line.startswith('id: GO:'):\n",
    "                        cur_id = line.split('id: ', 1)[1]\n",
    "                    elif line.startswith('namespace:'):\n",
    "                        cur_ns = line.split('namespace: ', 1)[1]\n",
    "                    elif line.startswith('is_a:') and cur_id:\n",
    "                        parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                        parents.setdefault(cur_id, set()).add(parent)\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "            return parents, namespaces\n",
    "        go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "    # Load test IDs\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id']\n",
    "    # Load stacker predictions\n",
    "    pred_path = WORK_ROOT / 'features' / 'test_pred_gcn.npy'\n",
    "    if not pred_path.exists():\n",
    "        raise FileNotFoundError(\"Missing `test_pred_gcn.npy`. Run Phase 3 (GCN stacker) first.\")\n",
    "    preds = np.load(pred_path)\n",
    "    # Load term list (must match Phase 3)\n",
    "    terms_path = WORK_ROOT / 'features' / 'top_terms_1500.json'\n",
    "    if terms_path.exists():\n",
    "        with open(terms_path, 'r') as f:\n",
    "            top_terms = json.load(f)\n",
    "    else:\n",
    "        print(\"Warning: top_terms_1500.json missing; rebuilding from train_terms counts (may mismatch Phase 3).\")\n",
    "        train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "        top_terms = train_terms['term'].value_counts().head(preds.shape[1]).index.tolist()\n",
    "    if preds.shape[1] != len(top_terms):\n",
    "        raise ValueError(f\"Shape mismatch: preds has {preds.shape[1]} terms, top_terms has {len(top_terms)}.\")\n",
    "    # ------------------------------------------\n",
    "    # Strict post-processing (Max/Min Propagation)\n",
    "    # ------------------------------------------\n",
    "    print(f\"Applying hierarchy rules on {len(top_terms)} terms...\")\n",
    "    df_pred = pd.DataFrame(preds, columns=top_terms)\n",
    "    term_set = set(top_terms)\n",
    "    term_to_parents = {}\n",
    "    term_to_children = {}\n",
    "    for term in top_terms:\n",
    "        parents = go_parents.get(term, set())\n",
    "        if not parents:\n",
    "            continue\n",
    "        parents = parents.intersection(term_set)\n",
    "        if not parents:\n",
    "            continue\n",
    "        term_to_parents[term] = list(parents)\n",
    "        for p in parents:\n",
    "            term_to_children.setdefault(p, []).append(term)\n",
    "    # Max Propagation (Child -> Parent)\n",
    "    for _ in range(2):\n",
    "        for child, parents in term_to_parents.items():\n",
    "            child_scores = df_pred[child].values\n",
    "            for parent in parents:\n",
    "                df_pred[parent] = np.maximum(df_pred[parent].values, child_scores)\n",
    "    # Min Propagation (Parent -> Child)\n",
    "    for _ in range(2):\n",
    "        for parent, children in term_to_children.items():\n",
    "            parent_scores = df_pred[parent].values\n",
    "            for child in children:\n",
    "                df_pred[child] = np.minimum(df_pred[child].values, parent_scores)\n",
    "    # ------------------------------------------\n",
    "    # Submission formatting (CAFA rules)\n",
    "    # - tab-separated, no header\n",
    "    # - score in (0, 1.000]\n",
    "    # - up to 3 significant figures\n",
    "    # - <= 1500 terms per target (MF/BP/CC combined)\n",
    "    # ------------------------------------------\n",
    "    df_pred['EntryID'] = test_ids.values\n",
    "    submission = df_pred.melt(id_vars='EntryID', var_name='term', value_name='score')\n",
    "    # Enforce score range + remove zeros\n",
    "    submission['score'] = submission['score'].clip(lower=0.0, upper=1.0)\n",
    "    submission = submission[submission['score'] > 0.0]\n",
    "    # Light pruning (keeps file size sane; still rule-compliant)\n",
    "    submission = submission[submission['score'] >= 0.001]\n",
    "    # Keep top 1500 per protein (rule)\n",
    "    submission = submission.sort_values(['EntryID', 'score'], ascending=[True, False])\n",
    "    submission = submission.groupby('EntryID', sort=False).head(1500)\n",
    "    # Write with <= 3 significant figures\n",
    "    submission.to_csv(\n",
    "        WORK_ROOT / 'submission.tsv',\n",
    "        sep='\\t',\n",
    "        index=False,\n",
    "        header=False,\n",
    "        float_format='%.3g',\n",
    "    )\n",
    "    print(f\"Done! Submission saved to {WORK_ROOT / 'submission.tsv'}\")\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.push('stage_09_submission', [WORK_ROOT / 'submission.tsv'], note='final submission')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
