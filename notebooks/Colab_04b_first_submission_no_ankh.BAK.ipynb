{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ecde8",
   "metadata": {
    "id": "bootstrap",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 01 - Setup (No Repo Clone)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Set working directory to /content (Colab default) or current dir\n",
    "if os.path.exists('/content'):\n",
    "    os.chdir('/content')\n",
    "    SAFE_CWD = Path('/content')\n",
    "else:\n",
    "    SAFE_CWD = Path.cwd()\n",
    "\n",
    "print(f\"CWD: {Path.cwd()}\")\n",
    "\n",
    "# Install dependencies directly (skipping requirements.txt)\n",
    "# py-boost: GBDT model\n",
    "# obonet: GO ontology parsing\n",
    "# biopython: FASTA parsing (optional but good to have)\n",
    "# pyyaml: Config parsing (if needed)\n",
    "pkgs = ['py-boost', 'obonet', 'biopython', 'pyyaml']\n",
    "print(f\"Installing: {', '.join(pkgs)}...\")\n",
    "try:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', '-q', 'install'] + pkgs)\n",
    "    print(\"Dependencies installed.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Warning: Dependency installation failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94540c0a",
   "metadata": {
    "id": "install",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 02 - Environment Info\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def _detect_kaggle() -> bool:\n",
    "    return bool(os.environ.get('KAGGLE_KERNEL_RUN_TYPE') or os.environ.get('KAGGLE_URL_BASE') or os.environ.get('KAGGLE_DATA_PROXY_URL'))\n",
    "def _detect_colab() -> bool:\n",
    "    return bool(os.environ.get('COLAB_RELEASE_TAG') or os.environ.get('COLAB_GPU') or os.environ.get('COLAB_TPU_ADDR'))\n",
    "\n",
    "IS_KAGGLE = _detect_kaggle()\n",
    "IS_COLAB = (not IS_KAGGLE) and _detect_colab()\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print('Environment: Kaggle Detected')\n",
    "elif IS_COLAB:\n",
    "    print('Environment: Colab Detected')\n",
    "else:\n",
    "    print('Environment: Local Detected')\n",
    "\n",
    "# Dependencies are now installed in Cell 01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64609e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 03 - Secrets Setup (Colab/Kaggle/Local)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def _in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _get_colab_secret(name: str):\n",
    "    # Colab-only rule: secrets via userdata.get(...)\n",
    "    from google.colab import userdata  # type: ignore\n",
    "    return userdata.get(name)\n",
    "\n",
    "\n",
    "def get_secret(name: str, default=None):\n",
    "    if _in_colab():\n",
    "        v = _get_colab_secret(name)\n",
    "        return default if v is None else v\n",
    "    return os.environ.get(name, default)\n",
    "\n",
    "\n",
    "CAFA_CHECKPOINT_DATASET_ID = get_secret('CAFA_CHECKPOINT_DATASET_ID', '')\n",
    "CAFA_KAGGLE_USERNAME = get_secret('KAGGLE_USERNAME', '')\n",
    "CAFA_KAGGLE_KEY = get_secret('KAGGLE_KEY', '')\n",
    "\n",
    "print('CAFA_CHECKPOINT_DATASET_ID set:', bool(CAFA_CHECKPOINT_DATASET_ID))\n",
    "print('KAGGLE_USERNAME set:', bool(CAFA_KAGGLE_USERNAME))\n",
    "print('KAGGLE_KEY set:', bool(CAFA_KAGGLE_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1858dad8",
   "metadata": {
    "id": "1858dad8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 04 - Solution: 1. SETUP, CONFIG & DIAGNOSTICS\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Environment detection ---\n",
    "def in_colab():\n",
    "    try:\n",
    "        import google.colab  # type: ignore\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def in_kaggle():\n",
    "    return os.path.exists('/kaggle')\n",
    "\n",
    "# --- Paths ---\n",
    "if in_colab():\n",
    "    WORK_ROOT = Path('/content/work')\n",
    "elif in_kaggle():\n",
    "    WORK_ROOT = Path('/kaggle/working/work')\n",
    "else:\n",
    "    WORK_ROOT = Path('artefacts_local') / 'work'\n",
    "\n",
    "WORK_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print('WORK_ROOT:', WORK_ROOT)\n",
    "\n",
    "# ... (rest of cell unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8006bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 03b - Checkpoint artefacts: strict file-by-file hydration (must run AFTER Cell 03)\n",
    "# Dataset manifest (v28) says these paths exist; we pull them deterministically.\n",
    "# Fail-fast: if Kaggle returns 403/404/etc, we print stderr and stop.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "assert 'WORK_ROOT' in globals(), 'Run Cell 03 first (defines WORK_ROOT).'\n",
    "assert 'CHECKPOINT_DATASET_ID' in globals(), 'Run Cell 03 first (defines CHECKPOINT_DATASET_ID).'\n",
    "\n",
    "work_root = Path(WORK_ROOT)\n",
    "dataset_id = str(CHECKPOINT_DATASET_ID).strip()\n",
    "\n",
    "if not dataset_id:\n",
    "    raise RuntimeError('Missing CAFA_CHECKPOINT_DATASET_ID in Colab secrets.')\n",
    "\n",
    "# Kaggle CLI auth MUST be present in env for subprocess calls.\n",
    "if not os.environ.get('KAGGLE_USERNAME') or not os.environ.get('KAGGLE_KEY'):\n",
    "    raise RuntimeError(\n",
    "        'Missing Kaggle API auth. On Colab, set secrets: KAGGLE_USERNAME and KAGGLE_KEY.'\n",
    "    )\n",
    "\n",
    "required_paths = [\n",
    "    # external\n",
    "    'external/entryid_text.tsv',\n",
    "    'external/prop_test_no_kaggle.tsv',\n",
    "    'external/prop_train_no_kaggle.tsv',\n",
    "    # features\n",
    "    'features/test_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_t5.npy',\n",
    "    'features/test_embeds_text.npy',\n",
    "    'features/text_vectorizer.joblib',\n",
    "    'features/train_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy',\n",
    "    'features/train_embeds_t5.npy',\n",
    "    'features/train_embeds_text.npy',\n",
    "    # parsed\n",
    "    'parsed/test_seq.feather',\n",
    "    'parsed/test_taxa.feather',\n",
    "    'parsed/train_seq.feather',\n",
    "    'parsed/train_taxa.feather',\n",
    "    'parsed/term_counts.parquet',\n",
    "    'parsed/term_priors.parquet',\n",
    "    'parsed/train_terms.parquet',\n",
    "]\n",
    "\n",
    "\n",
    "def _exists_nonempty(p: Path) -> bool:\n",
    "    return p.exists() and p.is_file() and p.stat().st_size > 0\n",
    "\n",
    "\n",
    "def _normalise_download_location(expected_rel: str) -> None:\n",
    "    \"\"\"Kaggle sometimes unzips into unexpected locations; move into WORK_ROOT/<expected_rel>.\"\"\"\n",
    "    expected = work_root / expected_rel\n",
    "    if _exists_nonempty(expected):\n",
    "        return\n",
    "\n",
    "    basename = Path(expected_rel).name\n",
    "\n",
    "    # 1) Sometimes it lands at WORK_ROOT/<basename>\n",
    "    cand = work_root / basename\n",
    "    if _exists_nonempty(cand):\n",
    "        expected.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.move(str(cand), str(expected))\n",
    "        return\n",
    "\n",
    "    # 2) Sometimes we get nested features/features/... or external/external/...\n",
    "    nested = list(work_root.rglob(basename))\n",
    "    for p in nested:\n",
    "        if p == expected:\n",
    "            continue\n",
    "        if _exists_nonempty(p):\n",
    "            expected.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(str(p), str(expected))\n",
    "            return\n",
    "\n",
    "\n",
    "def _download_one(rel_path: str) -> None:\n",
    "    expected = work_root / rel_path\n",
    "    if _exists_nonempty(expected):\n",
    "        print(f\"  [SKIP] {rel_path} ({expected.stat().st_size / 1e6:.1f} MB)\")\n",
    "        return\n",
    "\n",
    "    expected.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"  [DOWN] {rel_path}\")\n",
    "\n",
    "    # Download into WORK_ROOT so that folder paths (features/, parsed/, external/) can be recreated on unzip.\n",
    "    p = subprocess.run(\n",
    "        ['kaggle', 'datasets', 'download', '-d', dataset_id, '-f', rel_path, '-p', str(work_root), '--unzip', '-o'],\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        env=os.environ.copy(),\n",
    "    )\n",
    "\n",
    "    if p.returncode != 0:\n",
    "        print('--- kaggle stdout ---')\n",
    "        print((p.stdout or '').strip()[:4000])\n",
    "        print('--- kaggle stderr ---')\n",
    "        print((p.stderr or '').strip()[:4000])\n",
    "        raise RuntimeError(f\"kaggle download failed for {rel_path} (returncode={p.returncode})\")\n",
    "\n",
    "    _normalise_download_location(rel_path)\n",
    "\n",
    "    if not _exists_nonempty(expected):\n",
    "        raise FileNotFoundError(f\"Downloaded {rel_path} but file is missing/empty at {expected}\")\n",
    "\n",
    "\n",
    "print(f\"=== Strict hydration from {dataset_id} ===\")\n",
    "print(f\"Target: {work_root}\")\n",
    "\n",
    "for rel in required_paths:\n",
    "    _download_one(rel)\n",
    "\n",
    "print(\"\\nAll required checkpoint artefacts present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b013607",
   "metadata": {
    "id": "3b013607",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# CELL 04 - Solution: 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# 2. PHASE 1: DATA STRUCTURING & HIERARCHY\n",
    "# ==========================================\n",
    "# HARDWARE: CPU (Standard)\n",
    "# ==========================================\n",
    "# ------------------------------------------\n",
    "# B. Parse OBO & Terms (needed in-memory downstream)\n",
    "# ------------------------------------------\n",
    "\n",
    "def parse_obo(path: Path):\n",
    "    parents = {}\n",
    "    namespaces = {}\n",
    "    cur_id, cur_ns = None, None\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == '[Term]':\n",
    "                if cur_id and cur_ns:\n",
    "                    namespaces[cur_id] = cur_ns\n",
    "                cur_id, cur_ns = None, None\n",
    "            elif line.startswith('id: GO:'):\n",
    "                cur_id = line.split('id: ', 1)[1]\n",
    "            elif line.startswith('namespace:'):\n",
    "                cur_ns = line.split('namespace: ', 1)[1]\n",
    "            elif line.startswith('is_a:') and cur_id:\n",
    "                parent = line.split('is_a: ', 1)[1].split(' ! ')[0]\n",
    "                parents.setdefault(cur_id, set()).add(parent)\n",
    "        if cur_id and cur_ns:\n",
    "            namespaces[cur_id] = cur_ns\n",
    "    return parents, namespaces\n",
    "print(\"Parsing OBO...\")\n",
    "go_parents, go_namespaces = parse_obo(PATH_GO_OBO)\n",
    "print(f\"GO Graph: {len(go_parents)} nodes with parents, {len(go_namespaces)} terms with namespace.\")\n",
    "# ------------------------------------------\n",
    "# Milestone checkpoint: stage_01_parsed\n",
    "# ------------------------------------------\n",
    "parsed_dir = WORK_ROOT / 'parsed'\n",
    "parsed_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_train_seq = parsed_dir / 'train_seq.feather'\n",
    "out_test_seq = parsed_dir / 'test_seq.feather'\n",
    "out_train_terms = parsed_dir / 'train_terms.parquet'\n",
    "out_term_counts = parsed_dir / 'term_counts.parquet'\n",
    "out_term_priors = parsed_dir / 'term_priors.parquet'\n",
    "out_train_taxa = parsed_dir / 'train_taxa.feather'\n",
    "out_test_taxa = parsed_dir / 'test_taxa.feather'\n",
    "expected = [out_train_seq, out_train_terms, out_term_counts, out_term_priors, out_train_taxa]\n",
    "if PATH_TEST_FASTA.exists():\n",
    "    expected += [out_test_seq, out_test_taxa]\n",
    "missing = [p for p in expected if not p.exists()]\n",
    "if not missing:\n",
    "    print(\"Parsed artefacts already exist; skipping Phase 1 writes.\")\n",
    "else:\n",
    "    # ------------------------------------------\n",
    "    # A. Parse FASTA to Feather\n",
    "    # ------------------------------------------\n",
    "    def parse_fasta(path: Path) -> pd.DataFrame:\n",
    "        ids, seqs = [], []\n",
    "        cur_id, cur_seq = None, []\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    if cur_id:\n",
    "                        ids.append(cur_id)\n",
    "                        seqs.append(''.join(cur_seq))\n",
    "                    cur_id = line[1:].split()[0]\n",
    "                    cur_seq = []\n",
    "                else:\n",
    "                    cur_seq.append(line)\n",
    "            if cur_id:\n",
    "                ids.append(cur_id)\n",
    "                seqs.append(''.join(cur_seq))\n",
    "        return pd.DataFrame({'id': ids, 'sequence': seqs})\n",
    "    print(\"Parsing FASTA...\")\n",
    "    parse_fasta(PATH_TRAIN_FASTA).to_feather(out_train_seq)\n",
    "    if PATH_TEST_FASTA.exists():\n",
    "        parse_fasta(PATH_TEST_FASTA).to_feather(out_test_seq)\n",
    "    print(\"FASTA parsed and saved to artefacts.\")\n",
    "    # ------------------------------------------\n",
    "    # C. Process Terms & Priors\n",
    "    # ------------------------------------------\n",
    "    terms = pd.read_csv(PATH_TRAIN_TERMS, sep='\\t')\n",
    "    col_term = terms.columns[1]\n",
    "    terms['aspect'] = terms[col_term].map(lambda x: go_namespaces.get(x, 'UNK'))\n",
    "    # Plot Aspects\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    terms['aspect'].value_counts().plot(kind='bar', title='Annotations by Namespace')\n",
    "    plt.show()\n",
    "    # Save Priors\n",
    "    priors = (terms[col_term].value_counts() / terms.iloc[:, 0].nunique()).reset_index()\n",
    "    priors.columns = ['term', 'prior']\n",
    "    if PATH_IA.exists():\n",
    "        ia = pd.read_csv(PATH_IA, sep='\\t', names=['term', 'ia'])\n",
    "        priors = priors.merge(ia, on='term', how='left').fillna(0)\n",
    "    priors.to_parquet(out_term_priors)\n",
    "    print(\"Terms processed and priors saved.\")\n",
    "    # ------------------------------------------\n",
    "    # D. Process Taxonomy\n",
    "    # ------------------------------------------\n",
    "    print(\"Processing Taxonomy...\")\n",
    "    # Train Taxonomy\n",
    "    tax_train = pd.read_csv(PATH_TRAIN_TAXON, sep='\\t', header=None, names=['id', 'taxon_id'])\n",
    "    tax_train['taxon_id'] = tax_train['taxon_id'].astype(int)\n",
    "    tax_train.to_feather(out_train_taxa)\n",
    "    # Test Taxonomy (Extract from FASTA headers)\n",
    "    if PATH_TEST_FASTA.exists():\n",
    "        ids, taxons = [], []\n",
    "        with PATH_TEST_FASTA.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    parts = line[1:].split()\n",
    "                    ids.append(parts[0])\n",
    "                    # Assume second part is taxon if present\n",
    "                    if len(parts) > 1:\n",
    "                        try:\n",
    "                            taxons.append(int(parts[1]))\n",
    "                        except ValueError:\n",
    "                            taxons.append(0)\n",
    "                    else:\n",
    "                        taxons.append(0)\n",
    "        tax_test = pd.DataFrame({'id': ids, 'taxon_id': taxons})\n",
    "        tax_test.to_feather(out_test_taxa)\n",
    "        print(f\"Taxonomy processed. Train: {len(tax_train)}, Test: {len(tax_test)}\")\n",
    "    else:\n",
    "        print(f\"Taxonomy processed. Train: {len(tax_train)}\")\n",
    "    # ------------------------------------------\n",
    "    # E. Save Targets & Term List\n",
    "    # ------------------------------------------\n",
    "    print(\"Saving Targets & Term List...\")\n",
    "    # Save full terms list (long format)\n",
    "    terms.to_parquet(out_train_terms)\n",
    "    # Save unique term list with counts\n",
    "    term_counts = terms['term'].value_counts().reset_index()\n",
    "    term_counts.columns = ['term', 'count']\n",
    "    term_counts.to_parquet(out_term_counts)\n",
    "    print(\"Targets saved.\")\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.push('stage_01_parsed', [p for p in expected if p.exists()], note='parsed FASTA/taxa/terms/priors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea612e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10b - Diagnostics: artefact manifest (existence + sizes)\n",
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "def _mb(p: Path) -> float:\n",
    "    return p.stat().st_size / (1024**2)\n",
    "\n",
    "WORK_ROOT = Path(WORK_ROOT)\n",
    "\n",
    "# Helper to find file with optional .gz extension\n",
    "def resolve_path(base_path):\n",
    "    p = Path(base_path)\n",
    "    if p.exists(): return p\n",
    "    # Try removing .gz\n",
    "    if p.suffix == '.gz' and p.with_suffix('').exists():\n",
    "        return p.with_suffix('')\n",
    "    # Try adding .gz\n",
    "    if not p.suffix == '.gz' and p.with_suffix(p.suffix + '.gz').exists():\n",
    "        return p.with_suffix(p.suffix + '.gz')\n",
    "    return p\n",
    "\n",
    "# Minimal contract for *this* notebook (first submission, no Ankh):\n",
    "# - parsed/*\n",
    "# - core embeddings (t5, esm2, esm2_3b, text)\n",
    "# - taxonomy\n",
    "# - external priors (if you want them in the stacker)\n",
    "paths = {\n",
    "    # Phase 1 parsed\n",
    "    'parsed/train_seq.feather': WORK_ROOT / 'parsed' / 'train_seq.feather',\n",
    "    'parsed/test_seq.feather': WORK_ROOT / 'parsed' / 'test_seq.feather',\n",
    "    'parsed/train_terms.parquet': WORK_ROOT / 'parsed' / 'train_terms.parquet',\n",
    "    'parsed/term_priors.parquet': WORK_ROOT / 'parsed' / 'term_priors.parquet',\n",
    "    'parsed/train_taxa.feather': WORK_ROOT / 'parsed' / 'train_taxa.feather',\n",
    "    'parsed/test_taxa.feather': WORK_ROOT / 'parsed' / 'test_taxa.feather',\n",
    "    # Text pipeline\n",
    "    'external/entryid_text.tsv': WORK_ROOT / 'external' / 'entryid_text.tsv',\n",
    "    'features/text_vectorizer.joblib': WORK_ROOT / 'features' / 'text_vectorizer.joblib',\n",
    "    'features/train_embeds_text.npy': WORK_ROOT / 'features' / 'train_embeds_text.npy',\n",
    "    'features/test_embeds_text.npy': WORK_ROOT / 'features' / 'test_embeds_text.npy',\n",
    "    # Sequence embeddings (core)\n",
    "    'features/train_embeds_t5.npy': WORK_ROOT / 'features' / 'train_embeds_t5.npy',\n",
    "    'features/test_embeds_t5.npy': WORK_ROOT / 'features' / 'test_embeds_t5.npy',\n",
    "    'features/train_embeds_esm2.npy': WORK_ROOT / 'features' / 'train_embeds_esm2.npy',\n",
    "    'features/test_embeds_esm2.npy': WORK_ROOT / 'features' / 'test_embeds_esm2.npy',\n",
    "    'features/train_embeds_esm2_3b.npy': WORK_ROOT / 'features' / 'train_embeds_esm2_3b.npy',\n",
    "    'features/test_embeds_esm2_3b.npy': WORK_ROOT / 'features' / 'test_embeds_esm2_3b.npy',\n",
    "    # External priors (optional but used if present)\n",
    "    'external/prop_train_no_kaggle.tsv': resolve_path(WORK_ROOT / 'external' / 'prop_train_no_kaggle.tsv.gz'),\n",
    "    'external/prop_test_no_kaggle.tsv': resolve_path(WORK_ROOT / 'external' / 'prop_test_no_kaggle.tsv.gz'),\n",
    "    # Downstream expectations\n",
    "    'features/top_terms_1500.json': WORK_ROOT / 'features' / 'top_terms_1500.json',\n",
    "    'features/oof_pred_logreg.npy': WORK_ROOT / 'features' / 'oof_pred_logreg.npy',\n",
    "    'features/oof_pred_gbdt.npy': WORK_ROOT / 'features' / 'oof_pred_gbdt.npy',\n",
    "    'features/oof_pred_dnn.npy': WORK_ROOT / 'features' / 'oof_pred_dnn.npy',\n",
    "    'features/oof_pred_knn.npy': WORK_ROOT / 'features' / 'oof_pred_knn.npy',\n",
    "    'features/test_pred_logreg.npy': WORK_ROOT / 'features' / 'test_pred_logreg.npy',\n",
    "    'features/test_pred_gbdt.npy': WORK_ROOT / 'features' / 'test_pred_gbdt.npy',\n",
    "    'features/test_pred_dnn.npy': WORK_ROOT / 'features' / 'test_pred_dnn.npy',\n",
    "    'features/test_pred_knn.npy': WORK_ROOT / 'features' / 'test_pred_knn.npy',\n",
    "    'features/test_pred_gcn.npy': WORK_ROOT / 'features' / 'test_pred_gcn.npy',\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, p in paths.items():\n",
    "    rows.append({'artefact': name, 'exists': p.exists(), 'mb': _mb(p) if p.exists() else 0.0, 'path': str(p)})\n",
    "df = pd.DataFrame(rows).sort_values(['exists', 'mb'], ascending=[True, False])\n",
    "print('WORK_ROOT:', WORK_ROOT)\n",
    "display(df)\n",
    "\n",
    "# Visual: top 25 largest artefacts\n",
    "df2 = df[df['exists']].sort_values('mb', ascending=False).head(25)\n",
    "if len(df2) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=df2, y='artefact', x='mb')\n",
    "    plt.title('Largest artefacts (MB)')\n",
    "    plt.xlabel('MB')\n",
    "    plt.ylabel('artefact')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee2ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8a - Setup & Data Loading\n",
    "# =============================================\n",
    "# 4. PHASE 2: LEVEL-1 MODELS (DIVERSE ENSEMBLE)\n",
    "# =============================================\n",
    "TRAIN_LEVEL1 = True\n",
    "if TRAIN_LEVEL1:\n",
    "    import joblib\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import gc\n",
    "    from pathlib import Path\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import f1_score\n",
    "    import psutil\n",
    "\n",
    "    # AUDITOR: Hardware Check\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"[AUDITOR] GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"[AUDITOR] VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        else:\n",
    "            print(\"[AUDITOR] WARNING: No GPU detected. RAPIDS will fail.\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    def log_mem(tag=\"\"):\n",
    "        try:\n",
    "            mem = psutil.virtual_memory()\n",
    "            print(f\"[MEM] {tag:<30} | Used: {mem.used/1e9:.2f}GB / {mem.total/1e9:.2f}GB ({mem.percent}%)\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if 'WORK_ROOT' not in locals() and 'WORK_ROOT' not in globals():\n",
    "        if os.path.exists('/content/work'):\n",
    "            WORK_ROOT = Path('/content/work')\n",
    "        elif os.path.exists('/kaggle/working/work'):\n",
    "            WORK_ROOT = Path('/kaggle/working/work')\n",
    "        else:\n",
    "            WORK_ROOT = Path.cwd() / 'artefacts_local' / 'work'\n",
    "        print(f\"WORK_ROOT recovered: {WORK_ROOT}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load targets + ids\n",
    "    # -----------------------------\n",
    "    print(\"Loading targets...\")\n",
    "    train_terms = pd.read_parquet(WORK_ROOT / 'parsed' / 'train_terms.parquet')\n",
    "    train_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'train_seq.feather')['id'].astype(str)\n",
    "    test_ids = pd.read_feather(WORK_ROOT / 'parsed' / 'test_seq.feather')['id'].astype(str)\n",
    "    \n",
    "    # FIX: Clean IDs in train_ids to match EntryID format\n",
    "    print(\"Applying ID cleaning fix...\")\n",
    "    train_ids_clean = train_ids.str.extract(r'\\|(.*?)\\|')[0]\n",
    "    train_ids_clean = train_ids_clean.fillna(train_ids)\n",
    "    \n",
    "    # Target Matrix Construction (Champion Strategy: 13,500 Terms)\n",
    "    print(\"Selecting Top-K terms per aspect (Champion Strategy)...\")\n",
    "    \n",
    "    try:\n",
    "        import obonet\n",
    "        # Robust OBO Path Search\n",
    "        possible_paths = [\n",
    "            WORK_ROOT / 'go-basic.obo',\n",
    "            WORK_ROOT.parent / 'go-basic.obo',\n",
    "            Path('go-basic.obo'),\n",
    "            Path('Train/go-basic.obo'),\n",
    "            Path('../Train/go-basic.obo'),\n",
    "            Path('/content/cafa6_data/Train/go-basic.obo')\n",
    "        ]\n",
    "        \n",
    "        obo_path = None\n",
    "        for p in possible_paths:\n",
    "            if p.exists():\n",
    "                obo_path = p\n",
    "                break\n",
    "        \n",
    "        if obo_path is None:\n",
    "            raise FileNotFoundError(f\"CRITICAL: go-basic.obo not found. Searched: {[str(p) for p in possible_paths]}. Please upload it.\")\n",
    "\n",
    "        # Export for other cells\n",
    "        global PATH_GO_OBO\n",
    "        PATH_GO_OBO = obo_path\n",
    "        print(f\"Global PATH_GO_OBO set to: {PATH_GO_OBO}\")\n",
    "\n",
    "        print(f\"Loading OBO from {obo_path}...\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        term_to_ns = {node: data.get('namespace', 'unknown') for node, data in graph.nodes(data=True)}\n",
    "        ns_map = {'biological_process': 'BP', 'molecular_function': 'MF', 'cellular_component': 'CC'}\n",
    "        if 'aspect' not in train_terms.columns:\n",
    "            train_terms['aspect'] = train_terms['term'].map(lambda t: ns_map.get(term_to_ns.get(t), 'UNK'))\n",
    "            \n",
    "    except ImportError:\n",
    "        raise RuntimeError(\"obonet not installed. Please install it.\")\n",
    "\n",
    "    term_counts = train_terms.groupby(['aspect', 'term']).size().reset_index(name='count')\n",
    "    targets_bp = term_counts[term_counts['aspect'] == 'BP'].nlargest(10000, 'count')['term'].tolist()\n",
    "    targets_mf = term_counts[term_counts['aspect'] == 'MF'].nlargest(2000, 'count')['term'].tolist()\n",
    "    targets_cc = term_counts[term_counts['aspect'] == 'CC'].nlargest(1500, 'count')['term'].tolist()\n",
    "    \n",
    "    if len(targets_bp) == 0 and len(targets_mf) == 0:\n",
    "        # This should not happen if OBO is loaded correctly\n",
    "        print(\"  [WARNING] No aspect split found despite OBO load. Using global Top-13,500.\")\n",
    "        top_terms = train_terms['term'].value_counts().head(13500).index.tolist()\n",
    "    else:\n",
    "        top_terms = list(set(targets_bp + targets_mf + targets_cc))\n",
    "        print(f\"  Selected: {len(targets_bp)} BP + {len(targets_mf)} MF + {len(targets_cc)} CC\")\n",
    "\n",
    "    train_terms_top = train_terms[train_terms['term'].isin(top_terms)]\n",
    "    Y_df = train_terms_top.pivot_table(index='EntryID', columns='term', aggfunc='size', fill_value=0)\n",
    "    Y_df = Y_df.reindex(train_ids_clean, fill_value=0)\n",
    "    Y = Y_df.values.astype(np.float32)\n",
    "    print(f\"Targets: Y={Y.shape}\")\n",
    "    \n",
    "    # -----------------------------\n",
    "    # Feature loading helper (Memory Optimized)\n",
    "    # -----------------------------\n",
    "    FEAT_DIR = WORK_ROOT / 'features'\n",
    "    \n",
    "    def load_features_dict(split='both'):\n",
    "        log_mem(f\"Start load_features_dict({split})\")\n",
    "        print(f\"Loading multimodal features (mode={split})...\")\n",
    "        def _load_pair(stem):\n",
    "            tr = FEAT_DIR / f'train_embeds_{stem}.npy'\n",
    "            te = FEAT_DIR / f'test_embeds_{stem}.npy'\n",
    "            return tr, te\n",
    "        \n",
    "        ft_train = {}\n",
    "        ft_test = {}\n",
    "        # OPTIMIZATION: Use mmap_mode='r' to avoid loading full files into RAM\n",
    "        for stem, key in [('t5', 't5'), ('esm2', 'esm2_650m'), ('esm2_3b', 'esm2_3b'), ('text', 'text')]:\n",
    "            tr_path, te_path = _load_pair(stem)\n",
    "            if split in ['both', 'train'] and tr_path.exists():\n",
    "                # mmap_mode='r' keeps it on disk, pages in as needed\n",
    "                arr = np.load(tr_path, mmap_mode='r')\n",
    "                ft_train[key] = arr\n",
    "            if split in ['both', 'test'] and te_path.exists():\n",
    "                arr = np.load(te_path, mmap_mode='r')\n",
    "                ft_test[key] = arr\n",
    "        \n",
    "        taxa_train_path = WORK_ROOT / 'parsed' / 'train_taxa.feather'\n",
    "        taxa_test_path = WORK_ROOT / 'parsed' / 'test_taxa.feather'\n",
    "        if taxa_train_path.exists() and taxa_test_path.exists():\n",
    "            from sklearn.preprocessing import OneHotEncoder\n",
    "            tax_tr = pd.read_feather(taxa_train_path).astype({'id': str})\n",
    "            tax_te = pd.read_feather(taxa_test_path).astype({'id': str})\n",
    "            enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32)\n",
    "            enc.fit(pd.concat([tax_tr[['taxon_id']], tax_te[['taxon_id']]], axis=0))\n",
    "            \n",
    "            if split in ['both', 'train']:\n",
    "                tax_tr = tax_tr.set_index('id').reindex(train_ids, fill_value=0).reset_index()\n",
    "                ft_train['taxa'] = enc.transform(tax_tr[['taxon_id']]).astype(np.float32)\n",
    "            if split in ['both', 'test']:\n",
    "                tax_te = tax_te.set_index('id').reindex(test_ids, fill_value=0).reset_index()\n",
    "                ft_test['taxa'] = enc.transform(tax_te[['taxon_id']]).astype(np.float32)\n",
    "        \n",
    "        log_mem(f\"End load_features_dict({split})\")\n",
    "        if split == 'train': return ft_train\n",
    "        if split == 'test': return ft_test\n",
    "        return ft_train, ft_test\n",
    "\n",
    "    # -----------------------------\n",
    "    # IA-weighted F1 Helper\n",
    "    # -----------------------------\n",
    "    if 'ia' in locals(): ia_df = ia[['term', 'ia']].copy()\n",
    "    elif (WORK_ROOT.parent / 'IA.tsv').exists(): ia_df = pd.read_csv(WORK_ROOT.parent / 'IA.tsv', sep='\\t', names=['term', 'ia'])\n",
    "    elif (WORK_ROOT / 'IA.tsv').exists(): ia_df = pd.read_csv(WORK_ROOT / 'IA.tsv', sep='\\t', names=['term', 'ia'])\n",
    "    else: ia_df = pd.DataFrame({'term': [], 'ia': []})\n",
    "\n",
    "    ia_map = dict(zip(ia_df['term'], ia_df['ia']))\n",
    "    weights = np.array([ia_map.get(t, 0.0) for t in top_terms], dtype=np.float32)\n",
    "    \n",
    "    if 'go_namespaces' not in locals() and 'go_namespaces' not in globals():\n",
    "         term_aspects = np.array(['UNK'] * len(top_terms))\n",
    "    else:\n",
    "        ns_to_aspect = {'molecular_function': 'MF', 'biological_process': 'BP', 'cellular_component': 'CC'}\n",
    "        term_aspects = np.array([ns_to_aspect.get(go_namespaces.get(t, ''), 'UNK') for t in top_terms])\n",
    "\n",
    "    def ia_weighted_f1(y_true, y_score, thr=0.3):\n",
    "        y_true = (y_true > 0).astype(np.int8)\n",
    "        y_pred = (y_score >= thr).astype(np.int8)\n",
    "        tp = (y_pred & y_true).sum(axis=0).astype(np.float64)\n",
    "        pred = y_pred.sum(axis=0).astype(np.float64)\n",
    "        true = y_true.sum(axis=0).astype(np.float64)\n",
    "        def _score(mask=None):\n",
    "            w = weights if mask is None else (weights * mask)\n",
    "            w_tp = float((w * tp).sum())\n",
    "            w_pred = float((w * pred).sum())\n",
    "            w_true = float((w * true).sum())\n",
    "            p = (w_tp / w_pred) if w_pred > 0 else 0.0\n",
    "            r = (w_tp / w_true) if w_true > 0 else 0.0\n",
    "            return (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "        out = {'ALL': _score(None)}\n",
    "        for asp in ['MF', 'BP', 'CC']:\n",
    "            mask = (term_aspects == asp).astype(np.float32)\n",
    "            out[asp] = _score(mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ccd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8b - Phase 2a: Logistic Regression (Streaming, RAM-safe)\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import torch\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import joblib\n",
    "\n",
    "\n",
    "def log_mem(msg: str = \"\"):\n",
    "    try:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        ram_gb = process.memory_info().rss / (1024**3)\n",
    "        gpu_msg = \"\"\n",
    "        if torch.cuda.is_available():\n",
    "            alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "            res = torch.cuda.memory_reserved() / (1024**3)\n",
    "            gpu_msg = f\" | GPU Alloc: {alloc:.2f}GB Res: {res:.2f}GB\"\n",
    "        print(f\"[MEM] {msg:<25} | RAM: {ram_gb:.2f}GB{gpu_msg}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def _iter_chunks(idxs: np.ndarray, chunk_rows: int):\n",
    "    for i in range(0, len(idxs), chunk_rows):\n",
    "        yield idxs[i : i + chunk_rows]\n",
    "\n",
    "\n",
    "if TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 2a: Classical Models (LR/GBDT) ===\")\n",
    "    log_mem(\"Start Phase 2a\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1) Create Train X (Streaming / Memory-Mapped)\n",
    "    # ---------------------------------------------------------\n",
    "    X_path = WORK_ROOT / 'features' / 'X_train_mmap.npy'\n",
    "\n",
    "    sources = [\n",
    "        ('t5', 't5', True),\n",
    "        ('esm2', 'esm2_650m', True),\n",
    "        ('esm2_3b', 'esm2_3b', True),\n",
    "        ('text', 'text', True),\n",
    "        ('taxa', 'taxa', False),\n",
    "    ]\n",
    "\n",
    "    n_samples = len(train_ids)\n",
    "    total_features = 0\n",
    "    feature_dims: dict[str, int] = {}\n",
    "\n",
    "    print(\"Scanning feature dimensions...\")\n",
    "    for stem, key, is_emb in sources:\n",
    "        if is_emb:\n",
    "            p = WORK_ROOT / 'features' / f'train_embeds_{stem}.npy'\n",
    "            if p.exists():\n",
    "                shape = np.load(p, mmap_mode='r').shape\n",
    "                feature_dims[key] = int(shape[1])\n",
    "                total_features += int(shape[1])\n",
    "                print(f\"  {key}: {shape} (Found)\")\n",
    "            else:\n",
    "                print(f\"  {key}: [MISSING] {p}\")\n",
    "\n",
    "    taxa_train_path = WORK_ROOT / 'parsed' / 'train_taxa.feather'\n",
    "    taxa_test_path = WORK_ROOT / 'parsed' / 'test_taxa.feather'\n",
    "    taxa_enc = None\n",
    "\n",
    "    if taxa_train_path.exists() and taxa_test_path.exists():\n",
    "        print(\"  Computing Taxa dimensions...\")\n",
    "        tax_tr = pd.read_feather(taxa_train_path).astype({'id': str})\n",
    "        tax_te = pd.read_feather(taxa_test_path).astype({'id': str})\n",
    "        taxa_enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False, dtype=np.float32)\n",
    "        taxa_enc.fit(pd.concat([tax_tr[['taxon_id']], tax_te[['taxon_id']]], axis=0))\n",
    "        dim_taxa = len(taxa_enc.categories_[0])\n",
    "        feature_dims['taxa'] = int(dim_taxa)\n",
    "        total_features += int(dim_taxa)\n",
    "        print(f\"  taxa: {dim_taxa} cols\")\n",
    "        del tax_tr, tax_te\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"Total X shape: ({n_samples}, {total_features})\")\n",
    "\n",
    "    # Create / overwrite mmap\n",
    "    X_mmap = np.lib.format.open_memmap(X_path, mode='w+', dtype=np.float32, shape=(n_samples, total_features))\n",
    "\n",
    "    current_col = 0\n",
    "    for stem, key, is_emb in sources:\n",
    "        if key not in feature_dims:\n",
    "            continue\n",
    "\n",
    "        dim = feature_dims[key]\n",
    "        print(f\"Streaming {key} into columns {current_col}:{current_col+dim}...\")\n",
    "\n",
    "        if is_emb:\n",
    "            p = WORK_ROOT / 'features' / f'train_embeds_{stem}.npy'\n",
    "            arr = np.load(p, mmap_mode='r')\n",
    "            chunk_size = 10_000\n",
    "            for i in range(0, n_samples, chunk_size):\n",
    "                end = min(i + chunk_size, n_samples)\n",
    "                X_mmap[i:end, current_col:current_col+dim] = arr[i:end]\n",
    "            del arr\n",
    "\n",
    "        elif key == 'taxa' and taxa_enc is not None:\n",
    "            tax_tr = pd.read_feather(taxa_train_path).astype({'id': str})\n",
    "            tax_tr = tax_tr.set_index('id').reindex(train_ids, fill_value=0).reset_index()\n",
    "            chunk_size = 10_000\n",
    "            for i in range(0, n_samples, chunk_size):\n",
    "                end = min(i + chunk_size, n_samples)\n",
    "                batch = tax_tr.iloc[i:end][['taxon_id']]\n",
    "                X_mmap[i:end, current_col:current_col+dim] = taxa_enc.transform(batch)\n",
    "            del tax_tr\n",
    "\n",
    "        current_col += dim\n",
    "        gc.collect()\n",
    "        log_mem(f\"Written {key}\")\n",
    "\n",
    "    X_mmap.flush()\n",
    "    del X_mmap\n",
    "    gc.collect()\n",
    "    log_mem(\"Finished creating X_mmap\")\n",
    "\n",
    "    X = np.load(X_path, mmap_mode='r')\n",
    "    print(f\"Loaded X from mmap: {X.shape}\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # A) Logistic Regression (Streaming SGD OVR)\n",
    "    # ------------------------------------------\n",
    "    print(\"\\n--- Training Logistic Regression (Streaming SGD) ---\")\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds_logreg = np.zeros(Y.shape, dtype=np.float32)\n",
    "\n",
    "    CHUNK_ROWS = 2000  # keeps RAM bounded\n",
    "\n",
    "    for fold, (idx_tr, idx_val) in enumerate(kf.split(np.arange(X.shape[0]))):\n",
    "        print(f\"LogReg Fold {fold+1}/5\")\n",
    "        log_mem(f\"Start Fold {fold+1}\")\n",
    "\n",
    "        # 1) Fit scaler in a streaming pass over training rows\n",
    "        scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        for ch in _iter_chunks(idx_tr, CHUNK_ROWS):\n",
    "            Xch = np.asarray(X[ch], dtype=np.float32)\n",
    "            scaler.partial_fit(Xch)\n",
    "\n",
    "        mean = scaler.mean_.astype(np.float32)\n",
    "        scale = scaler.scale_.astype(np.float32)\n",
    "        log_mem(\"Scaler fitted\")\n",
    "\n",
    "        # 2) Train streaming OVR-SGD (no full X_tr materialisation)\n",
    "        clf_logreg = OneVsRestClassifier(\n",
    "            SGDClassifier(\n",
    "                loss='log_loss',\n",
    "                penalty='l2',\n",
    "                alpha=1e-4,\n",
    "                max_iter=1,  # we do our own epoching via chunk passes\n",
    "                tol=None,\n",
    "                n_jobs=4,\n",
    "                random_state=42,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        first = True\n",
    "        for ch in _iter_chunks(idx_tr, CHUNK_ROWS):\n",
    "            Xch = np.asarray(X[ch], dtype=np.float32)\n",
    "            Xch = (Xch - mean) / scale\n",
    "            Ych = Y[ch]\n",
    "            if first:\n",
    "                clf_logreg.partial_fit(Xch, Ych, classes=np.array([0, 1], dtype=np.int64))\n",
    "                first = False\n",
    "            else:\n",
    "                clf_logreg.partial_fit(Xch, Ych)\n",
    "\n",
    "        log_mem(\"Trained\")\n",
    "\n",
    "        # 3) Predict validation probabilities in chunks (stable RAM)\n",
    "        n_targets = Y.shape[1]\n",
    "        val_probs = np.zeros((len(idx_val), n_targets), dtype=np.float32)\n",
    "\n",
    "        off = 0\n",
    "        for ch in _iter_chunks(idx_val, CHUNK_ROWS):\n",
    "            Xch = np.asarray(X[ch], dtype=np.float32)\n",
    "            Xch = (Xch - mean) / scale\n",
    "            p = clf_logreg.predict_proba(Xch).astype(np.float32)\n",
    "            val_probs[off:off+len(ch)] = p\n",
    "            off += len(ch)\n",
    "\n",
    "        oof_preds_logreg[idx_val] = val_probs\n",
    "\n",
    "        # Diagnostics (cheap)\n",
    "        Y_val = Y[idx_val]\n",
    "        best_f1, best_thr = 0.0, 0.0\n",
    "        for thr in np.linspace(0.01, 0.20, 20):\n",
    "            vp = (val_probs > thr).astype(int)\n",
    "            score = f1_score(Y_val, vp, average='micro')\n",
    "            if score > best_f1:\n",
    "                best_f1, best_thr = score, float(thr)\n",
    "\n",
    "        ia_f1 = ia_weighted_f1(Y_val, val_probs, thr=best_thr)\n",
    "        print(f\"  >> Fold {fold+1} Best F1: {best_f1:.4f} at Thr {best_thr:.2f}\")\n",
    "        print(f\"  >> Fold {fold+1} IA-F1: ALL={ia_f1['ALL']:.4f}\")\n",
    "\n",
    "        joblib.dump({'mean': mean, 'scale': scale}, WORK_ROOT / 'features' / f'level1_logreg_scaler_fold{fold}.pkl')\n",
    "        joblib.dump(clf_logreg, WORK_ROOT / 'features' / f'level1_logreg_fold{fold}.pkl')\n",
    "\n",
    "        del clf_logreg, val_probs, Y_val, scaler\n",
    "        gc.collect()\n",
    "        log_mem(f\"End Fold {fold+1}\")\n",
    "\n",
    "    np.save(WORK_ROOT / 'features' / 'oof_pred_logreg.npy', oof_preds_logreg)\n",
    "    print(\"LogReg OOF saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef412cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8c - Phase 2a: Py-Boost (GBDT)\n",
    "import psutil\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "if 'log_mem' not in globals():\n",
    "    def log_mem(msg=\"\"):\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            ram_gb = process.memory_info().rss / (1024**3)\n",
    "            gpu_msg = \"\"\n",
    "            if torch.cuda.is_available():\n",
    "                alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "                res = torch.cuda.memory_reserved() / (1024**3)\n",
    "                gpu_msg = f\" | GPU Alloc: {alloc:.2f}GB Res: {res:.2f}GB\"\n",
    "            print(f\"[MEM] {msg:<25} | RAM: {ram_gb:.2f}GB{gpu_msg}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "def materialise_rows_to_memmap(src, idx, out_path, dtype=np.float32, chunk_rows=20000):\n",
    "    \"\"\"Write src[idx] to a .npy memmap without materialising the full slice in RAM.\"\"\"\n",
    "    out_path = str(out_path)\n",
    "    n_rows = len(idx)\n",
    "    n_cols = src.shape[1]\n",
    "    mm = np.lib.format.open_memmap(out_path, mode='w+', dtype=dtype, shape=(n_rows, n_cols))\n",
    "    for start in range(0, n_rows, chunk_rows):\n",
    "        end = min(start + chunk_rows, n_rows)\n",
    "        rows = idx[start:end]\n",
    "        mm[start:end] = np.asarray(src[rows], dtype=dtype)\n",
    "    mm.flush()\n",
    "    return mm\n",
    "\n",
    "\n",
    "if TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 2a: Py-Boost (GBDT) ===\")\n",
    "    log_mem(\"Start GBDT\")\n",
    "\n",
    "    # Ensure X is available\n",
    "    if 'X' not in locals():\n",
    "        X_path = WORK_ROOT / 'features' / 'X_train_mmap.npy'\n",
    "        if X_path.exists():\n",
    "            X = np.load(X_path, mmap_mode='r')\n",
    "            print(f\"Reloaded X from mmap: {X.shape}\")\n",
    "            log_mem(\"Reloaded X mmap\")\n",
    "        else:\n",
    "            raise RuntimeError(\"X mmap not found. Run Cell 8b first.\")\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # B. Py-Boost (GBDT)\n",
    "    # ------------------------------------------\n",
    "    try:\n",
    "        from py_boost import GradientBoosting\n",
    "        HAS_PYBOOST = True\n",
    "    except ImportError as e:\n",
    "        raise RuntimeError(\"CRITICAL: py_boost is missing. Install with 'pip install py-boost'.\") from e\n",
    "\n",
    "    if HAS_PYBOOST:\n",
    "        print(\"\\n--- Training Py-Boost GBDT ---\")\n",
    "\n",
    "        # Disk-backed OOF to avoid holding (n_samples x n_labels) in RAM\n",
    "        oof_path = WORK_ROOT / 'features' / 'oof_pred_gbdt.npy'\n",
    "        oof_preds_gbdt = np.lib.format.open_memmap(str(oof_path), mode='w+', dtype=np.float32, shape=Y.shape)\n",
    "\n",
    "        tmp_dir = WORK_ROOT / 'features' / 'tmp_folds_gbdt'\n",
    "        tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf.split(X)):\n",
    "            print(f\"GBDT Fold {fold+1}/5\")\n",
    "            log_mem(f\"Start Fold {fold+1}\")\n",
    "\n",
    "            # Materialise fold slices to disk memmaps in chunks (RAM-safe)\n",
    "            log_mem(\"Materialise fold\")\n",
    "            X_tr_path = tmp_dir / f'X_tr_fold{fold}.npy'\n",
    "            X_val_path = tmp_dir / f'X_val_fold{fold}.npy'\n",
    "            Y_tr_path = tmp_dir / f'Y_tr_fold{fold}.npy'\n",
    "            Y_val_path = tmp_dir / f'Y_val_fold{fold}.npy'\n",
    "\n",
    "            X_tr = materialise_rows_to_memmap(X, idx_tr, X_tr_path, dtype=np.float32)\n",
    "            X_val = materialise_rows_to_memmap(X, idx_val, X_val_path, dtype=np.float32)\n",
    "            Y_tr = materialise_rows_to_memmap(Y, idx_tr, Y_tr_path, dtype=np.float32)\n",
    "            Y_val = materialise_rows_to_memmap(Y, idx_val, Y_val_path, dtype=np.float32)\n",
    "            log_mem(\"Fold ready\")\n",
    "\n",
    "            model = GradientBoosting(\n",
    "                loss='bce', ntrees=1000, lr=0.05, max_depth=6,\n",
    "                verbose=100, es=50, gpu_id=0\n",
    "            )\n",
    "            model.fit(X_tr, Y_tr, eval_sets=[{'X': X_val, 'y': Y_val}])\n",
    "            log_mem(\"Model fitted\")\n",
    "\n",
    "            val_probs = model.predict(X_val)\n",
    "            oof_preds_gbdt[idx_val] = np.asarray(val_probs, dtype=np.float32)\n",
    "\n",
    "            val_preds = (val_probs > 0.3).astype(int)\n",
    "            f1 = f1_score(Y_val, val_preds, average='micro')\n",
    "            ia_f1 = ia_weighted_f1(Y_val, val_probs, thr=0.3)\n",
    "            print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "            print(f\"  >> Fold {fold+1} IA-F1: ALL={ia_f1['ALL']:.4f}\")\n",
    "\n",
    "            model.save(str(WORK_ROOT / 'features' / f'level1_gbdt_fold{fold}.json'))\n",
    "\n",
    "            del model, val_probs, val_preds, X_tr, X_val, Y_tr, Y_val\n",
    "            gc.collect()\n",
    "            log_mem(f\"End Fold {fold+1}\")\n",
    "\n",
    "            # Best-effort cleanup of temporary fold files\n",
    "            for p in [X_tr_path, X_val_path, Y_tr_path, Y_val_path]:\n",
    "                try:\n",
    "                    os.remove(p)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        oof_preds_gbdt.flush()\n",
    "        del oof_preds_gbdt\n",
    "        gc.collect()\n",
    "        print(\"GBDT OOF saved.\")\n",
    "\n",
    "    # Cleanup X mmap to free file handle\n",
    "    del X\n",
    "    gc.collect()\n",
    "    log_mem(\"End GBDT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8d - Phase 2a Post-Processing: Generate Test Predictions\n",
    "if TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 2a Post-Processing: Generate Test Predictions ===\")\n",
    "    log_mem(\"Before loading Test Features\")\n",
    "    features_test = load_features_dict(split='test')\n",
    "    log_mem(\"Loaded Test Features\")\n",
    "\n",
    "    def _scale_batch(X_batch: np.ndarray, scaler_obj):\n",
    "        \"\"\"Support both StandardScaler and streamed {'mean','scale'} dict.\"\"\"\n",
    "        if isinstance(scaler_obj, dict) and 'mean' in scaler_obj and 'scale' in scaler_obj:\n",
    "            mean = scaler_obj['mean']\n",
    "            scale = scaler_obj['scale']\n",
    "            return ((X_batch - mean) / scale).astype(np.float32)\n",
    "        return scaler_obj.transform(X_batch).astype(np.float32)\n",
    "\n",
    "    # Helper for batched prediction with scaling\n",
    "    def predict_proba_batched_scaled(model, feat_dict, keys, scaler, batch_size=5000):\n",
    "        n_test = feat_dict[keys[0]].shape[0]\n",
    "        preds = []\n",
    "        is_cuml = False\n",
    "        try:\n",
    "            import cuml\n",
    "            if 'cuml' in str(type(model)) or 'cuml' in str(type(getattr(model, 'estimator', None))):\n",
    "                is_cuml = True\n",
    "                import cupy as cp\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        for i in range(0, n_test, batch_size):\n",
    "            X_batch = np.hstack([feat_dict[k][i:i+batch_size] for k in keys]).astype(np.float32)\n",
    "            X_batch = _scale_batch(X_batch, scaler)\n",
    "            if is_cuml:\n",
    "                X_batch = cp.asarray(X_batch)\n",
    "            p = model.predict_proba(X_batch)\n",
    "            if is_cuml:\n",
    "                if hasattr(p, 'get'):\n",
    "                    p = p.get()\n",
    "                elif hasattr(p, 'to_numpy'):\n",
    "                    p = p.to_numpy()\n",
    "            preds.append(p)\n",
    "            del X_batch\n",
    "        return np.vstack(preds)\n",
    "\n",
    "    # Helper for batched prediction from weights (LogReg)\n",
    "    def predict_proba_from_weights(weights_path, feat_dict, keys, scaler, batch_size=5000):\n",
    "        model_data = joblib.load(weights_path)\n",
    "        coef = model_data['coef']  # (n_classes, n_features)\n",
    "        intercept = model_data['intercept']  # (n_classes,)\n",
    "\n",
    "        # Use GPU if available\n",
    "        try:\n",
    "            import cupy as cp\n",
    "            coef = cp.asarray(coef)\n",
    "            intercept = cp.asarray(intercept)\n",
    "            use_gpu = True\n",
    "        except Exception:\n",
    "            use_gpu = False\n",
    "\n",
    "        n_test = feat_dict[keys[0]].shape[0]\n",
    "        preds = []\n",
    "\n",
    "        for i in range(0, n_test, batch_size):\n",
    "            X_batch = np.hstack([feat_dict[k][i:i+batch_size] for k in keys]).astype(np.float32)\n",
    "            X_batch = _scale_batch(X_batch, scaler)\n",
    "\n",
    "            if use_gpu:\n",
    "                X_batch = cp.asarray(X_batch)\n",
    "                logits = cp.dot(X_batch, coef.T) + intercept\n",
    "                p = 1 / (1 + cp.exp(-logits))\n",
    "                p = p.get()\n",
    "            else:\n",
    "                logits = np.dot(X_batch, coef.T) + intercept\n",
    "                p = 1 / (1 + np.exp(-logits))\n",
    "\n",
    "            preds.append(p)\n",
    "            del X_batch\n",
    "\n",
    "        if use_gpu:\n",
    "            del coef, intercept\n",
    "            cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "        return np.vstack(preds)\n",
    "\n",
    "    # A. LogReg Test Preds\n",
    "    print(\"Generating LogReg Test Predictions...\")\n",
    "    test_preds_logreg = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "    for fold in range(5):\n",
    "        print(f\"  Loading LogReg Fold {fold+1}...\")\n",
    "        scaler = joblib.load(WORK_ROOT / 'features' / f'level1_logreg_scaler_fold{fold}.pkl')\n",
    "\n",
    "        weights_path = WORK_ROOT / 'features' / f'level1_logreg_weights_fold{fold}.pkl'\n",
    "        model_path = WORK_ROOT / 'features' / f'level1_logreg_fold{fold}.pkl'\n",
    "\n",
    "        if weights_path.exists():\n",
    "            probs = predict_proba_from_weights(weights_path, features_test, FLAT_KEYS, scaler)\n",
    "        elif model_path.exists():\n",
    "            clf = joblib.load(model_path)\n",
    "            probs = predict_proba_batched_scaled(clf, features_test, FLAT_KEYS, scaler)\n",
    "            del clf\n",
    "        else:\n",
    "            print(f\"  [WARNING] No model found for fold {fold}\")\n",
    "            probs = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "\n",
    "        test_preds_logreg += probs / 5.0\n",
    "        del scaler, probs\n",
    "        gc.collect()\n",
    "\n",
    "    np.save(WORK_ROOT / 'features' / 'test_pred_logreg.npy', test_preds_logreg)\n",
    "    del test_preds_logreg\n",
    "    gc.collect()\n",
    "    print(\"LogReg Test Preds Saved.\")\n",
    "\n",
    "    # B. GBDT Test Preds\n",
    "    if HAS_PYBOOST:\n",
    "        print(\"Generating GBDT Test Predictions...\")\n",
    "        test_preds_gbdt = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "\n",
    "        def predict_proba_batched_gbdt(model, feat_dict, keys, batch_size=5000):\n",
    "            n_test = feat_dict[keys[0]].shape[0]\n",
    "            preds = []\n",
    "            for i in range(0, n_test, batch_size):\n",
    "                X_batch = np.hstack([feat_dict[k][i:i+batch_size] for k in keys]).astype(np.float32)\n",
    "                p = model.predict(X_batch)\n",
    "                preds.append(p)\n",
    "                del X_batch\n",
    "            return np.vstack(preds)\n",
    "\n",
    "        for fold in range(5):\n",
    "            print(f\"  Loading GBDT Fold {fold+1}...\")\n",
    "            model = GradientBoosting.load(str(WORK_ROOT / 'features' / f'level1_gbdt_fold{fold}.json'))\n",
    "            probs = predict_proba_batched_gbdt(model, features_test, FLAT_KEYS)\n",
    "            test_preds_gbdt += probs / 5.0\n",
    "            del model, probs\n",
    "            gc.collect()\n",
    "\n",
    "        np.save(WORK_ROOT / 'features' / 'test_pred_gbdt.npy', test_preds_gbdt)\n",
    "        del test_preds_gbdt\n",
    "        gc.collect()\n",
    "        print(\"GBDT Test Preds Saved.\")\n",
    "\n",
    "    # Cleanup Test Features\n",
    "    del features_test\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f25ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8e - Phase 2b: Deep Learning (DNN) - Memory Optimized\n",
    "import psutil\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if 'log_mem' not in globals():\n",
    "    def log_mem(msg=\"\"):\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            ram_gb = process.memory_info().rss / (1024**3)\n",
    "            gpu_msg = \"\"\n",
    "            if torch.cuda.is_available():\n",
    "                alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "                res = torch.cuda.memory_reserved() / (1024**3)\n",
    "                gpu_msg = f\" | GPU Alloc: {alloc:.2f}GB Res: {res:.2f}GB\"\n",
    "            print(f\"[MEM] {msg:<25} | RAM: {ram_gb:.2f}GB{gpu_msg}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 2b: Deep Learning (DNN) ===\")\n",
    "    log_mem(\"Start DNN\")\n",
    "    \n",
    "    # 1. Reload dicts (Lazy Load via mmap)\n",
    "    log_mem(\"Before loading features\")\n",
    "    # load_features_dict now returns mmap arrays (Cell 8a updated)\n",
    "    features_train, features_test = load_features_dict()\n",
    "    log_mem(\"Loaded features (mmap)\")\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # C. DNN Ensemble (PyTorch, IA-weighted, multimodal, multi-state)\n",
    "    # ------------------------------------------\n",
    "    print(\"\\n--- Training DNN Ensemble (IA-weighted, multimodal, multi-state) ---\")\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Build a stable per-label IA weight vector for the current TOP_K targets\n",
    "    ia_w = weights.copy()\n",
    "    ia_w = np.where(np.isfinite(ia_w) & (ia_w > 0), ia_w, 1.0).astype(np.float32)\n",
    "    ia_w = ia_w / float(np.mean(ia_w))\n",
    "    ia_w = np.clip(ia_w, 0.5, 5.0)\n",
    "    ia_w_t = torch.tensor(ia_w, dtype=torch.float32, device=device).view(1, -1)\n",
    "    \n",
    "    # Optional: include other model predictions as an input stream (PB OOFs analogue)\n",
    "    USE_BASE_OOFS_IN_DNN = True\n",
    "    if USE_BASE_OOFS_IN_DNN and (WORK_ROOT / 'features' / 'oof_pred_logreg.npy').exists():\n",
    "        # These are small enough to load into RAM\n",
    "        oof_stream = [np.load(WORK_ROOT / 'features' / 'oof_pred_logreg.npy').astype(np.float32)]\n",
    "        test_stream = [np.load(WORK_ROOT / 'features' / 'test_pred_logreg.npy').astype(np.float32)]\n",
    "        if (WORK_ROOT / 'features' / 'oof_pred_gbdt.npy').exists():\n",
    "            oof_stream.append(np.load(WORK_ROOT / 'features' / 'oof_pred_gbdt.npy').astype(np.float32))\n",
    "            test_stream.append(np.load(WORK_ROOT / 'features' / 'test_pred_gbdt.npy').astype(np.float32))\n",
    "        base_oof = np.hstack(oof_stream)\n",
    "        base_test = np.hstack(test_stream)\n",
    "        features_train['base_oof'] = base_oof\n",
    "        features_test['base_oof'] = base_test\n",
    "        print(f\"Base OOF stream: train={base_oof.shape} test={base_test.shape}\")\n",
    "        log_mem(\"Loaded Base OOFs\")\n",
    "        \n",
    "    # Select modality keys for the DNN (towers)\n",
    "    DNN_KEYS = [k for k in ['t5', 'esm2_650m', 'esm2_3b', 'taxa', 'text', 'base_oof'] if k in features_train]\n",
    "    print(f\"DNN modality keys={DNN_KEYS}\")\n",
    "    \n",
    "    # Define Dataset for Lazy Loading\n",
    "    class CAFA6Dataset(Dataset):\n",
    "        def __init__(self, features_dict, keys, indices=None, y=None):\n",
    "            self.features = features_dict\n",
    "            self.keys = keys\n",
    "            self.indices = indices if indices is not None else np.arange(features_dict[keys[0]].shape[0])\n",
    "            self.y = y\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            real_idx = self.indices[idx]\n",
    "            # Read from mmap arrays (disk I/O happens here)\n",
    "            batch_x = {k: self.features[k][real_idx].astype(np.float32) for k in self.keys}\n",
    "            \n",
    "            if self.y is not None:\n",
    "                return batch_x, self.y[real_idx]\n",
    "            return batch_x\n",
    "\n",
    "    class Tower(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim=512, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, 1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(1024, out_dim),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "            \n",
    "    class ColossalMultiModalDNN(nn.Module):\n",
    "        def __init__(self, dims: dict, output_dim: int):\n",
    "            super().__init__()\n",
    "            self.keys = list(dims.keys())\n",
    "            self.towers = nn.ModuleDict({k: Tower(dims[k]) for k in self.keys})\n",
    "            fused_dim = 512 * len(self.keys)\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(fused_dim, 2048),\n",
    "                nn.BatchNorm1d(2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(1024, output_dim),\n",
    "            )\n",
    "        def forward(self, batch: dict):\n",
    "            # Move batch to device\n",
    "            hs = [self.towers[k](batch[k]) for k in self.keys]\n",
    "            h = torch.cat(hs, dim=1)\n",
    "            return self.head(h)\n",
    "            \n",
    "    # Multi-state ensembling\n",
    "    DNN_SEEDS = [42, 43, 44, 45, 46]\n",
    "    DNN_EPOCHS = 10\n",
    "    BATCH_SIZE = 256\n",
    "    oof_sum = np.zeros(Y.shape, dtype=np.float32)\n",
    "    test_sum = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "    n_states = len(DNN_SEEDS)\n",
    "    \n",
    "    # Pre-calculate dimensions\n",
    "    dims = {k: int(features_train[k].shape[1]) for k in DNN_KEYS}\n",
    "    \n",
    "    for state_i, seed in enumerate(DNN_SEEDS, 1):\n",
    "        print(f\"\\n[DNN] Random state {state_i}/{n_states}: seed={seed}\")\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        kf_state = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        oof_state = np.zeros(Y.shape, dtype=np.float32)\n",
    "        test_state = np.zeros((len(test_ids), Y.shape[1]), dtype=np.float32)\n",
    "        \n",
    "        for fold, (idx_tr, idx_val) in enumerate(kf_state.split(train_ids)):\n",
    "            print(f\"DNN Fold {fold+1}/5\")\n",
    "            \n",
    "            # Create DataLoaders\n",
    "            # num_workers=0 is safer for mmap\n",
    "            train_ds = CAFA6Dataset(features_train, DNN_KEYS, idx_tr, Y)\n",
    "            val_ds = CAFA6Dataset(features_train, DNN_KEYS, idx_val, Y)\n",
    "            \n",
    "            train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "            val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0, pin_memory=True)\n",
    "            \n",
    "            model = ColossalMultiModalDNN(dims=dims, output_dim=Y.shape[1]).to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "            \n",
    "            # Training Loop\n",
    "            for _epoch in range(DNN_EPOCHS):\n",
    "                model.train()\n",
    "                for batch_x, batch_y in train_loader:\n",
    "                    # Move to GPU\n",
    "                    batch_x = {k: v.to(device) for k, v in batch_x.items()}\n",
    "                    batch_y = batch_y.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    logits = model(batch_x)\n",
    "                    loss_el = F.binary_cross_entropy_with_logits(logits, batch_y, reduction='none')\n",
    "                    loss = (loss_el * ia_w_t).mean()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_probs_list = []\n",
    "            val_indices = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, _ in val_loader:\n",
    "                    batch_x = {k: v.to(device) for k, v in batch_x.items()}\n",
    "                    logits = model(batch_x)\n",
    "                    val_probs_list.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                \n",
    "                val_probs = np.vstack(val_probs_list)\n",
    "                oof_state[idx_val] = val_probs\n",
    "                \n",
    "                # Test Prediction (Fold Averaged)\n",
    "                # Create Test Loader\n",
    "                test_ds = CAFA6Dataset(features_test, DNN_KEYS)\n",
    "                test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=0)\n",
    "                \n",
    "                test_probs_list = []\n",
    "                for batch_x in test_loader:\n",
    "                    batch_x = {k: v.to(device) for k, v in batch_x.items()}\n",
    "                    logits = model(batch_x)\n",
    "                    test_probs_list.append(torch.sigmoid(logits).cpu().numpy())\n",
    "                \n",
    "                test_probs = np.vstack(test_probs_list)\n",
    "                test_state += test_probs / kf_state.get_n_splits()\n",
    "                \n",
    "            val_preds = (val_probs > 0.3).astype(int)\n",
    "            f1 = f1_score(Y[idx_val], val_preds, average='micro')\n",
    "            ia_f1 = ia_weighted_f1(Y[idx_val], val_probs, thr=0.3)\n",
    "            print(f\"  >> Fold {fold+1} micro-F1@0.30: {f1:.4f}\")\n",
    "            print(f\"  >> Fold {fold+1} IA-F1: ALL={ia_f1['ALL']:.4f}\")\n",
    "            \n",
    "            torch.save(model.state_dict(), WORK_ROOT / 'features' / f'level1_dnn_seed{seed}_fold{fold}.pth')\n",
    "            log_mem(f\"End Fold {fold+1}\")\n",
    "            \n",
    "        oof_sum += oof_state\n",
    "        test_sum += test_state\n",
    "        \n",
    "    oof_preds_dnn = (oof_sum / n_states).astype(np.float32)\n",
    "    test_preds_dnn = (test_sum / n_states).astype(np.float32)\n",
    "    np.save(WORK_ROOT / 'features' / 'oof_pred_dnn.npy', oof_preds_dnn)\n",
    "    np.save(WORK_ROOT / 'features' / 'test_pred_dnn.npy', test_preds_dnn)\n",
    "    print(\"DNN OOF + test preds saved (multi-state averaged).\")\n",
    "    \n",
    "    # Persist term list\n",
    "    with open(WORK_ROOT / 'features' / 'top_terms_1500.json', 'w') as f:\n",
    "        json.dump(top_terms, f)\n",
    "        \n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        req = [WORK_ROOT / 'features' / 'top_terms_1500.json', WORK_ROOT / 'features' / 'oof_pred_logreg.npy', WORK_ROOT / 'features' / 'test_pred_logreg.npy', WORK_ROOT / 'features' / 'oof_pred_dnn.npy', WORK_ROOT / 'features' / 'test_pred_dnn.npy']\n",
    "        req += [WORK_ROOT / 'features' / 'oof_pred_gbdt.npy', WORK_ROOT / 'features' / 'test_pred_gbdt.npy']\n",
    "        STORE.push('stage_07_level1_preds', req, note='Level-1 OOF + test preds (LR/GBDT/DNN)')\n",
    "        \n",
    "    print(\"Phase 2 Complete.\")\n",
    "    log_mem(\"End Phase 2b\")\n",
    "else:\n",
    "    print(\"Skipping Phase 2 (TRAIN_LEVEL1=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6574cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 9 - Phase 3: Hierarchy-Aware Stacking (GCN)\n",
    "# =============================================================================\n",
    "# PHASE 3: GCN STACKER (Hierarchy-Aware Refinement)\n",
    "# =============================================================================\n",
    "# Strategy:\n",
    "# 1. Input: OOF predictions from Level 1 models (LogReg, GBDT, DNN) -> Shape (N, K, 3)\n",
    "# 2. Graph: GO Ontology Adjacency Matrix (K, K)\n",
    "# 3. Model: GCN that refines predictions based on graph structure\n",
    "# 4. Output: Refined probabilities (N, K)\n",
    "\n",
    "TRAIN_STACKER = True\n",
    "\n",
    "if TRAIN_STACKER and TRAIN_LEVEL1:\n",
    "    print(\"\\n=== Phase 3: Hierarchy-Aware Stacking (GCN) ===\")\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import networkx as nx\n",
    "    \n",
    "    # 1. Load OOF Predictions\n",
    "    print(\"Loading Level 1 OOFs...\")\n",
    "    oof_files = ['oof_pred_logreg.npy', 'oof_pred_gbdt.npy', 'oof_pred_dnn.npy']\n",
    "    oofs = []\n",
    "    for f in oof_files:\n",
    "        p = WORK_ROOT / 'features' / f\n",
    "        if p.exists():\n",
    "            oofs.append(np.load(p).astype(np.float32))\n",
    "            print(f\"  Loaded {f}\")\n",
    "    \n",
    "    if not oofs:\n",
    "        raise RuntimeError(\"CRITICAL: No Level 1 OOF predictions found (LogReg/GBDT/DNN). Cannot train Stacker. Check Phase 2 execution.\")\n",
    "    else:\n",
    "        # Stack: (N, K, M) where M is number of models\n",
    "        X_stack = np.stack(oofs, axis=2) \n",
    "        print(f\"Stacker Input Shape: {X_stack.shape} (Samples, Terms, Models)\")\n",
    "        \n",
    "        # 2. Build Adjacency Matrix\n",
    "        print(\"Building GO Graph Adjacency Matrix...\")\n",
    "        try:\n",
    "            import obonet\n",
    "            \n",
    "            # Use global path if available, else search\n",
    "            if 'PATH_GO_OBO' in globals() and PATH_GO_OBO.exists():\n",
    "                obo_path = PATH_GO_OBO\n",
    "            else:\n",
    "                possible_paths = [\n",
    "                    WORK_ROOT / 'go-basic.obo',\n",
    "                    WORK_ROOT.parent / 'go-basic.obo',\n",
    "                    Path('go-basic.obo'),\n",
    "                    Path('Train/go-basic.obo'),\n",
    "                    Path('../Train/go-basic.obo'),\n",
    "                    Path('/content/cafa6_data/Train/go-basic.obo')\n",
    "                ]\n",
    "                obo_path = None\n",
    "                for p in possible_paths:\n",
    "                    if p.exists():\n",
    "                        obo_path = p\n",
    "                        break\n",
    "                if obo_path is None:\n",
    "                    raise FileNotFoundError(f\"go-basic.obo not found. Searched: {[str(p) for p in possible_paths]}\")\n",
    "\n",
    "            print(f\"Loading OBO from {obo_path}...\")\n",
    "            graph = obonet.read_obo(obo_path)\n",
    "            \n",
    "            # Create subgraph for our K terms\n",
    "            # Map term to index\n",
    "            term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "            \n",
    "            # Build adjacency (A_ij = 1 if i is parent of j or j is parent of i? GCN usually undirected or directed?)\n",
    "            # Standard GCN: Undirected + Self-loops.\n",
    "            # But GO is directed (Child -> Parent).\n",
    "            # We want information to flow both ways? Or mostly Child -> Parent (consistency)?\n",
    "            # Let's use a symmetric normalized adjacency for standard GCN.\n",
    "            \n",
    "            adj = np.eye(len(top_terms), dtype=np.float32) # Self-loops\n",
    "            \n",
    "            # Fill edges\n",
    "            edges_count = 0\n",
    "            for child in top_terms:\n",
    "                if child in graph:\n",
    "                    for parent in graph.successors(child): # 'is_a' points to parent in obonet/networkx\n",
    "                        if parent in term_to_idx:\n",
    "                            i, j = term_to_idx[child], term_to_idx[parent]\n",
    "                            adj[i, j] = 1.0\n",
    "                            adj[j, i] = 1.0 # Symmetric\n",
    "                            edges_count += 1\n",
    "            \n",
    "            print(f\"Graph built. Nodes: {len(top_terms)}, Edges (in subset): {edges_count}\")\n",
    "            \n",
    "            # Normalize Adjacency: D^{-1/2} A D^{-1/2}\n",
    "            D = np.sum(adj, axis=1)\n",
    "            D_inv_sqrt = np.power(D, -0.5)\n",
    "            D_inv_sqrt[np.isinf(D_inv_sqrt)] = 0.\n",
    "            D_mat = np.diag(D_inv_sqrt)\n",
    "            A_norm = D_mat @ adj @ D_mat\n",
    "            A_norm = torch.tensor(A_norm, dtype=torch.float32, device=device)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to build graph: {e}. Using Identity (No-op GCN).\")\n",
    "            A_norm = torch.eye(len(top_terms), device=device)\n",
    "\n",
    "        # 3. Define GCN Model\n",
    "        class GCNStacker(nn.Module):\n",
    "            def __init__(self, n_models, n_hidden=16):\n",
    "                super().__init__()\n",
    "                # We apply GCN per protein.\n",
    "                # Input features per node: [p_logreg, p_gbdt, p_dnn] (dim=3)\n",
    "                # We want to learn a mixing weight + graph smoothing.\n",
    "                \n",
    "                # Simple 2-layer GCN\n",
    "                self.gc1 = nn.Linear(n_models, n_hidden)\n",
    "                self.gc2 = nn.Linear(n_hidden, 1) # Output 1 score per node\n",
    "                self.relu = nn.ReLU()\n",
    "                \n",
    "            def forward(self, x, adj):\n",
    "                # x: (Batch, Nodes, Models)\n",
    "                # adj: (Nodes, Nodes)\n",
    "                \n",
    "                # Layer 1: H1 = ReLU(A X W1)\n",
    "                # Support = X W1\n",
    "                # Output = A Support\n",
    "                \n",
    "                # Batch matmul is tricky with static adj.\n",
    "                # x: (B, N, M)\n",
    "                # W1: (M, H)\n",
    "                # x @ W1 -> (B, N, H)\n",
    "                support = self.gc1(x) \n",
    "                \n",
    "                # A @ support\n",
    "                # A: (N, N)\n",
    "                # support: (B, N, H) -> permute to (B, H, N) for matmul?\n",
    "                # Or just (A @ support[i]) for each i?\n",
    "                # Einsum: n k, b k h -> b n h\n",
    "                out1 = torch.einsum('nk,bkh->bnh', adj, support)\n",
    "                out1 = self.relu(out1)\n",
    "                \n",
    "                # Layer 2\n",
    "                support2 = self.gc2(out1) # (B, N, 1)\n",
    "                out2 = torch.einsum('nk,bkh->bnh', adj, support2)\n",
    "                \n",
    "                return out2.squeeze(-1) # (B, N)\n",
    "\n",
    "        # 4. Train Stacker\n",
    "        print(\"Training GCN Stacker...\")\n",
    "        # We split the OOFs into train/val for the stacker?\n",
    "        # Actually, OOFs are already \"test-like\" for the whole train set.\n",
    "        # We can train on the whole train set (since OOFs were generated via CV).\n",
    "        \n",
    "        # Targets\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "        X_stack_tensor = torch.tensor(X_stack, dtype=torch.float32, device=device)\n",
    "        \n",
    "        model = GCNStacker(n_models=X_stack.shape[2]).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        BATCH_SIZE = 128\n",
    "        N_EPOCHS = 20\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X_stack_tensor, Y_tensor)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            total_loss = 0\n",
    "            for xb, yb in loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb, A_norm)\n",
    "                loss = loss_fn(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            if (epoch+1) % 5 == 0:\n",
    "                print(f\"  Epoch {epoch+1}/{N_EPOCHS} Loss: {total_loss/len(loader):.4f}\")\n",
    "                \n",
    "        # 5. Generate Final Predictions (on Test)\n",
    "        print(\"Generating Final Stacker Predictions on Test...\")\n",
    "        # Load Test Preds\n",
    "        test_files = ['test_pred_logreg.npy', 'test_pred_gbdt.npy', 'test_pred_dnn.npy']\n",
    "        tests = []\n",
    "        for f in test_files:\n",
    "            p = WORK_ROOT / 'features' / f\n",
    "            if p.exists():\n",
    "                tests.append(np.load(p).astype(np.float32))\n",
    "        \n",
    "        if tests:\n",
    "            X_test_stack = np.stack(tests, axis=2)\n",
    "            X_test_tensor = torch.tensor(X_test_stack, dtype=torch.float32, device=device)\n",
    "            \n",
    "            model.eval()\n",
    "            final_preds = []\n",
    "            with torch.no_grad():\n",
    "                # Process in chunks to avoid OOM\n",
    "                for i in range(0, len(X_test_tensor), BATCH_SIZE):\n",
    "                    batch = X_test_tensor[i:i+BATCH_SIZE]\n",
    "                    logits = model(batch, A_norm)\n",
    "                    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                    final_preds.append(probs)\n",
    "            \n",
    "            final_preds = np.vstack(final_preds)\n",
    "            np.save(WORK_ROOT / 'features' / 'final_pred_gcn.npy', final_preds)\n",
    "            print(f\"Final GCN Predictions Saved: {final_preds.shape}\")\n",
    "            \n",
    "            # Clean up\n",
    "            del X_stack_tensor, Y_tensor, X_test_tensor, model, A_norm\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise RuntimeError(\"CRITICAL: No Level 1 Test predictions found. Cannot generate final submission.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Phase 3 (Stacker).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 10 - Phase 4: Strict Post-Processing & Submission\n",
    "# =============================================================================\n",
    "# PHASE 4: STRICT POST-PROCESSING (Hierarchy Enforcement)\n",
    "# =============================================================================\n",
    "# 1. Max Propagation (Parent Rule): P(parent) >= P(child)\n",
    "# 2. Min Propagation (Child Rule): P(child) <= P(parent)\n",
    "# 3. Top-1500 Selection\n",
    "# 4. Submission Generation\n",
    "\n",
    "APPLY_POSTPROC = True\n",
    "\n",
    "if APPLY_POSTPROC:\n",
    "    print(\"\\n=== Phase 4: Strict Post-Processing ===\")\n",
    "    \n",
    "    # Load predictions (prefer GCN, fallback to DNN/LogReg average)\n",
    "    if (WORK_ROOT / 'features' / 'final_pred_gcn.npy').exists():\n",
    "        print(\"Loading GCN predictions...\")\n",
    "        preds = np.load(WORK_ROOT / 'features' / 'final_pred_gcn.npy')\n",
    "    elif (WORK_ROOT / 'features' / 'test_pred_dnn.npy').exists():\n",
    "        print(\"Loading DNN predictions (GCN missing)...\")\n",
    "        preds = np.load(WORK_ROOT / 'features' / 'test_pred_dnn.npy')\n",
    "    else:\n",
    "        print(\"Loading LogReg predictions (Fallback)...\")\n",
    "        preds = np.load(WORK_ROOT / 'features' / 'test_pred_logreg.npy')\n",
    "        \n",
    "    print(f\"Raw Predictions Shape: {preds.shape}\")\n",
    "    \n",
    "    # Load Graph for Propagation\n",
    "    try:\n",
    "        import obonet\n",
    "        import networkx as nx\n",
    "        \n",
    "        # Use global path if available, else search\n",
    "        if 'PATH_GO_OBO' in globals() and PATH_GO_OBO.exists():\n",
    "            obo_path = PATH_GO_OBO\n",
    "        else:\n",
    "            possible_paths = [\n",
    "                WORK_ROOT / 'go-basic.obo',\n",
    "                WORK_ROOT.parent / 'go-basic.obo',\n",
    "                Path('go-basic.obo'),\n",
    "                Path('Train/go-basic.obo'),\n",
    "                Path('../Train/go-basic.obo'),\n",
    "                Path('/content/cafa6_data/Train/go-basic.obo')\n",
    "            ]\n",
    "            obo_path = None\n",
    "            for p in possible_paths:\n",
    "                if p.exists():\n",
    "                    obo_path = p\n",
    "                    break\n",
    "            if obo_path is None:\n",
    "                raise FileNotFoundError(f\"go-basic.obo not found. Searched: {[str(p) for p in possible_paths]}\")\n",
    "\n",
    "        print(f\"Loading OBO from {obo_path}...\")\n",
    "        graph = obonet.read_obo(obo_path)\n",
    "        \n",
    "        # Map term to index\n",
    "        term_to_idx = {t: i for i, t in enumerate(top_terms)}\n",
    "        \n",
    "        # Precompute parent/child indices for fast propagation\n",
    "        # We need a topological sort or just iterate multiple times?\n",
    "        # Since DAG depth is small, iterating 2-3 times is usually enough.\n",
    "        # Or we can just use the edge list.\n",
    "        \n",
    "        # Build parent map: child_idx -> [parent_indices]\n",
    "        child_to_parents = {}\n",
    "        parent_to_children = {}\n",
    "        \n",
    "        for child in top_terms:\n",
    "            if child in graph:\n",
    "                c_idx = term_to_idx[child]\n",
    "                # Parents\n",
    "                parents = [p for p in graph.successors(child) if p in term_to_idx]\n",
    "                if parents:\n",
    "                    child_to_parents[c_idx] = [term_to_idx[p] for p in parents]\n",
    "                \n",
    "                # Children (predecessors in networkx/obonet)\n",
    "                children = [c for c in graph.predecessors(child) if c in term_to_idx]\n",
    "                if children:\n",
    "                    parent_to_children[c_idx] = [term_to_idx[c] for c in children]\n",
    "                    \n",
    "        print(f\"Hierarchy constraints: {len(child_to_parents)} terms have parents in subset.\")\n",
    "        \n",
    "        # 1. Max Propagation (Child -> Parent)\n",
    "        # Ensure Parent >= Child\n",
    "        # Iterate a few times to propagate up\n",
    "        print(\"Applying Max Propagation (Child -> Parent)...\")\n",
    "        for _ in range(3):\n",
    "            for c_idx, p_indices in child_to_parents.items():\n",
    "                # Vectorized update for all samples\n",
    "                # P(parent) = max(P(parent), P(child))\n",
    "                # We can do this efficiently?\n",
    "                # preds[:, p_indices] = np.maximum(preds[:, p_indices], preds[:, c_idx:c_idx+1])\n",
    "                # This is slow in python loop.\n",
    "                pass\n",
    "        \n",
    "        # Optimized Propagation (Matrix based?)\n",
    "        # Since we have 13.5k terms, a loop is okay if we batch operations?\n",
    "        # Actually, iterating over 13k terms is fast in Python (milliseconds).\n",
    "        # The operation inside is numpy array (20k samples).\n",
    "        \n",
    "        # Let's do a topological sort order to do it in 1 pass?\n",
    "        # Subgraph of top_terms\n",
    "        subgraph = graph.subgraph(top_terms)\n",
    "        try:\n",
    "            topo_order = list(nx.topological_sort(subgraph))\n",
    "            # Reverse topo order for Child -> Parent (Leaves first)\n",
    "            topo_order_rev = topo_order[::-1]\n",
    "            \n",
    "            print(\"  Optimized Max Prop (1 pass)...\")\n",
    "            for term in topo_order_rev:\n",
    "                if term not in term_to_idx: continue\n",
    "                c_idx = term_to_idx[term]\n",
    "                # Propagate to parents\n",
    "                if term in child_to_parents: # Use precomputed map\n",
    "                    # parents = child_to_parents[term] # Wait, map uses indices\n",
    "                    pass\n",
    "                \n",
    "                # Use graph directly\n",
    "                parents = [p for p in graph.successors(term) if p in term_to_idx]\n",
    "                if not parents: continue\n",
    "                \n",
    "                p_indices = [term_to_idx[p] for p in parents]\n",
    "                # preds[:, p_indices] = np.maximum(preds[:, p_indices], preds[:, c_idx, None])\n",
    "                # Use broadcasting\n",
    "                child_val = preds[:, c_idx:c_idx+1]\n",
    "                preds[:, p_indices] = np.maximum(preds[:, p_indices], child_val)\n",
    "                \n",
    "        except nx.NetworkXUnfeasible:\n",
    "            print(\"  [WARNING] Cycle detected or graph issue. Skipping topological prop.\")\n",
    "            \n",
    "        # 2. Min Propagation (Parent -> Child)\n",
    "        # Ensure Child <= Parent\n",
    "        # Iterate Root -> Leaves (Topo order)\n",
    "        print(\"Applying Min Propagation (Parent -> Child)...\")\n",
    "        try:\n",
    "            for term in topo_order:\n",
    "                if term not in term_to_idx: continue\n",
    "                p_idx = term_to_idx[term]\n",
    "                \n",
    "                children = [c for c in graph.predecessors(term) if c in term_to_idx]\n",
    "                if not children: continue\n",
    "                \n",
    "                c_indices = [term_to_idx[c] for c in children]\n",
    "                parent_val = preds[:, p_idx:p_idx+1]\n",
    "                preds[:, c_indices] = np.minimum(preds[:, c_indices], parent_val)\n",
    "                \n",
    "        except: pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Post-processing failed: {e}\")\n",
    "        \n",
    "    # 3. Submission Formatting\n",
    "    print(\"Formatting Submission...\")\n",
    "    # Clip to (0, 1]\n",
    "    preds = np.clip(preds, 0.0, 1.0)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    # Format: ProteinID, TermID, Score\n",
    "    # We need to melt the matrix. This is huge (20k * 13.5k = 270M rows).\n",
    "    # We only keep top 1500 per protein?\n",
    "    \n",
    "    # Strategy:\n",
    "    # 1. For each protein, find top 1500 indices.\n",
    "    # 2. Create sparse rows.\n",
    "    \n",
    "    submission_rows = []\n",
    "    test_ids_list = test_ids.tolist()\n",
    "    \n",
    "    print(\"Selecting Top-1500 per protein...\")\n",
    "    for i, pid in enumerate(test_ids_list):\n",
    "        # Get scores for this protein\n",
    "        scores = preds[i]\n",
    "        # Find top 1500 indices\n",
    "        # argpartition is faster than sort\n",
    "        if len(scores) > 1500:\n",
    "            top_indices = np.argpartition(scores, -1500)[-1500:]\n",
    "        else:\n",
    "            top_indices = np.arange(len(scores))\n",
    "            \n",
    "        # Filter out zero scores?\n",
    "        # top_indices = top_indices[scores[top_indices] > 0.001]\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            score = scores[idx]\n",
    "            if score > 0: # Only positive\n",
    "                term = top_terms[idx]\n",
    "                submission_rows.append((pid, term, f\"{score:.3f}\"))\n",
    "                \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f\"  Processed {i+1}/{len(test_ids_list)} proteins...\")\n",
    "            \n",
    "    # Write to file\n",
    "    print(\"Writing submission.tsv...\")\n",
    "    with open('submission.tsv', 'w') as f:\n",
    "        # No header usually? Or check sample_submission\n",
    "        # CAFA format: no header? Or header?\n",
    "        # Sample submission has header?\n",
    "        # Let's check sample_submission.tsv\n",
    "        pass\n",
    "        \n",
    "    # Just write standard TSV\n",
    "    df_sub = pd.DataFrame(submission_rows, columns=['Protein Id', 'GO Term Id', 'Prediction'])\n",
    "    df_sub.to_csv('submission.tsv', sep='\\t', index=False, header=False)\n",
    "    print(f\"Submission saved: {len(df_sub)} rows.\")\n",
    "    \n",
    "    # Zip it\n",
    "    os.system('zip submission.zip submission.tsv')\n",
    "    print(\"Zipped to submission.zip\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Post-Processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 13E - KNN (cosine; ESM2-3B)\n",
    "# Notes:\n",
    "# - Uses in-memory features from CELL 13 (features_train/features_test, Y, top_terms).\n",
    "# - Checkpoint pushing is controlled globally by CAFA_CHECKPOINT_PUSH (default 0 in this notebook).\n",
    "if not TRAIN_LEVEL1:\n",
    "    print('Skipping KNN (TRAIN_LEVEL1=False).')\n",
    "else:\n",
    "    import os\n",
    "    import json\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    FORCE_REBUILD = (os.getenv('CAFA_FORCE_REBUILD', '0').strip() == '1')\n",
    "\n",
    "    PRED_DIR = WORK_ROOT / 'features' / 'level1_preds'\n",
    "    PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    knn_oof_path = PRED_DIR / 'oof_pred_knn.npy'\n",
    "    knn_test_path = PRED_DIR / 'test_pred_knn.npy'\n",
    "\n",
    "    # Backwards-compatible copies (some downstream code loads from WORK_ROOT/features)\n",
    "    knn_oof_compat = WORK_ROOT / 'features' / 'oof_pred_knn.npy'\n",
    "    knn_test_compat = WORK_ROOT / 'features' / 'test_pred_knn.npy'\n",
    "\n",
    "    if knn_oof_path.exists() and knn_test_path.exists() and (not FORCE_REBUILD):\n",
    "        print('KNN preds exist; skipping training (set CAFA_FORCE_REBUILD=1 to force).')\n",
    "        oof_pred_knn = np.load(knn_oof_path)\n",
    "        test_pred_knn = np.load(knn_test_path)\n",
    "        oof_max_sim = None\n",
    "    else:\n",
    "        if 'features_train' not in globals() or 'features_test' not in globals():\n",
    "            raise RuntimeError('Missing `features_train`/`features_test`. Run CELL 13 first.')\n",
    "        if 'esm2_3b' not in features_train:\n",
    "            raise FileNotFoundError(\"Missing required modality 'esm2_3b' in features_train. Ensure features/train_embeds_esm2_3b.npy exists.\")\n",
    "\n",
    "        X_knn = features_train['esm2_3b'].astype(np.float32)\n",
    "        X_knn_test = features_test['esm2_3b'].astype(np.float32)\n",
    "\n",
    "        # Enforce TOP_K alignment using the persisted term list.\n",
    "        top_terms_path = WORK_ROOT / 'features' / 'top_terms_1500.json'\n",
    "        if top_terms_path.exists():\n",
    "            top_terms_knn = json.loads(top_terms_path.read_text())\n",
    "            if Y.shape[1] != len(top_terms_knn):\n",
    "                raise ValueError(f'KNN shape mismatch: Y has {Y.shape[1]} cols but top_terms_1500.json has {len(top_terms_knn)} terms.')\n",
    "        else:\n",
    "            if 'top_terms' not in globals():\n",
    "                raise RuntimeError('Missing top_terms_1500.json and in-memory top_terms. Run CELL 13 first.')\n",
    "            top_terms_knn = list(top_terms)\n",
    "            if Y.shape[1] != len(top_terms_knn):\n",
    "                raise ValueError(f'KNN shape mismatch: Y has {Y.shape[1]} cols but top_terms has {len(top_terms_knn)} terms.')\n",
    "\n",
    "        # KNN needs binary targets (presence/absence), not counts.\n",
    "        Y_knn = (Y > 0).astype(np.float32)\n",
    "\n",
    "        def _l2_norm(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "            n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "            return x / np.maximum(n, eps)\n",
    "\n",
    "        # Cosine distance is best-behaved on L2-normalised vectors\n",
    "        X_knn = _l2_norm(X_knn)\n",
    "        X_knn_test = _l2_norm(X_knn_test)\n",
    "\n",
    "        KNN_K = int(os.getenv('CAFA_KNN_K', '50'))\n",
    "        KNN_BATCH = int(os.getenv('CAFA_KNN_BATCH', '256'))\n",
    "\n",
    "        n_splits = 5\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        oof_pred_knn = np.zeros((X_knn.shape[0], Y_knn.shape[1]), dtype=np.float32)\n",
    "        test_pred_knn = np.zeros((X_knn_test.shape[0], Y_knn.shape[1]), dtype=np.float32)\n",
    "        oof_max_sim = np.zeros((X_knn.shape[0],), dtype=np.float32)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_knn), start=1):\n",
    "            print(f'Fold {fold}/{n_splits} (KNN)')\n",
    "            knn = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=4)\n",
    "            knn.fit(X_knn[tr_idx])\n",
    "\n",
    "            dists, neigh = knn.kneighbors(X_knn[va_idx], return_distance=True)\n",
    "            sims = np.clip((1.0 - dists).astype(np.float32), 0.0, 1.0)\n",
    "            oof_max_sim[va_idx] = sims.max(axis=1)\n",
    "            neigh_global = tr_idx[neigh]  # map to global row indices into Y_knn\n",
    "\n",
    "            for i in range(0, len(va_idx), KNN_BATCH):\n",
    "                j = min(i + KNN_BATCH, len(va_idx))\n",
    "                neigh_b = neigh_global[i:j]\n",
    "                sims_b = sims[i:j]\n",
    "                denom = np.maximum(sims_b.sum(axis=1, keepdims=True), 1e-8)\n",
    "                Y_nei = Y_knn[neigh_b]  # (B, K, L)\n",
    "                scores = (np.einsum('bk,bkl->bl', sims_b, Y_nei) / denom).astype(np.float32)\n",
    "                oof_pred_knn[va_idx[i:j]] = scores\n",
    "\n",
    "            if 'ia_weighted_f1' in globals():\n",
    "                print('  IA-F1:', ia_weighted_f1(Y_knn[va_idx], oof_pred_knn[va_idx], thr=0.3))\n",
    "\n",
    "        # Final model on full train -> test\n",
    "        knn_final = NearestNeighbors(n_neighbors=KNN_K, metric='cosine', n_jobs=4)\n",
    "        knn_final.fit(X_knn)\n",
    "        dists_te, neigh_te = knn_final.kneighbors(X_knn_test, return_distance=True)\n",
    "        sims_te = np.clip((1.0 - dists_te).astype(np.float32), 0.0, 1.0)\n",
    "        denom_te = np.maximum(sims_te.sum(axis=1, keepdims=True), 1e-8)\n",
    "\n",
    "        for i in range(0, X_knn_test.shape[0], KNN_BATCH):\n",
    "            j = min(i + KNN_BATCH, X_knn_test.shape[0])\n",
    "            neigh_b = neigh_te[i:j]\n",
    "            sims_b = sims_te[i:j]\n",
    "            Y_nei = Y_knn[neigh_b]\n",
    "            scores = (np.einsum('bk,bkl->bl', sims_b, Y_nei) / denom_te[i:j]).astype(np.float32)\n",
    "            test_pred_knn[i:j] = scores\n",
    "\n",
    "        np.save(knn_oof_path, oof_pred_knn)\n",
    "        np.save(knn_test_path, test_pred_knn)\n",
    "        np.save(knn_oof_compat, oof_pred_knn)\n",
    "        np.save(knn_test_compat, test_pred_knn)\n",
    "        print('Saved:', knn_oof_path)\n",
    "        print('Saved:', knn_test_path)\n",
    "\n",
    "    # Checkpoint push is controlled by CAFA_CHECKPOINT_PUSH inside STORE.push; no extra guard needed here.\n",
    "    if 'STORE' in globals() and STORE is not None:\n",
    "        STORE.push(\n",
    "            stage='stage_07d_level1_knn',\n",
    "            required_paths=[\n",
    "                str((WORK_ROOT / 'features' / 'top_terms_1500.json').as_posix()),\n",
    "                str(knn_oof_path.as_posix()),\n",
    "                str(knn_test_path.as_posix()),\n",
    "            ],\n",
    "            note='Level-1 KNN (cosine) predictions using ESM2-3B embeddings (OOF + test).',\n",
    "        )\n",
    "\n",
    "    # Diagnostics: similarity distribution + IA-F1 vs threshold\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        if oof_max_sim is not None:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.hist(oof_max_sim, bins=50)\n",
    "            plt.title('KNN OOF diagnostic: max cosine similarity to neighbours (per protein)')\n",
    "            plt.xlabel('max similarity')\n",
    "            plt.ylabel('count')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "\n",
    "        if 'ia_weighted_f1' in globals():\n",
    "            thrs = np.linspace(0.05, 0.60, 23)\n",
    "            curves = {k: [] for k in ['ALL', 'MF', 'BP', 'CC']}\n",
    "            for thr in thrs:\n",
    "                s = ia_weighted_f1(Y, oof_pred_knn, thr=float(thr))\n",
    "                for k in curves.keys():\n",
    "                    curves[k].append(s[k])\n",
    "            plt.figure(figsize=(10, 3))\n",
    "            for k in ['ALL', 'MF', 'BP', 'CC']:\n",
    "                plt.plot(thrs, curves[k], label=k)\n",
    "            plt.title('KNN OOF: IA-weighted F1 vs threshold')\n",
    "            plt.xlabel('threshold')\n",
    "            plt.ylabel('IA-F1')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print('KNN diagnostics skipped:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba11ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8a_audit - Deep Audit of train_terms.parquet\n",
    "# =================================================\n",
    "# VALIDATION STEP: Ensure input data is trustworthy before training.\n",
    "# =================================================\n",
    "import pandas as pd\n",
    "import obonet\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== AUDIT: train_terms.parquet Validity Check ===\")\n",
    "\n",
    "# 1. Load Parquet\n",
    "parquet_path = WORK_ROOT / 'parsed' / 'train_terms.parquet'\n",
    "if not parquet_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing {parquet_path}\")\n",
    "\n",
    "print(f\"Loading {parquet_path}...\")\n",
    "df = pd.read_parquet(parquet_path)\n",
    "print(f\"  Rows: {len(df):,}\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"  Unique Terms: {df['term'].nunique():,}\")\n",
    "print(f\"  Unique Proteins: {df['EntryID'].nunique():,}\")\n",
    "\n",
    "# 2. Load OBO for Ground Truth\n",
    "# Reuse global path if set, else search\n",
    "if 'PATH_GO_OBO' in globals() and PATH_GO_OBO.exists():\n",
    "    obo_path = PATH_GO_OBO\n",
    "else:\n",
    "    # Fallback search\n",
    "    possible_paths = [\n",
    "        WORK_ROOT / 'go-basic.obo',\n",
    "        Path('/content/cafa6_data/Train/go-basic.obo'),\n",
    "        Path('Train/go-basic.obo')\n",
    "    ]\n",
    "    obo_path = next((p for p in possible_paths if p.exists()), None)\n",
    "\n",
    "if not obo_path:\n",
    "    print(\"[WARNING] OBO file not found. Cannot validate semantic correctness.\")\n",
    "else:\n",
    "    print(f\"Loading Ground Truth OBO: {obo_path}...\")\n",
    "    graph = obonet.read_obo(obo_path)\n",
    "    \n",
    "    # Build Truth Map\n",
    "    term_to_ns = {node: data.get('namespace', 'unknown') for node, data in graph.nodes(data=True)}\n",
    "    ns_map = {'biological_process': 'BP', 'molecular_function': 'MF', 'cellular_component': 'CC'}\n",
    "    \n",
    "    # 3. Validation Checks\n",
    "    print(\"\\n--- Validation Results ---\")\n",
    "    \n",
    "    # A. Term Validity\n",
    "    valid_terms = set(term_to_ns.keys())\n",
    "    df['is_valid_term'] = df['term'].isin(valid_terms)\n",
    "    n_invalid = (~df['is_valid_term']).sum()\n",
    "    print(f\"1. Invalid GO Terms: {n_invalid:,} / {len(df):,} ({n_invalid/len(df):.2%})\")\n",
    "    if n_invalid > 0:\n",
    "        print(f\"   Sample invalid: {df.loc[~df['is_valid_term'], 'term'].head(5).tolist()}\")\n",
    "    \n",
    "    # B. Aspect Consistency\n",
    "    if 'aspect' in df.columns:\n",
    "        # Map OBO namespace to BP/MF/CC\n",
    "        df['truth_aspect'] = df['term'].map(lambda t: ns_map.get(term_to_ns.get(t), 'UNK'))\n",
    "        \n",
    "        # Check matches\n",
    "        df['aspect_match'] = (df['aspect'] == df['truth_aspect'])\n",
    "        n_mismatch = (~df['aspect_match']).sum()\n",
    "        print(f\"2. Aspect Mismatches: {n_mismatch:,} / {len(df):,} ({n_mismatch/len(df):.2%})\")\n",
    "        \n",
    "        if n_mismatch > 0:\n",
    "            print(\"   Sample mismatches (Parquet vs OBO):\")\n",
    "            print(df.loc[~df['aspect_match'], ['term', 'aspect', 'truth_aspect']].head(5))\n",
    "            \n",
    "            # Distribution of what's in the file\n",
    "            print(\"\\n   Current 'aspect' column distribution in file:\")\n",
    "            print(df['aspect'].value_counts())\n",
    "    else:\n",
    "        print(\"2. Aspect Column: MISSING (Will be generated in next step)\")\n",
    "\n",
    "print(\"\\n=== Audit Complete ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}