{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9ba7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration\n",
    "ENVIRONMENT = 'local'  # Change to 'kaggle' when running on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f4dbdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers pandas numpy scikit-learn tqdm biopython matplotlib seaborn pyarrow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb95cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports successful | Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Imports successful | Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f175e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Base directory: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\n"
     ]
    }
   ],
   "source": [
    "# Set base directory\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    base_dir = Path(\"/kaggle/input/cafa-6-dataset\")\n",
    "else:\n",
    "    # Handle both running from notebooks/ and project root\n",
    "    if Path.cwd().name == 'notebooks':\n",
    "        base_dir = Path.cwd().parent\n",
    "    else:\n",
    "        base_dir = Path.cwd()\n",
    "\n",
    "print(f\"ðŸ“ Base directory: {base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e1c9d7",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799496bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Model: facebook/esm2_t30_150M_UR50D\n",
      "   Effective batch size: 32\n",
      "   Max sequence length: 448\n",
      "   Mixed precision (fp16): True\n",
      "   Vocabulary: Top 5000 GO terms\n",
      "   Save directory: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\models\\esm_finetuned_150M\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# MODEL CONFIGURATION\n",
    "# ========================================\n",
    "# Options: \n",
    "#   \"facebook/esm2_t6_8M_UR50D\"    - 8M params, fast (~1h training)\n",
    "#   \"facebook/esm2_t30_150M_UR50D\" - 150M params, ~4h training\n",
    "#   \"facebook/esm2_t33_650M_UR50D\" - 650M params, ~60h training\n",
    "MODEL_NAME = \"facebook/esm2_t30_150M_UR50D\"  # ðŸ”¥ UPGRADED TO 150M\n",
    "\n",
    "# ========================================\n",
    "# TRAINING CONFIGURATION (adjusted for 150M)\n",
    "# ========================================\n",
    "# Memory-optimised settings for 150M on 8GB VRAM\n",
    "BATCH_SIZE = 2                    # Reduced from 8 (150M needs more memory)\n",
    "GRADIENT_ACCUMULATION = 16        # Increased to maintain effective batch size 32\n",
    "LEARNING_RATE = 1e-5              # Slightly lower for larger model\n",
    "NUM_EPOCHS = 10\n",
    "MAX_LENGTH = 448                  # Reduced from 512 to save memory\n",
    "USE_FP16 = True                   # Mixed precision for memory efficiency\n",
    "\n",
    "# ========================================\n",
    "# VOCABULARY CONFIGURATION\n",
    "# ========================================\n",
    "VOCAB_SIZE = 5000\n",
    "MIN_COUNT = 10\n",
    "\n",
    "# ========================================\n",
    "# REGULARISATION\n",
    "# ========================================\n",
    "PATIENCE = 3\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Paths\n",
    "SAVE_DIR = base_dir / \"models\" / \"esm_finetuned_150M\" if ENVIRONMENT == 'local' else Path(\"/kaggle/working/models/esm_finetuned_150M\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ”¥ Model: {MODEL_NAME}\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"   Max sequence length: {MAX_LENGTH}\")\n",
    "print(f\"   Mixed precision (fp16): {USE_FP16}\")\n",
    "print(f\"   Vocabulary: Top {VOCAB_SIZE} GO terms\")\n",
    "print(f\"   Save directory: {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfb0e9",
   "metadata": {},
   "source": [
    "## 2. Load Data and Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b3c558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sequences...\n",
      "Loaded 82404 sequences\n",
      "Sample IDs: ['A0A0C5B5G6', 'A0JNW5', 'A0JP26']\n",
      "Loading annotations...\n",
      "Total annotations: 537027\n",
      "Loaded 82404 sequences\n",
      "Sample IDs: ['A0A0C5B5G6', 'A0JNW5', 'A0JP26']\n",
      "Loading annotations...\n",
      "Total annotations: 537027\n"
     ]
    }
   ],
   "source": [
    "# Load sequences\n",
    "print(\"Loading sequences...\")\n",
    "sequences = {}\n",
    "for record in SeqIO.parse(base_dir / \"Train\" / \"train_sequences.fasta\", \"fasta\"):\n",
    "    seq_id = record.id\n",
    "    # Parse pipe-delimited format: sp|P53919|NAF1_YEAST -> P53919\n",
    "    if '|' in seq_id:\n",
    "        seq_id = seq_id.split('|')[1]\n",
    "    sequences[seq_id] = str(record.seq)\n",
    "\n",
    "print(f\"Loaded {len(sequences)} sequences\")\n",
    "print(f\"Sample IDs: {list(sequences.keys())[:3]}\")\n",
    "\n",
    "# Load annotations\n",
    "print(\"Loading annotations...\")\n",
    "train_terms = pd.read_csv(base_dir / \"Train\" / \"train_terms.tsv\", sep='\\t')\n",
    "print(f\"Total annotations: {len(train_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1881e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Vocabulary size: 5000\n",
      "Coverage: 81.96% of annotations\n",
      "âœ… Vocabulary saved\n",
      "\n",
      "Loading IA weights...\n",
      "âœ… IA weights loaded: 40122 terms\n",
      "\n",
      "ðŸ” Checking ID alignment...\n",
      "Annotation IDs: 82,404\n",
      "Sequence IDs: 82,404\n",
      "Matching IDs: 82,404\n",
      "âœ… ID alignment verified (100.0% coverage)\n",
      "Vocabulary size: 5000\n",
      "Coverage: 81.96% of annotations\n",
      "âœ… Vocabulary saved\n",
      "\n",
      "Loading IA weights...\n",
      "âœ… IA weights loaded: 40122 terms\n",
      "\n",
      "ðŸ” Checking ID alignment...\n",
      "Annotation IDs: 82,404\n",
      "Sequence IDs: 82,404\n",
      "Matching IDs: 82,404\n",
      "âœ… ID alignment verified (100.0% coverage)\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary: top N most frequent GO terms\n",
    "print(\"Building vocabulary...\")\n",
    "term_counts = Counter(train_terms['term'])\n",
    "filtered_terms = {term: count for term, count in term_counts.items() if count >= MIN_COUNT}\n",
    "vocab_terms = [term for term, _ in sorted(filtered_terms.items(), key=lambda x: x[1], reverse=True)[:VOCAB_SIZE]]\n",
    "vocab = {term: idx for idx, term in enumerate(vocab_terms)}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Coverage: {sum(filtered_terms[t] for t in vocab_terms) / len(train_terms):.2%} of annotations\")\n",
    "\n",
    "# Save vocabulary\n",
    "with open(SAVE_DIR / \"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f, indent=2)\n",
    "print(f\"âœ… Vocabulary saved\")\n",
    "\n",
    "# Load IA weights for evaluation\n",
    "print(\"\\nLoading IA weights...\")\n",
    "ia_df = pd.read_csv(base_dir / \"IA.tsv\", sep='\\t', header=None, names=['term', 'IA'])\n",
    "ia_weights = dict(zip(ia_df['term'], ia_df['IA']))\n",
    "print(f\"âœ… IA weights loaded: {len(ia_weights)} terms\")\n",
    "\n",
    "# Verify ID alignment\n",
    "print(\"\\nðŸ” Checking ID alignment...\")\n",
    "terms_ids = set(train_terms['EntryID'].unique())\n",
    "seq_ids = set(sequences.keys())\n",
    "matching_ids = terms_ids & seq_ids\n",
    "print(f\"Annotation IDs: {len(terms_ids):,}\")\n",
    "print(f\"Sequence IDs: {len(seq_ids):,}\")\n",
    "print(f\"Matching IDs: {len(matching_ids):,}\")\n",
    "if len(matching_ids) == 0:\n",
    "    print(\"âš ï¸ WARNING: No matching IDs! Check format alignment.\")\n",
    "else:\n",
    "    print(f\"âœ… ID alignment verified ({100*len(matching_ids)/len(terms_ids):.1f}% coverage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41fd1cd",
   "metadata": {},
   "source": [
    "## 3. Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62a9bcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset class defined (max_length=448)\n"
     ]
    }
   ],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, protein_ids, sequences_dict, annotations_df, vocab_dict, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.protein_ids = protein_ids\n",
    "        self.sequences = sequences_dict\n",
    "        self.vocab = vocab_dict\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Build protein -> terms mapping\n",
    "        self.protein_terms = {}\n",
    "        for protein_id in protein_ids:\n",
    "            terms = annotations_df[annotations_df['EntryID'] == protein_id]['term'].tolist()\n",
    "            # Filter to vocabulary\n",
    "            terms_in_vocab = [t for t in terms if t in vocab_dict]\n",
    "            self.protein_terms[protein_id] = terms_in_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.protein_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        protein_id = self.protein_ids[idx]\n",
    "        sequence = self.sequences[protein_id]\n",
    "        \n",
    "        # Tokenize sequence\n",
    "        tokens = self.tokenizer(\n",
    "            sequence,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create multi-hot label vector\n",
    "        labels = torch.zeros(len(self.vocab), dtype=torch.float32)\n",
    "        for term in self.protein_terms[protein_id]:\n",
    "            labels[self.vocab[term]] = 1.0\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokens['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(0),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "print(f\"âœ… Dataset class defined (max_length={MAX_LENGTH})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211dd813",
   "metadata": {},
   "source": [
    "## 4. Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe6f15b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model class defined\n"
     ]
    }
   ],
   "source": [
    "class ESMForGOPrediction(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.esm = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.esm.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get ESM-2 embeddings\n",
    "        outputs = self.esm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Mean pooling\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        pooled = sum_embeddings / sum_mask\n",
    "        \n",
    "        # Classification\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits\n",
    "\n",
    "print(\"âœ… Model class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc747711",
   "metadata": {},
   "source": [
    "## 5. Create Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a73194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train proteins: 65923\n",
      "Val proteins: 16481\n"
     ]
    }
   ],
   "source": [
    "# Split proteins\n",
    "all_proteins = [p for p in train_terms['EntryID'].unique() if p in sequences]\n",
    "train_proteins, val_proteins = train_test_split(\n",
    "    all_proteins, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train proteins: {len(train_proteins)}\")\n",
    "print(f\"Val proteins: {len(val_proteins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7cfd6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f66d751fcf64997b93b33fe5f7510ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\venv\\cafa6\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Olale\\.cache\\huggingface\\hub\\models--facebook--esm2_t30_150M_UR50D. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc1f093f18f45c8aa10c2eff03df740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/93.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fa99db21934126886115512cef4858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "âœ… Train batches: 32962\n",
      "âœ… Val batches: 8241\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = ProteinDataset(train_proteins, sequences, train_terms, vocab, tokenizer)\n",
    "val_dataset = ProteinDataset(val_proteins, sequences, train_terms, vocab, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"âœ… Train batches: {len(train_loader)}\")\n",
    "print(f\"âœ… Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a199b8",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e63dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AsymmetricLoss class defined\n"
     ]
    }
   ],
   "source": [
    "class AsymmetricLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Asymmetric Loss for imbalanced multi-label classification.\n",
    "    \n",
    "    Down-weights easy negatives to focus model on hard positives.\n",
    "    Used in archive's successful F1=0.23 run.\n",
    "    \n",
    "    Args:\n",
    "        gamma_neg: Focusing parameter for negatives (higher = more down-weighting of easy negatives)\n",
    "        gamma_pos: Focusing parameter for positives (usually 0)\n",
    "        clip: Clipping threshold for negative probabilities\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma_neg=2.0, gamma_pos=0.0, clip=0.05):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "    \n",
    "    def forward(self, logits, labels):\n",
    "        # Get probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Asymmetric clipping for negatives\n",
    "        probs_pos = probs\n",
    "        probs_neg = (1 - probs).clamp(max=1 - self.clip)\n",
    "        \n",
    "        # Asymmetric focusing\n",
    "        # Positive loss: standard cross-entropy with optional focusing\n",
    "        pos_loss = -labels * torch.log(probs_pos.clamp(min=1e-8)) * ((1 - probs_pos) ** self.gamma_pos)\n",
    "        \n",
    "        # Negative loss: down-weight easy negatives (high confidence correct rejections)\n",
    "        neg_loss = -(1 - labels) * torch.log(probs_neg.clamp(min=1e-8)) * (probs ** self.gamma_neg)\n",
    "        \n",
    "        loss = pos_loss + neg_loss\n",
    "        return loss.mean()\n",
    "\n",
    "print(\"âœ… AsymmetricLoss class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70ddd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model: facebook/esm2_t30_150M_UR50D...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea6bba503654f44b50e25f417f5cc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/779 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4538c22a3149d5963844c8da3057a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/595M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AsymmetricLoss (gamma_neg=2.0, gamma_pos=0.0)\n",
      "âœ… Mixed precision (fp16) enabled\n",
      "âœ… Model initialized\n",
      "   Total parameters: 151,343,841\n",
      "   Trainable parameters: 151,343,841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olale\\AppData\\Local\\Temp\\ipykernel_37224\\1285750156.py:22: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "print(f\"Initializing model: {MODEL_NAME}...\")\n",
    "model = ESMForGOPrediction(MODEL_NAME, len(vocab), dropout=DROPOUT).to(device)\n",
    "\n",
    "# Loss function - AsymmetricLoss for imbalanced multi-label classification\n",
    "criterion = AsymmetricLoss(gamma_neg=2.0, gamma_pos=0.0, clip=0.05)\n",
    "print(\"Using AsymmetricLoss (gamma_neg=2.0, gamma_pos=0.0)\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Mixed precision scaler (for fp16)\n",
    "if USE_FP16:\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    print(\"âœ… Mixed precision (fp16) enabled\")\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters())\n",
    "trainable_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"âœ… Model initialized\")\n",
    "print(f\"   Total parameters: {param_count:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45925200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Analyzing class balance...\n",
      "\n",
      "Positive rate: 0.0011 (0.11%)\n",
      "Negative rate: 0.9989 (99.89%)\n",
      "Imbalance ratio: 1 : 897.6\n",
      "\n",
      "ðŸ’¡ Recommended pos_weight: 897.6\n",
      "   (Currently using: 50.0)\n"
     ]
    }
   ],
   "source": [
    "# ðŸ” DIAGNOSTIC: Calculate actual positive rate\n",
    "print(\"ðŸ” Analyzing class balance...\")\n",
    "\n",
    "total_labels = 0\n",
    "positive_labels = 0\n",
    "\n",
    "for protein_id in train_proteins[:1000]:  # Sample 1000 proteins\n",
    "    terms = train_terms[train_terms['EntryID'] == protein_id]['term'].tolist()\n",
    "    terms_in_vocab = [t for t in terms if t in vocab]\n",
    "    total_labels += len(vocab)\n",
    "    positive_labels += len(terms_in_vocab)\n",
    "\n",
    "positive_rate = positive_labels / total_labels\n",
    "print(f\"\\nPositive rate: {positive_rate:.4f} ({100*positive_rate:.2f}%)\")\n",
    "print(f\"Negative rate: {1-positive_rate:.4f} ({100*(1-positive_rate):.2f}%)\")\n",
    "print(f\"Imbalance ratio: 1 : {(1-positive_rate)/positive_rate:.1f}\")\n",
    "print(f\"\\nðŸ’¡ Recommended pos_weight: {(1-positive_rate)/positive_rate:.1f}\")\n",
    "print(f\"   (Currently using: 50.0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbcc91a",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35b6942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation function defined (per-aspect CAFA metric)\n",
      "âœ… Per-aspect threshold evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, criterion, device, vocab_list, train_terms_df, ia_weights_dict, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model with per-aspect CAFA metric (MF, BP, CC split).\n",
    "    \n",
    "    Args:\n",
    "        model: trained model\n",
    "        dataloader: validation dataloader\n",
    "        criterion: loss function\n",
    "        device: cuda/cpu\n",
    "        vocab_list: list of GO terms in vocabulary order\n",
    "        train_terms_df: DataFrame with 'term' and 'aspect' columns\n",
    "        ia_weights_dict: dict mapping GO terms to IA weights\n",
    "        threshold: prediction threshold\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (avg_loss, overall_f1, aspect_breakdown)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_preds.append(probs)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Map vocab terms to aspects\n",
    "    term_to_aspect = dict(zip(train_terms_df['term'], train_terms_df['aspect']))\n",
    "    aspect_map = {'F': 'MF', 'P': 'BP', 'C': 'CC'}\n",
    "    \n",
    "    # Get aspect indices for each vocab term\n",
    "    vocab_aspects = {}\n",
    "    for idx, term in enumerate(vocab_list):\n",
    "        aspect_letter = term_to_aspect.get(term, None)\n",
    "        if aspect_letter:\n",
    "            aspect_name = aspect_map.get(aspect_letter)\n",
    "            if aspect_name:\n",
    "                if aspect_name not in vocab_aspects:\n",
    "                    vocab_aspects[aspect_name] = []\n",
    "                vocab_aspects[aspect_name].append(idx)\n",
    "    \n",
    "    # Compute per-aspect F1\n",
    "    aspect_results = {}\n",
    "    \n",
    "    for aspect_name, aspect_indices in vocab_aspects.items():\n",
    "        if not aspect_indices:\n",
    "            continue\n",
    "        \n",
    "        # Extract predictions and labels for this aspect\n",
    "        aspect_preds = all_preds[:, aspect_indices]\n",
    "        aspect_labels = all_labels[:, aspect_indices]\n",
    "        aspect_terms = [vocab_list[i] for i in aspect_indices]\n",
    "        \n",
    "        # Threshold predictions\n",
    "        aspect_preds_binary = (aspect_preds >= threshold).astype(int)\n",
    "        \n",
    "        # Get IA weights for this aspect's terms\n",
    "        aspect_weights = np.array([ia_weights_dict.get(term, 1.0) for term in aspect_terms])\n",
    "        \n",
    "        # Compute per-sample F1\n",
    "        f1_scores = []\n",
    "        for i in range(len(aspect_labels)):\n",
    "            true_pos = (aspect_labels[i] == 1) & (aspect_preds_binary[i] == 1)\n",
    "            pred_pos = (aspect_preds_binary[i] == 1)\n",
    "            actual_pos = (aspect_labels[i] == 1)\n",
    "            \n",
    "            tp = (true_pos * aspect_weights).sum()\n",
    "            fp = ((pred_pos & ~true_pos) * aspect_weights).sum()\n",
    "            fn = ((actual_pos & ~true_pos) * aspect_weights).sum()\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        aspect_results[aspect_name] = np.mean(f1_scores)\n",
    "    \n",
    "    # Overall F1 = mean of aspect F1s\n",
    "    overall_f1 = np.mean(list(aspect_results.values())) if aspect_results else 0.0\n",
    "    \n",
    "    return total_loss / len(dataloader), overall_f1, aspect_results\n",
    "\n",
    "print(\"âœ… Evaluation function defined (per-aspect CAFA metric)\")\n",
    "\n",
    "def evaluate_model_with_aspect_thresholds(model, dataloader, criterion, device, vocab_list, train_terms_df, ia_weights_dict,\n",
    "                                          mf_threshold=0.30, bp_threshold=0.01, cc_threshold=0.30):\n",
    "    \"\"\"\n",
    "    Evaluate model with per-aspect thresholds (different threshold for each aspect).\n",
    "    \n",
    "    Args:\n",
    "        model: trained model\n",
    "        dataloader: validation dataloader  \n",
    "        criterion: loss function\n",
    "        device: cuda/cpu\n",
    "        vocab_list: list of GO terms in vocabulary order\n",
    "        train_terms_df: DataFrame with 'term' and 'aspect' columns\n",
    "        ia_weights_dict: dict mapping GO terms to IA weights\n",
    "        mf_threshold: threshold for MF predictions\n",
    "        bp_threshold: threshold for BP predictions (typically lower)\n",
    "        cc_threshold: threshold for CC predictions\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (avg_loss, overall_f1, aspect_breakdown)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get probabilities\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_preds.append(probs)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Map vocab terms to aspects\n",
    "    term_to_aspect = dict(zip(train_terms_df['term'], train_terms_df['aspect']))\n",
    "    aspect_map = {'F': 'MF', 'P': 'BP', 'C': 'CC'}\n",
    "    \n",
    "    # Get aspect indices for each vocab term\n",
    "    vocab_aspects = {'MF': [], 'BP': [], 'CC': []}\n",
    "    for idx, term in enumerate(vocab_list):\n",
    "        aspect_letter = term_to_aspect.get(term, None)\n",
    "        if aspect_letter:\n",
    "            aspect_name = aspect_map.get(aspect_letter)\n",
    "            if aspect_name:\n",
    "                vocab_aspects[aspect_name].append(idx)\n",
    "    \n",
    "    # Per-aspect thresholds\n",
    "    thresholds = {'MF': mf_threshold, 'BP': bp_threshold, 'CC': cc_threshold}\n",
    "    \n",
    "    # Compute per-aspect F1\n",
    "    aspect_results = {}\n",
    "    \n",
    "    for aspect_name, aspect_indices in vocab_aspects.items():\n",
    "        if not aspect_indices:\n",
    "            continue\n",
    "        \n",
    "        threshold = thresholds[aspect_name]\n",
    "        \n",
    "        # Extract predictions and labels for this aspect\n",
    "        aspect_preds = all_preds[:, aspect_indices]\n",
    "        aspect_labels = all_labels[:, aspect_indices]\n",
    "        aspect_terms = [vocab_list[i] for i in aspect_indices]\n",
    "        \n",
    "        # Threshold predictions with aspect-specific threshold\n",
    "        aspect_preds_binary = (aspect_preds >= threshold).astype(int)\n",
    "        \n",
    "        # Get IA weights for this aspect's terms\n",
    "        aspect_weights = np.array([ia_weights_dict.get(term, 1.0) for term in aspect_terms])\n",
    "        \n",
    "        # Compute per-sample F1\n",
    "        f1_scores = []\n",
    "        for i in range(len(aspect_labels)):\n",
    "            true_pos = (aspect_labels[i] == 1) & (aspect_preds_binary[i] == 1)\n",
    "            pred_pos = (aspect_preds_binary[i] == 1)\n",
    "            actual_pos = (aspect_labels[i] == 1)\n",
    "            \n",
    "            tp = (true_pos * aspect_weights).sum()\n",
    "            fp = ((pred_pos & ~true_pos) * aspect_weights).sum()\n",
    "            fn = ((actual_pos & ~true_pos) * aspect_weights).sum()\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        aspect_results[aspect_name] = np.mean(f1_scores)\n",
    "    \n",
    "    # Overall F1 = mean of aspect F1s\n",
    "    overall_f1 = np.mean(list(aspect_results.values())) if aspect_results else 0.0\n",
    "    \n",
    "    return total_loss / len(dataloader), overall_f1, aspect_results\n",
    "\n",
    "print(\"âœ… Per-aspect threshold evaluation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ae50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "ðŸ”¥ Model: facebook/esm2_t30_150M_UR50D\n",
      "   Batch size: 2 x 16 = 32\n",
      "   Learning rate: 1e-05\n",
      "   Mixed precision: True\n",
      "\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba378779c6b4fdda697922db0fb42ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/32962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olale\\AppData\\Local\\Temp\\ipykernel_37224\\1010690236.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\\n\")\n",
    "print(f\"ðŸ”¥ Model: {MODEL_NAME}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE} x {GRADIENT_ACCUMULATION} = {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Mixed precision: {USE_FP16}\")\n",
    "\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch_idx, batch in enumerate(progress):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if USE_FP16 and scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                loss = loss / GRADIENT_ACCUMULATION\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights every GRADIENT_ACCUMULATION steps\n",
    "            if (batch_idx + 1) % GRADIENT_ACCUMULATION == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            # Standard training (no fp16)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss = loss / GRADIENT_ACCUMULATION\n",
    "            loss.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % GRADIENT_ACCUMULATION == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item() * GRADIENT_ACCUMULATION\n",
    "        progress.set_postfix({'loss': train_loss / (batch_idx + 1)})\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation with per-aspect thresholds\n",
    "    val_loss, val_f1, aspect_breakdown = evaluate_model_with_aspect_thresholds(\n",
    "        model, val_loader, criterion, device, \n",
    "        vocab_terms, train_terms, ia_weights,\n",
    "        mf_threshold=0.40, bp_threshold=0.20, cc_threshold=0.40\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val F1 (Overall): {val_f1:.4f}\")\n",
    "    print(f\"  MF: {aspect_breakdown.get('MF', 0):.4f} (threshold=0.40)\")\n",
    "    print(f\"  BP: {aspect_breakdown.get('BP', 0):.4f} (threshold=0.20)\")\n",
    "    print(f\"  CC: {aspect_breakdown.get('CC', 0):.4f} (threshold=0.40)\")\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_f1': val_f1,\n",
    "        'val_f1_mf': aspect_breakdown.get('MF', 0),\n",
    "        'val_f1_bp': aspect_breakdown.get('BP', 0),\n",
    "        'val_f1_cc': aspect_breakdown.get('CC', 0)\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        patience_counter = 0\n",
    "        \n",
    "        best_model_dir = SAVE_DIR / \"best_model\"\n",
    "        best_model_dir.mkdir(exist_ok=True)\n",
    "        torch.save(model.state_dict(), best_model_dir / \"pytorch_model.bin\")\n",
    "        \n",
    "        # Save config\n",
    "        config = {\n",
    "            'model_name': MODEL_NAME,\n",
    "            'num_labels': len(vocab),\n",
    "            'dropout': DROPOUT,\n",
    "            'max_length': MAX_LENGTH,\n",
    "            'best_f1': float(best_f1),\n",
    "            'optimal_thresholds': {'MF': 0.40, 'BP': 0.20, 'CC': 0.40},\n",
    "            'aspect_breakdown': {k: float(v) for k, v in aspect_breakdown.items()}\n",
    "        }\n",
    "        with open(best_model_dir / \"config.json\", \"w\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        print(f\"ðŸ† New best model saved! F1: {best_f1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nâ¹ï¸ Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… Training complete!\")\n",
    "print(f\"   Best F1: {best_f1:.4f}\")\n",
    "print(f\"   Model saved to: {SAVE_DIR / 'best_model'}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ” DIAGNOSTIC: Check predictions\n",
    "print(\"ðŸ” Checking predictions on validation set...\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get one batch\n",
    "    batch = next(iter(val_loader))\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    logits = model(input_ids, attention_mask)\n",
    "    probs = torch.sigmoid(logits)\n",
    "    \n",
    "    print(f\"\\nBatch shape: {probs.shape}\")\n",
    "    print(f\"Prediction stats:\")\n",
    "    print(f\"  Min: {probs.min().item():.6f}\")\n",
    "    print(f\"  Max: {probs.max().item():.6f}\")\n",
    "    print(f\"  Mean: {probs.mean().item():.6f}\")\n",
    "    print(f\"  Median: {probs.median().item():.6f}\")\n",
    "    \n",
    "    print(f\"\\nLabel stats:\")\n",
    "    print(f\"  Positive labels per sample: {labels.sum(dim=1).mean().item():.1f}\")\n",
    "    print(f\"  Total positive labels: {labels.sum().item():.0f}\")\n",
    "    \n",
    "    # Check predictions at different thresholds\n",
    "    for thresh in [0.1, 0.3, 0.5, 0.7]:\n",
    "        preds_count = (probs >= thresh).sum().item()\n",
    "        print(f\"  Predictions >= {thresh}: {preds_count}\")\n",
    "    \n",
    "    # Check if any predictions match labels\n",
    "    preds_05 = (probs >= 0.5).float()\n",
    "    matches = (preds_05 == labels).sum().item()\n",
    "    total = labels.numel()\n",
    "    print(f\"\\nExact matches at 0.5 threshold: {matches}/{total} ({100*matches/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793051e",
   "metadata": {},
   "source": [
    "## 8. Save Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05383f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.to_csv(SAVE_DIR / \"training_history.csv\", index=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Training History:\")\n",
    "print(history_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nâœ… Model and history saved to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c30e2d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**ESM-2 150M Fine-Tuned Classifier:**\n",
    "- Model: `facebook/esm2_t30_150M_UR50D` (150M parameters)\n",
    "- Trained on 5000 most common GO terms\n",
    "- Multi-label classification with AsymmetricLoss\n",
    "- Mixed precision (fp16) for memory efficiency\n",
    "- Per-aspect thresholds: MF=0.40, BP=0.20, CC=0.40\n",
    "- Expected F1: ~0.34-0.37\n",
    "\n",
    "**Memory Optimisation:**\n",
    "- Batch size 2 (with gradient accumulation 16)\n",
    "- Max sequence length 448 (reduced from 512)\n",
    "- FP16 mixed precision training\n",
    "\n",
    "**Next Steps:**\n",
    "1. Scale to ESM-2 650M if 150M successful\n",
    "2. Generate test predictions for submission\n",
    "3. Build ensemble with KNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cafa6)",
   "language": "python",
   "name": "cafa6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
