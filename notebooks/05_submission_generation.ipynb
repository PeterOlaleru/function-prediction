{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7857ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65acc850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Environment: LOCAL\n",
      "üìä Thresholds: MF=0.4, BP=0.2, CC=0.4\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "ENVIRONMENT = 'local'  # 'local' or 'kaggle'\n",
    "\n",
    "# Optimal thresholds from notebook 04\n",
    "THRESHOLDS = {\n",
    "    'MF': 0.40,\n",
    "    'BP': 0.20,\n",
    "    'CC': 0.40\n",
    "}\n",
    "\n",
    "MAX_TERMS_PER_PROTEIN = 1500\n",
    "\n",
    "print(f\"üîß Environment: {ENVIRONMENT.upper()}\")\n",
    "print(f\"üìä Thresholds: MF={THRESHOLDS['MF']}, BP={THRESHOLDS['BP']}, CC={THRESHOLDS['CC']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7a7f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Base directory: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\n",
      "üìÅ Output directory: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\submissions\n",
      "üìÑ Test FASTA: testsuperset.fasta\n"
     ]
    }
   ],
   "source": [
    "# Set paths\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    base_dir = Path(\"/kaggle/input/cafa-6-dataset\")\n",
    "    output_dir = Path(\"/kaggle/working\")\n",
    "else:\n",
    "    if Path.cwd().name == 'notebooks':\n",
    "        base_dir = Path.cwd().parent\n",
    "    else:\n",
    "        base_dir = Path.cwd()\n",
    "    output_dir = base_dir / \"submissions\"\n",
    "\n",
    "# Create output directory\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Data paths\n",
    "TEST_FASTA = base_dir / 'Test' / 'testsuperset.fasta'\n",
    "TRAIN_TERMS = base_dir / 'Train' / 'train_terms.tsv'\n",
    "KNN_OUTPUT = base_dir / 'outputs' / 'knn_baseline'\n",
    "\n",
    "print(f\"üìÅ Base directory: {base_dir}\")\n",
    "print(f\"üìÅ Output directory: {output_dir}\")\n",
    "print(f\"üìÑ Test FASTA: {TEST_FASTA.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264be2e",
   "metadata": {},
   "source": [
    "## 1. Load Test Protein IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7093b890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test proteins...\n",
      "‚úÖ Loaded 224,309 test proteins\n",
      "   First 5: ['A0A0C5B5G6', 'A0A1B0GTW7', 'A0JNW5', 'A0JP26', 'A0PK11']\n",
      "‚úÖ Loaded 224,309 test proteins\n",
      "   First 5: ['A0A0C5B5G6', 'A0A1B0GTW7', 'A0JNW5', 'A0JP26', 'A0PK11']\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "print(\"Loading test proteins...\")\n",
    "\n",
    "test_proteins = []\n",
    "test_sequences = {}\n",
    "\n",
    "for record in SeqIO.parse(TEST_FASTA, \"fasta\"):\n",
    "    header = record.id\n",
    "    # Handle different FASTA formats\n",
    "    if \"|\" in header:\n",
    "        parts = header.split(\"|\")\n",
    "        protein_id = parts[1] if len(parts) >= 2 else header\n",
    "    else:\n",
    "        protein_id = header.split()[0]\n",
    "    \n",
    "    test_proteins.append(protein_id)\n",
    "    test_sequences[protein_id] = str(record.seq)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(test_proteins):,} test proteins\")\n",
    "print(f\"   First 5: {test_proteins[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a339d",
   "metadata": {},
   "source": [
    "## 2. Load KNN Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b41ca508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading KNN model artifacts...\n",
      "‚úÖ KNN Model: k=10\n",
      "   Embedding: facebook/esm2_t6_8M_UR50D\n",
      "   Validation F1: 0.2604 (aspect-specific)\n",
      "   Optimal thresholds: MF=0.5, BP=0.25, CC=0.35\n",
      "   ‚ö†Ô∏è vocab.json not found, will extract from predictions\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "print(\"Loading KNN model artifacts...\")\n",
    "\n",
    "# Check if KNN outputs exist\n",
    "if not KNN_OUTPUT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"KNN outputs not found at {KNN_OUTPUT}\\n\"\n",
    "        \"Run notebook 02_baseline_knn.ipynb first to generate predictions.\"\n",
    "    )\n",
    "\n",
    "# Load metadata\n",
    "with open(KNN_OUTPUT / \"metadata.json\", 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ KNN Model: k={metadata['k_neighbors']}\")\n",
    "print(f\"   Embedding: {metadata['embedding_model']}\")\n",
    "\n",
    "# Handle both old and new metadata formats\n",
    "if 'aspect_specific_f1' in metadata:\n",
    "    print(f\"   Validation F1: {metadata['aspect_specific_f1']:.4f} (aspect-specific)\")\n",
    "    # Use optimal thresholds from metadata if available\n",
    "    if 'optimal_thresholds' in metadata:\n",
    "        print(f\"   Optimal thresholds: MF={metadata['optimal_thresholds']['MF']}, \"\n",
    "              f\"BP={metadata['optimal_thresholds']['BP']}, CC={metadata['optimal_thresholds']['CC']}\")\n",
    "elif 'best_f1' in metadata:\n",
    "    print(f\"   Validation F1: {metadata['best_f1']:.4f} (single threshold)\")\n",
    "else:\n",
    "    print(\"   Validation F1: (not available)\")\n",
    "\n",
    "# Load vocabulary\n",
    "vocab_path = KNN_OUTPUT / \"vocab.json\"\n",
    "if vocab_path.exists():\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocab = json.load(f)\n",
    "    print(f\"   Vocabulary size: {len(vocab)} terms\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è vocab.json not found, will extract from predictions\")\n",
    "    vocab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d24a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading term-to-aspect mapping...\n",
      "‚úÖ Loaded aspect mapping for 26,125 terms\n",
      "‚úÖ Loaded aspect mapping for 26,125 terms\n"
     ]
    }
   ],
   "source": [
    "# Load term-to-aspect mapping\n",
    "print(\"\\nLoading term-to-aspect mapping...\")\n",
    "\n",
    "train_terms_df = pd.read_csv(TRAIN_TERMS, sep='\\t')\n",
    "term_to_aspect = dict(zip(train_terms_df['term'], train_terms_df['aspect']))\n",
    "\n",
    "# Map aspect letters to names\n",
    "aspect_letter_to_name = {'F': 'MF', 'P': 'BP', 'C': 'CC'}\n",
    "\n",
    "print(f\"‚úÖ Loaded aspect mapping for {len(term_to_aspect):,} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9fe58",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions for Test Set\n",
    "\n",
    "**Note:** This requires the KNN model and embeddings. If not available, we'll use a placeholder approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "788a75dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-computed test predictions...\n",
      "‚úÖ Loaded 9,802,962 predictions\n",
      "‚úÖ Loaded 9,802,962 predictions\n"
     ]
    }
   ],
   "source": [
    "# Check if we have pre-computed test predictions\n",
    "test_predictions_path = KNN_OUTPUT / \"test_predictions.parquet\"\n",
    "\n",
    "if test_predictions_path.exists():\n",
    "    print(\"Loading pre-computed test predictions...\")\n",
    "    test_pred_df = pd.read_parquet(test_predictions_path)\n",
    "    print(f\"‚úÖ Loaded {len(test_pred_df):,} predictions\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pre-computed test predictions not found.\")\n",
    "    print(\"   Need to run KNN inference on test set.\")\n",
    "    print(\"   This requires:\")\n",
    "    print(\"   1. Test sequence embeddings\")\n",
    "    print(\"   2. Training embeddings + labels\")\n",
    "    print(\"   3. KNN model\")\n",
    "    print(\"\\n   For now, creating a placeholder...\")\n",
    "    test_pred_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777d645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no pre-computed predictions, FAIL - don't generate garbage\n",
    "if test_pred_df is None:\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚ùå CANNOT GENERATE SUBMISSION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  test_predictions.parquet not found!\")\n",
    "    print(f\"    Expected at: {test_predictions_path}\")\n",
    "    \n",
    "    print(\"\\nüìã Required steps:\")\n",
    "    print(\"   1. Run notebook 02_baseline_knn.ipynb\")\n",
    "    print(\"   2. Set GENERATE_TEST_PREDICTIONS = True\")\n",
    "    print(\"   3. Re-run this notebook\")\n",
    "    \n",
    "    print(\"\\nüí° The notebook will NOT generate placeholder predictions.\")\n",
    "    print(\"   This prevents submitting meaningless results.\")\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"Test predictions not found at {test_predictions_path}. \"\n",
    "        \"Run notebook 02 with GENERATE_TEST_PREDICTIONS=True first.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0206d9",
   "metadata": {},
   "source": [
    "## 4. Apply Aspect-Specific Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54dcca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aspect_thresholds(pred_df, term_to_aspect, thresholds, aspect_letter_to_name):\n",
    "    \"\"\"\n",
    "    Filter predictions using aspect-specific thresholds.\n",
    "    \n",
    "    Args:\n",
    "        pred_df: DataFrame with [EntryID, term, probability]\n",
    "        term_to_aspect: Dict mapping term -> aspect letter (F/P/C)\n",
    "        thresholds: Dict mapping aspect name -> threshold\n",
    "        aspect_letter_to_name: Dict mapping F->MF, P->BP, C->CC\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    print(\"Applying aspect-specific thresholds...\")\n",
    "    \n",
    "    # Add aspect column\n",
    "    pred_df = pred_df.copy()\n",
    "    pred_df['aspect_letter'] = pred_df['term'].map(term_to_aspect)\n",
    "    pred_df['aspect'] = pred_df['aspect_letter'].map(aspect_letter_to_name)\n",
    "    \n",
    "    # Get threshold for each row\n",
    "    pred_df['threshold'] = pred_df['aspect'].map(thresholds)\n",
    "    \n",
    "    # Handle missing aspects (use highest threshold as default)\n",
    "    default_threshold = max(thresholds.values())\n",
    "    pred_df['threshold'] = pred_df['threshold'].fillna(default_threshold)\n",
    "    \n",
    "    # Filter by threshold\n",
    "    filtered = pred_df[pred_df['probability'] >= pred_df['threshold']].copy()\n",
    "    \n",
    "    print(f\"   Before filtering: {len(pred_df):,} predictions\")\n",
    "    print(f\"   After filtering:  {len(filtered):,} predictions\")\n",
    "    \n",
    "    # Stats per aspect\n",
    "    for aspect in ['MF', 'BP', 'CC']:\n",
    "        before = len(pred_df[pred_df['aspect'] == aspect])\n",
    "        after = len(filtered[filtered['aspect'] == aspect])\n",
    "        print(f\"   {aspect}: {before:,} ‚Üí {after:,} (threshold={thresholds[aspect]})\")\n",
    "    \n",
    "    return filtered[['EntryID', 'term', 'probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3678e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying aspect-specific thresholds...\n",
      "   Before filtering: 9,802,962 predictions\n",
      "   After filtering:  3,153,866 predictions\n",
      "   Before filtering: 9,802,962 predictions\n",
      "   After filtering:  3,153,866 predictions\n",
      "   MF: 1,925,633 ‚Üí 425,899 (threshold=0.4)\n",
      "   MF: 1,925,633 ‚Üí 425,899 (threshold=0.4)\n",
      "   BP: 5,350,528 ‚Üí 2,198,926 (threshold=0.2)\n",
      "   BP: 5,350,528 ‚Üí 2,198,926 (threshold=0.2)\n",
      "   CC: 2,526,801 ‚Üí 529,041 (threshold=0.4)\n",
      "   CC: 2,526,801 ‚Üí 529,041 (threshold=0.4)\n"
     ]
    }
   ],
   "source": [
    "if test_pred_df is not None:\n",
    "    filtered_df = apply_aspect_thresholds(\n",
    "        test_pred_df, \n",
    "        term_to_aspect, \n",
    "        THRESHOLDS, \n",
    "        aspect_letter_to_name\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå No predictions to filter\")\n",
    "    filtered_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afddd11",
   "metadata": {},
   "source": [
    "## 5. Limit Terms Per Protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "972deb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_terms_per_protein(pred_df, max_terms=1500):\n",
    "    \"\"\"\n",
    "    Limit predictions to top N terms per protein by confidence.\n",
    "    \n",
    "    Args:\n",
    "        pred_df: DataFrame with [EntryID, term, probability]\n",
    "        max_terms: Maximum terms per protein\n",
    "    \n",
    "    Returns:\n",
    "        Limited DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Limiting to max {max_terms} terms per protein...\")\n",
    "    \n",
    "    # Sort by protein and probability (descending)\n",
    "    sorted_df = pred_df.sort_values(\n",
    "        ['EntryID', 'probability'], \n",
    "        ascending=[True, False]\n",
    "    )\n",
    "    \n",
    "    # Keep top N per protein\n",
    "    limited = sorted_df.groupby('EntryID').head(max_terms)\n",
    "    \n",
    "    # Check how many proteins were affected\n",
    "    terms_per_protein = pred_df.groupby('EntryID').size()\n",
    "    affected = (terms_per_protein > max_terms).sum()\n",
    "    \n",
    "    print(f\"   Proteins with >{max_terms} terms: {affected:,}\")\n",
    "    print(f\"   Total predictions: {len(pred_df):,} ‚Üí {len(limited):,}\")\n",
    "    \n",
    "    return limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7bc9a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting to max 1500 terms per protein...\n",
      "   Proteins with >1500 terms: 0\n",
      "   Total predictions: 3,153,866 ‚Üí 3,153,866\n",
      "   Proteins with >1500 terms: 0\n",
      "   Total predictions: 3,153,866 ‚Üí 3,153,866\n"
     ]
    }
   ],
   "source": [
    "if filtered_df is not None:\n",
    "    limited_df = limit_terms_per_protein(filtered_df, MAX_TERMS_PER_PROTEIN)\n",
    "else:\n",
    "    limited_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b783858",
   "metadata": {},
   "source": [
    "## 6. Format and Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecaaa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(pred_df, output_path, version=\"v1\"):\n",
    "    \"\"\"\n",
    "    Create competition submission file.\n",
    "    \n",
    "    Format: EntryID<tab>GO_term<tab>confidence\n",
    "    No header, confidence with 3 decimal places.\n",
    "    \"\"\"\n",
    "    print(f\"Creating submission file...\")\n",
    "    \n",
    "    # Prepare submission DataFrame\n",
    "    submission = pred_df.copy()\n",
    "    submission = submission.rename(columns={'term': 'GO_term', 'probability': 'confidence'})\n",
    "    \n",
    "    # Format confidence to 3 decimal places\n",
    "    submission['confidence'] = submission['confidence'].round(3)\n",
    "    \n",
    "    # Ensure confidence > 0 (competition requirement)\n",
    "    submission['confidence'] = submission['confidence'].clip(lower=0.001)\n",
    "    \n",
    "    # Sort by EntryID then confidence (descending)\n",
    "    submission = submission.sort_values(\n",
    "        ['EntryID', 'confidence'], \n",
    "        ascending=[True, False]\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    submission[['EntryID', 'GO_term', 'confidence']].to_csv(\n",
    "        output_path, \n",
    "        sep='\\t', \n",
    "        header=False, \n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved to: {output_path}\")\n",
    "    print(f\"   Total rows: {len(submission):,}\")\n",
    "    print(f\"   Unique proteins: {submission['EntryID'].nunique():,}\")\n",
    "    print(f\"   Unique GO terms: {submission['GO_term'].nunique():,}\")\n",
    "    print(f\"   Avg terms/protein: {len(submission) / submission['EntryID'].nunique():.1f}\")\n",
    "    print(f\"   Confidence range: [{submission['confidence'].min():.3f}, {submission['confidence'].max():.3f}]\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec555f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "\n",
      "‚úÖ Saved to: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\submissions\\submission_knn_aspect_20251125_1255.tsv\n",
      "   Total rows: 3,153,866\n",
      "   Unique proteins: 224,309\n",
      "\n",
      "‚úÖ Saved to: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\submissions\\submission_knn_aspect_20251125_1255.tsv\n",
      "   Total rows: 3,153,866\n",
      "   Unique proteins: 224,309\n",
      "   Unique GO terms: 22,530\n",
      "   Avg terms/protein: 14.1\n",
      "   Confidence range: [0.200, 1.000]\n",
      "   Unique GO terms: 22,530\n",
      "   Avg terms/protein: 14.1\n",
      "   Confidence range: [0.200, 1.000]\n"
     ]
    }
   ],
   "source": [
    "if limited_df is not None:\n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    submission_filename = f\"submission_knn_aspect_{timestamp}.tsv\"\n",
    "    submission_path = output_dir / submission_filename\n",
    "    \n",
    "    submission = create_submission(limited_df, submission_path)\n",
    "else:\n",
    "    print(\"‚ùå No predictions available for submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fbccc",
   "metadata": {},
   "source": [
    "## 7. Validate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebede0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_submission(filepath):\n",
    "    \"\"\"\n",
    "    Validate submission file format for CAFA-6 competition.\n",
    "    \"\"\"\n",
    "    print(f\"Validating: {filepath}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load submission\n",
    "    df = pd.read_csv(filepath, sep='\\t', header=None, \n",
    "                     names=['EntryID', 'GO_term', 'confidence'])\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    # Check 1: Three columns\n",
    "    if len(df.columns) != 3:\n",
    "        errors.append(f\"Expected 3 columns, got {len(df.columns)}\")\n",
    "    \n",
    "    # Check 2: Confidence range (0, 1]\n",
    "    df['confidence'] = df['confidence'].astype(float)\n",
    "    if df['confidence'].min() <= 0:\n",
    "        errors.append(f\"Confidence must be > 0 (min: {df['confidence'].min()})\")\n",
    "    if df['confidence'].max() > 1:\n",
    "        errors.append(f\"Confidence must be <= 1 (max: {df['confidence'].max()})\")\n",
    "    \n",
    "    # Check 3: Max 1500 terms per protein\n",
    "    terms_per_protein = df.groupby('EntryID').size()\n",
    "    if terms_per_protein.max() > 1500:\n",
    "        errors.append(f\"Max 1500 terms/protein (found {terms_per_protein.max()})\")\n",
    "    \n",
    "    # Check 4: GO term format\n",
    "    invalid_terms = ~df['GO_term'].str.match(r'^GO:\\d{7}$')\n",
    "    if invalid_terms.any():\n",
    "        bad_examples = df.loc[invalid_terms, 'GO_term'].head(3).tolist()\n",
    "        errors.append(f\"Invalid GO term format: {bad_examples}\")\n",
    "    \n",
    "    # Check 5: No duplicates\n",
    "    duplicates = df.duplicated(subset=['EntryID', 'GO_term']).sum()\n",
    "    if duplicates > 0:\n",
    "        errors.append(f\"Found {duplicates} duplicate (EntryID, GO_term) pairs\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"‚ùå VALIDATION FAILED:\")\n",
    "        for error in errors:\n",
    "            print(f\"   - {error}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"‚úÖ VALIDATION PASSED!\")\n",
    "        print(f\"\\nüìä Summary:\")\n",
    "        print(f\"   Total rows: {len(df):,}\")\n",
    "        print(f\"   Unique proteins: {df['EntryID'].nunique():,}\")\n",
    "        print(f\"   Unique GO terms: {df['GO_term'].nunique():,}\")\n",
    "        print(f\"   Avg terms/protein: {terms_per_protein.mean():.1f}\")\n",
    "        print(f\"   Max terms/protein: {terms_per_protein.max()}\")\n",
    "        print(f\"   Median confidence: {df['confidence'].median():.3f}\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e2f9fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\submissions\\submission_knn_aspect_20251125_1255.tsv\n",
      "==================================================\n",
      "‚úÖ VALIDATION PASSED!\n",
      "\n",
      "üìä Summary:\n",
      "   Total rows: 3,153,866\n",
      "   Unique proteins: 224,309\n",
      "‚úÖ VALIDATION PASSED!\n",
      "\n",
      "üìä Summary:\n",
      "   Total rows: 3,153,866\n",
      "   Unique proteins: 224,309\n",
      "   Unique GO terms: 22,530\n",
      "   Avg terms/protein: 14.1\n",
      "   Max terms/protein: 304\n",
      "   Median confidence: 0.333\n",
      "\n",
      "üéâ Submission ready for upload: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\submissions\\submission_knn_aspect_20251125_1255.tsv\n",
      "   Unique GO terms: 22,530\n",
      "   Avg terms/protein: 14.1\n",
      "   Max terms/protein: 304\n",
      "   Median confidence: 0.333\n",
      "\n",
      "üéâ Submission ready for upload: c:\\Users\\Olale\\Documents\\Codebase\\Science\\cafa-6-protein-function-prediction\\submissions\\submission_knn_aspect_20251125_1255.tsv\n"
     ]
    }
   ],
   "source": [
    "if limited_df is not None:\n",
    "    is_valid = validate_submission(submission_path)\n",
    "    \n",
    "    if is_valid:\n",
    "        print(f\"\\nüéâ Submission ready for upload: {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bee165",
   "metadata": {},
   "source": [
    "## 8. Preview Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "282b4f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 rows of submission:\n",
      "==================================================\n",
      "   EntryID    GO_term  confidence\n",
      "A0A017SE81 GO:0016020       1.000\n",
      "A0A017SE81 GO:0004745       0.999\n",
      "A0A017SE81 GO:0047023       0.751\n",
      "A0A017SE81 GO:0047044       0.751\n",
      "A0A017SE81 GO:0047024       0.501\n",
      "A0A017SE81 GO:0000140       0.501\n",
      "A0A017SE81 GO:0008611       0.501\n",
      "A0A017SE81 GO:0005737       0.501\n",
      "A0A017SE81 GO:0004303       0.500\n",
      "A0A017SE81 GO:0047035       0.500\n",
      "A0A017SE81 GO:0005515       0.500\n",
      "A0A017SE81 GO:0062175       0.250\n",
      "A0A017SE81 GO:0006710       0.250\n",
      "A0A017SE81 GO:0006355       0.250\n",
      "A0A017SE81 GO:0050873       0.250\n",
      "A0A017SE81 GO:0010468       0.250\n",
      "A0A017SE81 GO:0030223       0.250\n",
      "A0A017SE81 GO:0120161       0.250\n",
      "A0A017SE81 GO:0006656       0.250\n",
      "A0A017SE81 GO:0006954       0.250\n"
     ]
    }
   ],
   "source": [
    "if limited_df is not None:\n",
    "    print(\"First 20 rows of submission:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    preview = pd.read_csv(submission_path, sep='\\t', header=None, nrows=20,\n",
    "                          names=['EntryID', 'GO_term', 'confidence'])\n",
    "    print(preview.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d1d161",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd26a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìã SUBMISSION GENERATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "üîß Configuration:\n",
      "   Model: KNN with ESM-2 embeddings\n",
      "   Thresholds: MF=0.4, BP=0.2, CC=0.4\n",
      "   Max terms/protein: 1500\n",
      "\n",
      "üìä Submission Stats:\n",
      "   File: submission_knn_aspect_20251125_1255.tsv\n",
      "   Total predictions: 3,153,866\n",
      "   Proteins covered: 224,309\n",
      "\n",
      "üéØ Expected Validation F1: ~0.2579\n",
      "   (Based on per-aspect CAFA metric with optimal thresholds)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìã SUBMISSION GENERATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüîß Configuration:\")\n",
    "print(f\"   Model: KNN with ESM-2 embeddings\")\n",
    "print(f\"   Thresholds: MF={THRESHOLDS['MF']}, BP={THRESHOLDS['BP']}, CC={THRESHOLDS['CC']}\")\n",
    "print(f\"   Max terms/protein: {MAX_TERMS_PER_PROTEIN}\")\n",
    "\n",
    "if limited_df is not None:\n",
    "    print(f\"\\nüìä Submission Stats:\")\n",
    "    print(f\"   File: {submission_path.name}\")\n",
    "    print(f\"   Total predictions: {len(limited_df):,}\")\n",
    "    print(f\"   Proteins covered: {limited_df['EntryID'].nunique():,}\")\n",
    "    \n",
    "print(f\"\\nüéØ Expected Validation F1: ~0.2579\")\n",
    "print(f\"   (Based on per-aspect CAFA metric with optimal thresholds)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc3eb88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **If predictions missing:** Run notebook 02 with `GENERATE_TEST_PREDICTIONS=True`\n",
    "2. **Upload to Kaggle:** Submit the generated `.tsv` file\n",
    "3. **Improve score:** \n",
    "   - Re-evaluate ESM-2 with per-aspect metric\n",
    "   - Scale to ESM-2 150M/650M\n",
    "   - Build ensemble (KNN + ESM-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cafa6)",
   "language": "python",
   "name": "cafa6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
